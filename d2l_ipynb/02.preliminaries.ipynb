{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1856a00c",
   "metadata": {},
   "source": [
    "#  预备知识\n",
    ":label:`chap_preliminaries`\n",
    "\n",
    "要学习深度学习，首先需要先掌握一些基本技能。\n",
    "所有机器学习方法都涉及从数据中提取信息。\n",
    "因此，我们先学习一些关于数据的实用技能，包括存储、操作和预处理数据。\n",
    "\n",
    "机器学习通常需要处理大型数据集。\n",
    "我们可以将某些数据集视为一个表，其中表的行对应样本，列对应属性。\n",
    "线性代数为人们提供了一些用来处理表格数据的方法。\n",
    "我们不会太深究细节，而是将重点放在矩阵运算的基本原理及其实现上。\n",
    "\n",
    "深度学习是关于优化的学习。\n",
    "对于一个带有参数的模型，我们想要找到其中能拟合数据的最好模型。\n",
    "在算法的每个步骤中，决定以何种方式调整参数需要一点微积分知识。\n",
    "本章将简要介绍这些知识。\n",
    "幸运的是，`autograd`包会自动计算微分，本章也将介绍它。\n",
    "\n",
    "机器学习还涉及如何做出预测：给定观察到的信息，某些未知属性可能的值是多少？\n",
    "要在不确定的情况下进行严格的推断，我们需要借用概率语言。\n",
    "\n",
    "最后，官方文档提供了本书之外的大量描述和示例。\n",
    "在本章的结尾，我们将展示如何在官方文档中查找所需信息。\n",
    "\n",
    "本书对读者数学基础无过分要求，只要可以正确理解深度学习所需的数学知识即可。\n",
    "但这并不意味着本书中不涉及数学方面的内容，本章会快速介绍一些基本且常用的数学知识，\n",
    "以便读者能够理解书中的大部分数学内容。\n",
    "如果读者想要深入理解全部数学内容，可以进一步学习本书数学附录中给出的数学基础知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9c54f",
   "metadata": {
    "attributes": {
     "classes": [
      "toc"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    ":maxdepth: 2\n",
    "\n",
    "ndarray\n",
    "pandas\n",
    "linear-algebra\n",
    "calculus\n",
    "autograd\n",
    "probability\n",
    "lookup-api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c3c15",
   "metadata": {},
   "source": [
    "# 数据操作\n",
    ":label:`sec_ndarray`\n",
    "\n",
    "为了能够完成各种数据操作，我们需要某种方法来存储和操作数据。\n",
    "通常，我们需要做两件重要的事：（1）获取数据；（2）将数据读入计算机后对其进行处理。\n",
    "如果没有某种方法来存储数据，那么获取数据是没有意义的。\n",
    "\n",
    "首先，我们介绍$n$维数组，也称为*张量*（tensor）。\n",
    "使用过Python中NumPy计算包的读者会对本部分很熟悉。\n",
    "无论使用哪个深度学习框架，它的*张量类*（在MXNet中为`ndarray`，\n",
    "在PyTorch和TensorFlow中为`Tensor`）都与Numpy的`ndarray`类似。\n",
    "但深度学习框架又比Numpy的`ndarray`多一些重要功能：\n",
    "首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算；\n",
    "其次，张量类支持自动微分。\n",
    "这些功能使得张量类更适合深度学习。\n",
    "如果没有特殊说明，本书中所说的张量均指的是张量类的实例。\n",
    "\n",
    "## 入门\n",
    "\n",
    "本节的目标是帮助读者了解并运行一些在阅读本书的过程中会用到的基本数值计算工具。\n",
    "如果你很难理解一些数学概念或库函数，请不要担心。\n",
    "后面的章节将通过一些实际的例子来回顾这些内容。\n",
    "如果你已经具有相关经验，想要深入学习数学内容，可以跳过本节。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "首先，我们从MXNet导入`np`（`numpy`）模块和`npx`（`numpy_extension`）模块。\n",
    "`np`模块包含NumPy支持的函数；\n",
    "而`npx`模块包含一组扩展函数，用来在类似NumPy的环境中实现深度学习开发。\n",
    "当使用张量时，几乎总是会调用`set_np`函数，这是为了兼容MXNet的其他张量处理组件。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "(**首先，我们导入`torch`。请注意，虽然它被称为PyTorch，但是代码中使用`torch`而不是`pytorch`。**)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "首先，我们导入`tensorflow`。\n",
    "由于`tensorflow`名称有点长，我们经常在导入它后使用短别名`tf`。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d6213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5346ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf04065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454bda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2737ed15",
   "metadata": {},
   "source": [
    "[**张量表示一个由数值组成的数组，这个数组可能有多个维度**]。\n",
    "具有一个轴的张量对应数学上的*向量*（vector）；\n",
    "具有两个轴的张量对应数学上的*矩阵*（matrix）；\n",
    "具有两个轴以上的张量没有特殊的数学名称。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "首先，我们可以使用 `arange` 创建一个行向量 `x`。这个行向量包含以0开始的前12个整数，它们默认创建为浮点数。张量中的每个值都称为张量的 *元素*（element）。例如，张量 `x` 中有 12 个元素。除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "首先，我们可以使用 `arange` 创建一个行向量 `x`。这个行向量包含以0开始的前12个整数，它们默认创建为整数。也可指定创建类型为浮点数。张量中的每个值都称为张量的 *元素*（element）。例如，张量 `x` 中有 12 个元素。除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "首先，我们可以使用 `range` 创建一个行向量 `x`。这个行向量包含以0开始的前12个整数，它们默认创建为整数。也可指定创建类型为浮点数。张量中的每个值都称为张量的 *元素*（element）。例如，张量 `x` 中有 12 个元素。除非额外指定，新的张量将存储在内存中，并采用基于CPU的计算。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb5b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b05574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "x = tf.range(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d2057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x = paddle.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60267eaf",
   "metadata": {},
   "source": [
    "[**可以通过张量的`shape`属性来访问张量（沿每个轴的长度）的*形状***]\n",
    "(~~和张量中元素的总数~~)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddc56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f0d68",
   "metadata": {},
   "source": [
    "如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。\n",
    "因为这里在处理的是一个向量，所以它的`shape`与它的`size`相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ce3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.size(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ee7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a9821",
   "metadata": {},
   "source": [
    "[**要想改变一个张量的形状而不改变元素数量和元素值，可以调用`reshape`函数。**]\n",
    "例如，可以把张量`x`从形状为（12,）的行向量转换为形状为（3,4）的矩阵。\n",
    "这个新的张量包含与转换前相同的值，但是它被看成一个3行4列的矩阵。\n",
    "要重点说明一下，虽然张量的形状发生了改变，但其元素值并没有变。\n",
    "注意，通过改变张量的形状，张量的大小不会改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "X = x.reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "X = tf.reshape(x, (3, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefdc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "X = paddle.reshape(x, (3, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5813e",
   "metadata": {},
   "source": [
    "我们不需要通过手动指定每个维度来改变形状。\n",
    "也就是说，如果我们的目标形状是（高度,宽度），\n",
    "那么在知道宽度后，高度会被自动计算得出，不必我们自己做除法。\n",
    "在上面的例子中，为了获得一个3行的矩阵，我们手动指定了它有3行和4列。\n",
    "幸运的是，我们可以通过`-1`来调用此自动计算出维度的功能。\n",
    "即我们可以用`x.reshape(-1,4)`或`x.reshape(3,-1)`来取代`x.reshape(3,4)`。\n",
    "\n",
    "有时，我们希望[**使用全0、全1、其他常量，或者从特定分布中随机采样的数字**]来初始化矩阵。\n",
    "我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72641793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "torch.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d40042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4849224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.zeros((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62f6f7",
   "metadata": {},
   "source": [
    "同样，我们可以创建一个形状为`(2,3,4)`的张量，其中所有元素都设置为1。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679de26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9bf99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33824dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea6154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8833d59",
   "metadata": {},
   "source": [
    "有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。\n",
    "例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。\n",
    "以下代码创建一个形状为（3,4）的张量。\n",
    "其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.normal(0, 1, size=(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec88924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0cd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.random.normal(shape=[3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bff7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.randn((3, 4),'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5915f6c",
   "metadata": {},
   "source": [
    "我们还可以[**通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值**]。\n",
    "在这里，最外层的列表对应于轴0，内层的列表对应于轴1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784acabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86c48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92855412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.constant([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cdfebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.to_tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d65cbb",
   "metadata": {},
   "source": [
    "## 运算符\n",
    "\n",
    "我们的兴趣不仅限于读取数据和写入数据。\n",
    "我们想在这些数据上执行数学运算，其中最简单且最有用的操作是*按元素*（elementwise）运算。\n",
    "它们将标准标量运算符应用于数组的每个元素。\n",
    "对于将两个数组作为输入的函数，按元素运算将二元运算符应用于两个数组中的每对位置对应的元素。\n",
    "我们可以基于任何从标量到标量的函数来创建按元素函数。\n",
    "\n",
    "在数学表示法中，我们将通过符号$f: \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "来表示*一元*标量运算符（只接收一个输入）。\n",
    "这意味着该函数从任何实数（$\\mathbb{R}$）映射到另一个实数。\n",
    "同样，我们通过符号$f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$\n",
    "表示*二元*标量运算符，这意味着该函数接收两个输入，并产生一个输出。\n",
    "给定同一形状的任意两个向量$\\mathbf{u}$和$\\mathbf{v}$和二元运算符$f$，\n",
    "我们可以得到向量$\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$。\n",
    "具体计算方法是$c_i \\gets f(u_i, v_i)$，\n",
    "其中$c_i$、$u_i$和$v_i$分别是向量$\\mathbf{c}$、$\\mathbf{u}$和$\\mathbf{v}$中的元素。\n",
    "在这里，我们通过将标量函数升级为按元素向量运算来生成向量值\n",
    "$F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$。\n",
    "\n",
    "对于任意具有相同形状的张量，\n",
    "[**常见的标准算术运算符（`+`、`-`、`*`、`/`和`**`）都可以被升级为按元素运算**]。\n",
    "我们可以在同一形状的任意两个张量上调用按元素操作。\n",
    "在下面的例子中，我们使用逗号来表示一个具有5个元素的元组，其中每个元素都是按元素操作的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 4, 8])\n",
    "y = np.array([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  # **运算符是求幂运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29646f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  # **运算符是求幂运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515a51b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "x = tf.constant([1.0, 2, 4, 8])\n",
    "y = tf.constant([2.0, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  # **运算符是求幂运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f0f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x = paddle.to_tensor([1.0, 2, 4, 8])\n",
    "y = paddle.to_tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x**y  # **运算符是求幂运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d18a04",
   "metadata": {},
   "source": [
    "(**“按元素”方式可以应用更多的计算**)，包括像求幂这样的一元运算符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87070c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac61b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ab163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee44b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4075976b",
   "metadata": {},
   "source": [
    "除了按元素计算外，我们还可以执行线性代数运算，包括向量点积和矩阵乘法。\n",
    "我们将在 :numref:`sec_linear-algebra`中解释线性代数的重点内容。\n",
    "\n",
    "[**我们也可以把多个张量*连结*（concatenate）在一起**]，\n",
    "把它们端对端地叠起来形成一个更大的张量。\n",
    "我们只需要提供张量列表，并给出沿哪个轴连结。\n",
    "下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素）\n",
    "和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况。\n",
    "我们可以看到，第一个输出张量的轴-0长度（$6$）是两个输入张量轴-0长度的总和（$3 + 3$）；\n",
    "第二个输出张量的轴-1长度（$8$）是两个输入张量轴-1长度的总和（$4 + 4$）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(12).reshape(3, 4)\n",
    "Y = np.array([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "np.concatenate([X, Y], axis=0), np.concatenate([X, Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f585ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "X = tf.reshape(tf.range(12, dtype=tf.float32), (3, 4))\n",
    "Y = tf.constant([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "tf.concat([X, Y], axis=0), tf.concat([X, Y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96bec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "X = paddle.arange(12, dtype='float32').reshape((3, 4))\n",
    "Y = paddle.to_tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "paddle.concat((X, Y), axis=0), paddle.concat((X, Y), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b165574",
   "metadata": {},
   "source": [
    "有时，我们想[**通过*逻辑运算符*构建二元张量**]。\n",
    "以`X == Y`为例：\n",
    "对于每个位置，如果`X`和`Y`在该位置相等，则新张量中相应项的值为1。\n",
    "这意味着逻辑语句`X == Y`在该位置处为真，否则该位置为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9036f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b801bc",
   "metadata": {},
   "source": [
    "[**对张量中的所有元素进行求和，会产生一个单元素张量。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch, paddle\n",
    "X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.reduce_sum(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c0c2d",
   "metadata": {},
   "source": [
    "## 广播机制\n",
    ":label:`subsec_broadcasting`\n",
    "\n",
    "在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。\n",
    "在某些情况下，[**即使形状不同，我们仍然可以通过调用\n",
    "*广播机制*（broadcasting mechanism）来执行按元素操作**]。\n",
    "这种机制的工作方式如下：\n",
    "\n",
    "1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；\n",
    "2. 对生成的数组执行按元素操作。\n",
    "\n",
    "在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651df6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(3).reshape(3, 1)\n",
    "b = np.arange(2).reshape(1, 2)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23d9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ac036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "a = tf.reshape(tf.range(3), (3, 1))\n",
    "b = tf.reshape(tf.range(2), (1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc648d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "a = paddle.reshape(paddle.arange(3), (3, 1))\n",
    "b = paddle.reshape(paddle.arange(2), (1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e8fe75",
   "metadata": {},
   "source": [
    "由于`a`和`b`分别是$3\\times1$和$1\\times2$矩阵，如果让它们相加，它们的形状不匹配。\n",
    "我们将两个矩阵*广播*为一个更大的$3\\times2$矩阵，如下所示：矩阵`a`将复制列，\n",
    "矩阵`b`将复制行，然后再按元素相加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e809e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87626475",
   "metadata": {},
   "source": [
    "## 索引和切片\n",
    "\n",
    "就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。\n",
    "与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1；\n",
    "可以指定范围以包含第一个元素和最后一个之前的元素。\n",
    "\n",
    "如下所示，我们[**可以用`[-1]`选择最后一个元素，可以用`[1:3]`选择第二个和第三个元素**]："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e6d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3495ae99",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet, pytorch`\n",
    "[**除读取外，我们还可以通过指定索引来将元素写入矩阵。**]\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "TensorFlow中的`Tensors`是不可变的，也不能被赋值。\n",
    "TensorFlow中的`Variables`是支持赋值的可变容器。\n",
    "请记住，TensorFlow中的梯度不会通过`Variable`反向传播。\n",
    "\n",
    "除了为整个`Variable`分配一个值之外，我们还可以通过索引来写入`Variable`的元素。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16366690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch, paddle\n",
    "X[1, 2] = 9\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "X_var = tf.Variable(X)\n",
    "X_var[1, 2].assign(9)\n",
    "X_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670121d0",
   "metadata": {},
   "source": [
    "如果我们想[**为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。**]\n",
    "例如，`[0:2, :]`访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。\n",
    "虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c1944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch, paddle\n",
    "X[0:2, :] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d63ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "X_var = tf.Variable(X)\n",
    "X_var[0:2, :].assign(tf.ones(X_var[0:2,:].shape, dtype = tf.float32) * 12)\n",
    "X_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc261e6",
   "metadata": {},
   "source": [
    "## 节省内存\n",
    "\n",
    "[**运行一些操作可能会导致为新结果分配内存**]。\n",
    "例如，如果我们用`Y = X + Y`，我们将取消引用`Y`指向的张量，而是指向新分配的内存处的张量。\n",
    "\n",
    "在下面的例子中，我们用Python的`id()`函数演示了这一点，\n",
    "它给我们提供了内存中引用对象的确切地址。\n",
    "运行`Y = Y + X`后，我们会发现`id(Y)`指向另一个位置。\n",
    "这是因为Python首先计算`Y + X`，为结果分配新的内存，然后使`Y`指向内存中的这个新位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746cedef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb4b27",
   "metadata": {},
   "source": [
    "这可能是不可取的，原因有两个：\n",
    "\n",
    "1. 首先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；\n",
    "2. 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。\n",
    "\n",
    ":begin_tab:`mxnet, pytorch`\n",
    "幸运的是，(**执行原地操作**)非常简单。\n",
    "我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如`Y[:] = <expression>`。\n",
    "为了说明这一点，我们首先创建一个新的矩阵`Z`，其形状与另一个`Y`相同，\n",
    "使用`zeros_like`来分配一个全$0$的块。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "`Variables`是TensorFlow中的可变容器，它们提供了一种存储模型参数的方法。\n",
    "我们可以通过`assign`将一个操作的结果分配给一个`Variable`。\n",
    "为了说明这一点，我们创建了一个与另一个张量`Y`相同的形状的`Z`，\n",
    "使用`zeros_like`来分配一个全$0$的块。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cfce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266274d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb173cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "Z = tf.Variable(tf.zeros_like(Y))\n",
    "print('id(Z):', id(Z))\n",
    "Z.assign(X + Y)\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee61f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "Z = paddle.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d4328",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet, pytorch`\n",
    "[**如果在后续计算中没有重复使用`X`，\n",
    "我们也可以使用`X[:] = X + Y`或`X += Y`来减少操作的内存开销。**]\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "即使你将状态持久存储在`Variable`中，\n",
    "你也可能希望避免为不是模型参数的张量过度分配内存，从而进一步减少内存使用量。\n",
    "\n",
    "由于TensorFlow的`Tensors`是不可变的，而且梯度不会通过`Variable`流动，\n",
    "因此TensorFlow没有提供一种明确的方式来原地运行单个操作。\n",
    "\n",
    "但是，TensorFlow提供了`tf.function`修饰符，\n",
    "将计算封装在TensorFlow图中，该图在运行前经过编译和优化。\n",
    "这允许TensorFlow删除未使用的值，并复用先前分配的且不再需要的值。\n",
    "这样可以最大限度地减少TensorFlow计算的内存开销。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcdb0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch, paddle\n",
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "@tf.function\n",
    "def computation(X, Y):\n",
    "    Z = tf.zeros_like(Y)  # 这个未使用的值将被删除\n",
    "    A = X + Y  # 当不再需要时，分配将被复用\n",
    "    B = A + Y\n",
    "    C = B + Y\n",
    "    return C + Y\n",
    "\n",
    "computation(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ced7c8",
   "metadata": {},
   "source": [
    "## 转换为其他Python对象\n",
    ":begin_tab:`mxnet, tensorflow`\n",
    "将深度学习框架定义的张量[**转换为NumPy张量（`ndarray`）**]很容易，反之也同样容易。\n",
    "转换后的结果不共享内存。\n",
    "这个小的不便实际上是非常重要的：当在CPU或GPU上执行操作的时候，\n",
    "如果Python的NumPy包也希望使用相同的内存块执行其他操作，人们不希望停下计算来等它。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "将深度学习框架定义的张量[**转换为NumPy张量（`ndarray`）**]很容易，反之也同样容易。\n",
    "torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e46664",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X.asnumpy()\n",
    "B = np.array(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "A = X.numpy()\n",
    "B = tf.constant(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61562f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A = X.numpy()\n",
    "B = paddle.to_tensor(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c6ed0",
   "metadata": {},
   "source": [
    "要(**将大小为1的张量转换为Python标量**)，我们可以调用`item`函数或Python的内置函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8525e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04740e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ec4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "a = tf.constant([3.5]).numpy()\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8dec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "a = paddle.to_tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b055dde",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 深度学习存储和操作数据的主要接口是张量（$n$维数组）。它提供了各种功能，包括基本数学运算、广播、索引、切片、内存节省和转换其他Python对象。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 运行本节中的代码。将本节中的条件语句`X == Y`更改为`X < Y`或`X > Y`，然后看看你可以得到什么样的张量。\n",
    "1. 用其他形状（例如三维张量）替换广播机制中按元素操作的两个张量。结果是否与预期相同？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/1745)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1747)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1746)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11680)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef744933",
   "metadata": {},
   "source": [
    "# 数据预处理\n",
    ":label:`sec_pandas`\n",
    "\n",
    "为了能用深度学习来解决现实世界的问题，我们经常从预处理原始数据开始，\n",
    "而不是从那些准备好的张量格式数据开始。\n",
    "在Python中常用的数据分析工具中，我们通常使用`pandas`软件包。\n",
    "像庞大的Python生态系统中的许多其他扩展包一样，`pandas`可以与张量兼容。\n",
    "本节我们将简要介绍使用`pandas`预处理原始数据，并将原始数据转换为张量格式的步骤。\n",
    "后面的章节将介绍更多的数据预处理技术。\n",
    "\n",
    "## 读取数据集\n",
    "\n",
    "举一个例子，我们首先(**创建一个人工数据集，并存储在CSV（逗号分隔值）文件**)\n",
    "`../data/house_tiny.csv`中。\n",
    "以其他格式存储的数据也可以通过类似的方式进行处理。\n",
    "下面我们将数据集按行写入CSV文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b75122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "import os\n",
    "\n",
    "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
    "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n')  # 列名\n",
    "    f.write('NA,Pave,127500\\n')  # 每行表示一个数据样本\n",
    "    f.write('2,NA,106000\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,NA,140000\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e6cc5",
   "metadata": {},
   "source": [
    "要[**从创建的CSV文件中加载原始数据集**]，我们导入`pandas`包并调用`read_csv`函数。该数据集有四行三列。其中每行描述了房间数量（“NumRooms”）、巷子类型（“Alley”）和房屋价格（“Price”）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "# 如果没有安装pandas，只需取消对以下行的注释来安装pandas\n",
    "# !pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521e792",
   "metadata": {},
   "source": [
    "## 处理缺失值\n",
    "\n",
    "注意，“NaN”项代表缺失值。\n",
    "[**为了处理缺失的数据，典型的方法包括*插值法*和*删除法*，**]\n",
    "其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。\n",
    "在(**这里，我们将考虑插值法**)。\n",
    "\n",
    "通过位置索引`iloc`，我们将`data`分成`inputs`和`outputs`，\n",
    "其中前者为`data`的前两列，而后者为`data`的最后一列。\n",
    "对于`inputs`中缺少的数值，我们用同一列的均值替换“NaN”项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5cf18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c2a908",
   "metadata": {},
   "source": [
    "[**对于`inputs`中的类别值或离散值，我们将“NaN”视为一个类别。**]\n",
    "由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，\n",
    "`pandas`可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。\n",
    "巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。\n",
    "缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9062fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4ff64",
   "metadata": {},
   "source": [
    "## 转换为张量格式\n",
    "\n",
    "[**现在`inputs`和`outputs`中的所有条目都是数值类型，它们可以转换为张量格式。**]\n",
    "当数据采用张量格式后，可以通过在 :numref:`sec_ndarray`中引入的那些张量函数来进一步操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np\n",
    "\n",
    "X, y = np.array(inputs.values), np.array(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae35e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import torch\n",
    "\n",
    "X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "X, y = tf.constant(inputs.values), tf.constant(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import paddle\n",
    "\n",
    "X, y = paddle.to_tensor(inputs.values), paddle.to_tensor(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6dc1b8",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* `pandas`软件包是Python中常用的数据分析工具中，`pandas`可以与张量兼容。\n",
    "* 用`pandas`处理缺失的数据时，我们可根据情况选择用插值法和删除法。\n",
    "\n",
    "## 练习\n",
    "\n",
    "创建包含更多行和列的原始数据集。\n",
    "\n",
    "1. 删除缺失值最多的列。\n",
    "2. 将预处理后的数据集转换为张量格式。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/1749)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1750)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1748)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11681)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89982b5",
   "metadata": {},
   "source": [
    "# 线性代数\n",
    ":label:`sec_linear-algebra`\n",
    "\n",
    "在介绍完如何存储和操作数据后，接下来将简要地回顾一下部分基本线性代数内容。\n",
    "这些内容有助于读者了解和实现本书中介绍的大多数模型。\n",
    "本节将介绍线性代数中的基本数学对象、算术和运算，并用数学符号和相应的代码实现来表示它们。\n",
    "\n",
    "## 标量\n",
    "\n",
    "\n",
    "如果你曾经在餐厅支付餐费，那么应该已经知道一些基本的线性代数，比如在数字间相加或相乘。\n",
    "例如，北京的温度为$52^{\\circ}F$（华氏度，除摄氏度外的另一种温度计量单位）。\n",
    "严格来说，仅包含一个数值被称为*标量*（scalar）。\n",
    "如果要将此华氏度值转换为更常用的摄氏度，\n",
    "则可以计算表达式$c=\\frac{5}{9}(f-32)$，并将$f$赋为$52$。\n",
    "在此等式中，每一项（$5$、$9$和$32$）都是标量值。\n",
    "符号$c$和$f$称为*变量*（variable），它们表示未知的标量值。\n",
    "\n",
    "本书采用了数学表示法，其中标量变量由普通小写字母表示（例如，$x$、$y$和$z$）。\n",
    "本书用$\\mathbb{R}$表示所有（连续）*实数*标量的空间，之后将严格定义*空间*（space）是什么，\n",
    "但现在只要记住表达式$x\\in\\mathbb{R}$是表示$x$是一个实值标量的正式形式。\n",
    "符号$\\in$称为“属于”，它表示“是集合中的成员”。\n",
    "例如$x, y \\in \\{0,1\\}$可以用来表明$x$和$y$是值只能为$0$或$1$的数字。\n",
    "\n",
    "(**标量由只有一个元素的张量表示**)。\n",
    "下面的代码将实例化两个标量，并执行一些熟悉的算术运算，即加法、乘法、除法和指数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef98db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "npx.set_np()\n",
    "\n",
    "x = np.array(3.0)\n",
    "y = np.array(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57374dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(3.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b46861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "y = tf.constant(2.0)\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add72744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import paddle\n",
    "\n",
    "x = paddle.to_tensor([3.0])\n",
    "y = paddle.to_tensor([2.0])\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f2cdc",
   "metadata": {},
   "source": [
    "## 向量\n",
    "\n",
    "[**向量可以被视为标量值组成的列表**]。\n",
    "这些标量值被称为向量的*元素*（element）或*分量*（component）。\n",
    "当向量表示数据集中的样本时，它们的值具有一定的现实意义。\n",
    "例如，如果我们正在训练一个模型来预测贷款违约风险，可能会将每个申请人与一个向量相关联，\n",
    "其分量与其收入、工作年限、过往违约次数和其他因素相对应。\n",
    "如果我们正在研究医院患者可能面临的心脏病发作风险，可能会用一个向量来表示每个患者，\n",
    "其分量为最近的生命体征、胆固醇水平、每天运动时间等。\n",
    "在数学表示法中，向量通常记为粗体、小写的符号\n",
    "（例如，$\\mathbf{x}$、$\\mathbf{y}$和$\\mathbf{z})$）。\n",
    "\n",
    "人们通过一维张量表示向量。一般来说，张量可以具有任意长度，取决于机器的内存限制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf03cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a864696",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x = torch.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884a21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "x = tf.range(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x = paddle.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be29a31",
   "metadata": {},
   "source": [
    "我们可以使用下标来引用向量的任一元素，例如可以通过$x_i$来引用第$i$个元素。\n",
    "注意，元素$x_i$是一个标量，所以我们在引用它时不会加粗。\n",
    "大量文献认为列向量是向量的默认方向，在本书中也是如此。\n",
    "在数学中，向量$\\mathbf{x}$可以写为：\n",
    "\n",
    "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n",
    ":eqlabel:`eq_vec_def`\n",
    "\n",
    "其中$x_1,\\ldots,x_n$是向量的元素。在代码中，我们(**通过张量的索引来访问任一元素**)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f53bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00441609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2775f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271c173",
   "metadata": {},
   "source": [
    "### 长度、维度和形状\n",
    "\n",
    "向量只是一个数字数组，就像每个数组都有一个长度一样，每个向量也是如此。\n",
    "在数学表示法中，如果我们想说一个向量$\\mathbf{x}$由$n$个实值标量组成，\n",
    "可以将其表示为$\\mathbf{x}\\in\\mathbb{R}^n$。\n",
    "向量的长度通常称为向量的*维度*（dimension）。\n",
    "\n",
    "与普通的Python数组一样，我们可以通过调用Python的内置`len()`函数来[**访问张量的长度**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eccbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce62f2",
   "metadata": {},
   "source": [
    "当用张量表示一个向量（只有一个轴）时，我们也可以通过`.shape`属性访问向量的长度。\n",
    "形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。\n",
    "对于(**只有一个轴的张量，形状只有一个元素。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e6756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf72da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a56a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d0114",
   "metadata": {},
   "source": [
    "请注意，*维度*（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。\n",
    "为了清楚起见，我们在此明确一下：\n",
    "*向量*或*轴*的维度被用来表示*向量*或*轴*的长度，即向量或轴的元素数量。\n",
    "然而，张量的维度用来表示张量具有的轴数。\n",
    "在这个意义上，张量的某个轴的维数就是这个轴的长度。\n",
    "\n",
    "## 矩阵\n",
    "\n",
    "正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。\n",
    "矩阵，我们通常用粗体、大写字母来表示\n",
    "（例如，$\\mathbf{X}$、$\\mathbf{Y}$和$\\mathbf{Z}$），\n",
    "在代码中表示为具有两个轴的张量。\n",
    "\n",
    "数学表示法使用$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
    "来表示矩阵$\\mathbf{A}$，其由$m$行和$n$列的实值标量组成。\n",
    "我们可以将任意矩阵$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$视为一个表格，\n",
    "其中每个元素$a_{ij}$属于第$i$行第$j$列：\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n",
    ":eqlabel:`eq_matrix_def`\n",
    "\n",
    "对于任意$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$，\n",
    "$\\mathbf{A}$的形状是（$m$,$n$）或$m \\times n$。\n",
    "当矩阵具有相同数量的行和列时，其形状将变为正方形；\n",
    "因此，它被称为*方阵*（square matrix）。\n",
    "\n",
    "当调用函数来实例化张量时，\n",
    "我们可以[**通过指定两个分量$m$和$n$来创建一个形状为$m \\times n$的矩阵**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c7a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4194f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c90910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "A = tf.reshape(tf.range(20), (5, 4))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A = paddle.reshape(paddle.arange(20), (5, 4))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d0727",
   "metadata": {},
   "source": [
    "我们可以通过行索引（$i$）和列索引（$j$）来访问矩阵中的标量元素$a_{ij}$，\n",
    "例如$[\\mathbf{A}]_{ij}$。\n",
    "如果没有给出矩阵$\\mathbf{A}$的标量元素，如在 :eqref:`eq_matrix_def`那样，\n",
    "我们可以简单地使用矩阵$\\mathbf{A}$的小写字母索引下标$a_{ij}$\n",
    "来引用$[\\mathbf{A}]_{ij}$。\n",
    "为了表示起来简单，只有在必要时才会将逗号插入到单独的索引中，\n",
    "例如$a_{2,3j}$和$[\\mathbf{A}]_{2i-1,3}$。\n",
    "\n",
    "当我们交换矩阵的行和列时，结果称为矩阵的*转置*（transpose）。\n",
    "通常用$\\mathbf{a}^\\top$来表示矩阵的转置，如果$\\mathbf{B}=\\mathbf{A}^\\top$，\n",
    "则对于任意$i$和$j$，都有$b_{ij}=a_{ji}$。\n",
    "因此，在 :eqref:`eq_matrix_def`中的转置是一个形状为$n \\times m$的矩阵：\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^\\top =\n",
    "\\begin{bmatrix}\n",
    "    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n",
    "    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    a_{1n} & a_{2n} & \\dots  & a_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "现在在代码中访问(**矩阵的转置**)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a76cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.transpose(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e1382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.transpose(A, perm=[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de1bc2",
   "metadata": {},
   "source": [
    "作为方阵的一种特殊类型，[***对称矩阵*（symmetric matrix）$\\mathbf{A}$等于其转置：$\\mathbf{A} = \\mathbf{A}^\\top$**]。\n",
    "这里定义一个对称矩阵$\\mathbf{B}$："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "B = tf.constant([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "B = paddle.to_tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c33d1b",
   "metadata": {},
   "source": [
    "现在我们将`B`与它的转置进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92909da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "B == B.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56530f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "B == B.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa843141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "B == tf.transpose(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "B == paddle.transpose(B, perm=[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acdc78e",
   "metadata": {},
   "source": [
    "矩阵是有用的数据结构：它们允许我们组织具有不同模式的数据。\n",
    "例如，我们矩阵中的行可能对应于不同的房屋（数据样本），而列可能对应于不同的属性。\n",
    "曾经使用过电子表格软件或已阅读过 :numref:`sec_pandas`的人，应该对此很熟悉。\n",
    "因此，尽管单个向量的默认方向是列向量，但在表示表格数据集的矩阵中，\n",
    "将每个数据样本作为矩阵中的行向量更为常见。\n",
    "后面的章节将讲到这点，这种约定将支持常见的深度学习实践。\n",
    "例如，沿着张量的最外轴，我们可以访问或遍历小批量的数据样本。\n",
    "\n",
    "\n",
    "## 张量\n",
    "\n",
    "[**就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构**]。\n",
    "张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的$n$维数组的通用方法。\n",
    "例如，向量是一阶张量，矩阵是二阶张量。\n",
    "张量用特殊字体的大写字母表示（例如，$\\mathsf{X}$、$\\mathsf{Y}$和$\\mathsf{Z}$），\n",
    "它们的索引机制（例如$x_{ijk}$和$[\\mathsf{X}]_{1,2i-1,3}$）与矩阵类似。\n",
    "\n",
    "当我们开始处理图像时，张量将变得更加重要，图像以$n$维数组形式出现，\n",
    "其中3个轴对应于高度、宽度，以及一个*通道*（channel）轴，\n",
    "用于表示颜色通道（红色、绿色和蓝色）。\n",
    "现在先将高阶张量暂放一边，而是专注学习其基础知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ecd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(24).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf26df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "X = tf.reshape(tf.range(24), (2, 3, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878042e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "X = paddle.reshape(paddle.arange(24), (2, 3, 4))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a55adc6",
   "metadata": {},
   "source": [
    "## 张量算法的基本性质\n",
    "\n",
    "标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。\n",
    "例如，从按元素操作的定义中可以注意到，任何按元素的一元运算都不会改变其操作数的形状。\n",
    "同样，[**给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量**]。\n",
    "例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9536b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(20).reshape(5, 4)\n",
    "B = A.copy()  # 通过分配新内存，将A的一个副本分配给B\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4809a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()  # 通过分配新内存，将A的一个副本分配给B\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dfe135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "A = tf.reshape(tf.range(20, dtype=tf.float32), (5, 4))\n",
    "B = A  # 不能通过分配新内存将A克隆到B\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A = paddle.reshape(paddle.arange(20, dtype=paddle.float32), (5, 4))\n",
    "B = A.clone()  # 通过分配新内存，将A的一个副本分配给B\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e6446",
   "metadata": {},
   "source": [
    "具体而言，[**两个矩阵的按元素乘法称为*Hadamard积*（Hadamard product）（数学符号$\\odot$）**]。\n",
    "对于矩阵$\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$，\n",
    "其中第$i$行和第$j$列的元素是$b_{ij}$。\n",
    "矩阵$\\mathbf{A}$（在 :eqref:`eq_matrix_def`中定义）和$\\mathbf{B}$的Hadamard积为：\n",
    "$$\n",
    "\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f9480",
   "metadata": {},
   "outputs": [],
   "source": [
    "A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b6bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bad64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c7eab",
   "metadata": {},
   "source": [
    "将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671ecfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "X = np.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b96ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8175835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "a = 2\n",
    "X = tf.reshape(tf.range(24), (2, 3, 4))\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b319ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "a = 2\n",
    "X = paddle.reshape(paddle.arange(24), (2, 3, 4))\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8fdad",
   "metadata": {},
   "source": [
    "## 降维\n",
    "\n",
    ":label:`subseq_lin-alg-reduction`\n",
    "\n",
    "我们可以对任意张量进行的一个有用的操作是[**计算其元素的和**]。\n",
    "数学表示法使用$\\sum$符号表示求和。\n",
    "为了表示长度为$d$的向量中元素的总和，可以记为$\\sum_{i=1}^dx_i$。\n",
    "在代码中可以调用计算求和的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3088b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(4)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2590362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "x = tf.range(4, dtype=tf.float32)\n",
    "x, tf.reduce_sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442dd1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x = paddle.arange(4, dtype=paddle.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daef528",
   "metadata": {},
   "source": [
    "我们可以(**表示任意形状张量的元素和**)。\n",
    "例如，矩阵$\\mathbf{A}$中元素的和可以记为$\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce807f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff995795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "A.shape, tf.reduce_sum(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f6deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5ab00",
   "metadata": {},
   "source": [
    "默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。\n",
    "我们还可以[**指定张量沿哪一个轴来通过求和降低维度**]。\n",
    "以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定`axis=0`。\n",
    "由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bfcc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9424740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "A_sum_axis0 = tf.reduce_sum(A, axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ca2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee73fc",
   "metadata": {},
   "source": [
    "指定`axis=1`将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0238ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6911d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "A_sum_axis1 = tf.reduce_sum(A, axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a806c09",
   "metadata": {},
   "source": [
    "沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23edd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.sum(axis=[0, 1])  # 结果和A.sum()相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c554c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A.sum(axis=[0, 1])  # 结果和A.sum()相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.reduce_sum(A, axis=[0, 1])  # 结果和tf.reduce_sum(A)相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A.sum(axis=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef5d36",
   "metadata": {},
   "source": [
    "[**一个与求和相关的量是*平均值*（mean或average）**]。\n",
    "我们通过将总和除以元素总数来计算平均值。\n",
    "在代码中，我们可以调用函数来计算任意形状张量的平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.mean(), A.sum() / A.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb865a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.reduce_mean(A), tf.reduce_sum(A) / tf.size(A).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08942c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3883962f",
   "metadata": {},
   "source": [
    "同样，计算平均值的函数也可以沿指定轴降低张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcedaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88396a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.reduce_mean(A, axis=0), tf.reduce_sum(A, axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342ac85",
   "metadata": {},
   "source": [
    "### 非降维求和\n",
    "\n",
    ":label:`subseq_lin-alg-non-reduction`\n",
    "\n",
    "但是，有时在调用函数来[**计算总和或均值时保持轴数不变**]会很有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "sum_A = tf.reduce_sum(A, axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76438e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "sum_A = paddle.sum(A, axis=1, keepdim=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a629f1",
   "metadata": {},
   "source": [
    "例如，由于`sum_A`在对每行进行求和后仍保持两个轴，我们可以(**通过广播将`A`除以`sum_A`**)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68435f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e081aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A / sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd790fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "A / sum_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b67dc",
   "metadata": {},
   "source": [
    "如果我们想沿[**某个轴计算`A`元素的累积总和**]，\n",
    "比如`axis=0`（按行计算），可以调用`cumsum`函数。\n",
    "此函数不会沿任何轴降低输入张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678d7386",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac7ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5b00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.cumsum(A, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a17f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c8e85",
   "metadata": {},
   "source": [
    "## 点积（Dot Product）\n",
    "\n",
    "我们已经学习了按元素操作、求和及平均值。\n",
    "另一个最基本的操作之一是点积。\n",
    "给定两个向量$\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^d$，\n",
    "它们的*点积*（dot product）$\\mathbf{x}^\\top\\mathbf{y}$\n",
    "（或$\\langle\\mathbf{x},\\mathbf{y}\\rangle$）\n",
    "是相同位置的按元素乘积的和：$\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$。\n",
    "\n",
    "[~~点积是相同位置的按元素乘积的和~~]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f44a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.ones(4)\n",
    "x, y, np.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "y = torch.ones(4, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d827db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "y = tf.ones(4, dtype=tf.float32)\n",
    "x, y, tf.tensordot(x, y, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41479817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "y = paddle.ones(shape=[4], dtype='float32')\n",
    "x, y, paddle.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b09424",
   "metadata": {},
   "source": [
    "注意，(**我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积**)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95610c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8182aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.reduce_sum(x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d6989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a160355",
   "metadata": {},
   "source": [
    "点积在很多场合都很有用。\n",
    "例如，给定一组由向量$\\mathbf{x} \\in \\mathbb{R}^d$表示的值，\n",
    "和一组由$\\mathbf{w} \\in \\mathbb{R}^d$表示的权重。\n",
    "$\\mathbf{x}$中的值根据权重$\\mathbf{w}$的加权和，\n",
    "可以表示为点积$\\mathbf{x}^\\top \\mathbf{w}$。\n",
    "当权重为非负数且和为1（即$\\left(\\sum_{i=1}^{d}{w_i}=1\\right)$）时，\n",
    "点积表示*加权平均*（weighted average）。\n",
    "将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。\n",
    "本节后面的内容将正式介绍*长度*（length）的概念。\n",
    "\n",
    "## 矩阵-向量积\n",
    "\n",
    "现在我们知道如何计算点积，可以开始理解*矩阵-向量积*（matrix-vector product）。\n",
    "回顾分别在 :eqref:`eq_matrix_def`和 :eqref:`eq_vec_def`中定义的矩阵$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$和向量$\\mathbf{x} \\in \\mathbb{R}^n$。\n",
    "让我们将矩阵$\\mathbf{A}$用它的行向量表示：\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "其中每个$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$都是行向量，表示矩阵的第$i$行。\n",
    "[**矩阵向量积$\\mathbf{A}\\mathbf{x}$是一个长度为$m$的列向量，\n",
    "其第$i$个元素是点积$\\mathbf{a}^\\top_i \\mathbf{x}$**]：\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "我们可以把一个矩阵$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$乘法看作一个从$\\mathbb{R}^{n}$到$\\mathbb{R}^{m}$向量的转换。\n",
    "这些转换是非常有用的，例如可以用方阵的乘法来表示旋转。\n",
    "后续章节将讲到，我们也可以使用矩阵-向量积来描述在给定前一层的值时，\n",
    "求解神经网络每一层所需的复杂计算。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "在代码中使用张量表示矩阵-向量积，我们使用与点积相同的`dot`函数。\n",
    "当我们为矩阵`A`和向量`x`调用`np.dot(A,x)`时，会执行矩阵-向量积。\n",
    "注意，`A`的列维数（沿轴1的长度）必须与`x`的维数（其长度）相同。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "在代码中使用张量表示矩阵-向量积，我们使用`mv`函数。\n",
    "当我们为矩阵`A`和向量`x`调用`torch.mv(A, x)`时，会执行矩阵-向量积。\n",
    "注意，`A`的列维数（沿轴1的长度）必须与`x`的维数（其长度）相同。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "在代码中使用张量表示矩阵-向量积，我们使用与点积相同的`matvec`函数。\n",
    "当我们为矩阵`A`和向量`x`调用`tf.linalg.matvec(A, x)`时，会执行矩阵-向量积。\n",
    "注意，`A`的列维数（沿轴1的长度）必须与`x`的维数（其长度）相同。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb601d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape, x.shape, np.dot(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8361f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "A.shape, x.shape, tf.linalg.matvec(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ccc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "A.shape, x.shape, paddle.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c3b4f",
   "metadata": {},
   "source": [
    "## 矩阵-矩阵乘法\n",
    "\n",
    "在掌握点积和矩阵-向量积的知识后，\n",
    "那么**矩阵-矩阵乘法**（matrix-matrix multiplication）应该很简单。\n",
    "\n",
    "假设有两个矩阵$\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$和$\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$：\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{B}=\\begin{bmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "用行向量$\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$表示矩阵$\\mathbf{A}$的第$i$行，并让列向量$\\mathbf{b}_{j} \\in \\mathbb{R}^k$作为矩阵$\\mathbf{B}$的第$j$列。要生成矩阵积$\\mathbf{C} = \\mathbf{A}\\mathbf{B}$，最简单的方法是考虑$\\mathbf{A}$的行向量和$\\mathbf{B}$的列向量:\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad \\mathbf{B}=\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "当我们简单地将每个元素$c_{ij}$计算为点积$\\mathbf{a}^\\top_i \\mathbf{b}_j$:\n",
    "\n",
    "$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "[**我们可以将矩阵-矩阵乘法$\\mathbf{AB}$看作简单地执行$m$次矩阵-向量积，并将结果拼接在一起，形成一个$n \\times m$矩阵**]。\n",
    "在下面的代码中，我们在`A`和`B`上执行矩阵乘法。\n",
    "这里的`A`是一个5行4列的矩阵，`B`是一个4行3列的矩阵。\n",
    "两者相乘后，我们得到了一个5行3列的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.ones(shape=(4, 3))\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f52390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877650c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "B = tf.ones((4, 3), tf.float32)\n",
    "tf.matmul(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302983f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "B = paddle.ones(shape=[4, 3], dtype='float32')\n",
    "paddle.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efbe75",
   "metadata": {},
   "source": [
    "矩阵-矩阵乘法可以简单地称为**矩阵乘法**，不应与\"Hadamard积\"混淆。\n",
    "\n",
    "## 范数\n",
    ":label:`subsec_lin-algebra-norms`\n",
    "\n",
    "线性代数中最有用的一些运算符是*范数*（norm）。\n",
    "非正式地说，向量的*范数*是表示一个向量有多大。\n",
    "这里考虑的*大小*（size）概念不涉及维度，而是分量的大小。\n",
    "\n",
    "在线性代数中，向量范数是将向量映射到标量的函数$f$。\n",
    "给定任意向量$\\mathbf{x}$，向量范数要满足一些属性。\n",
    "第一个性质是：如果我们按常数因子$\\alpha$缩放向量的所有元素，\n",
    "其范数也会按相同常数因子的*绝对值*缩放：\n",
    "\n",
    "$$f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).$$\n",
    "\n",
    "第二个性质是熟悉的三角不等式:\n",
    "\n",
    "$$f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).$$\n",
    "\n",
    "第三个性质简单地说范数必须是非负的:\n",
    "\n",
    "$$f(\\mathbf{x}) \\geq 0.$$\n",
    "\n",
    "这是有道理的。因为在大多数情况下，任何东西的最小的*大小*是0。\n",
    "最后一个性质要求范数最小为0，当且仅当向量全由0组成。\n",
    "\n",
    "$$\\forall i, [\\mathbf{x}]_i = 0 \\Leftrightarrow f(\\mathbf{x})=0.$$\n",
    "\n",
    "范数听起来很像距离的度量。\n",
    "欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些启发。\n",
    "事实上，欧几里得距离是一个$L_2$范数：\n",
    "假设$n$维向量$\\mathbf{x}$中的元素是$x_1,\\ldots,x_n$，其[**$L_2$*范数*是向量元素平方和的平方根：**]\n",
    "\n",
    "(**$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$**)\n",
    "\n",
    "其中，在$L_2$范数中常常省略下标$2$，也就是说$\\|\\mathbf{x}\\|$等同于$\\|\\mathbf{x}\\|_2$。\n",
    "在代码中，我们可以按如下方式计算向量的$L_2$范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709e54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([3, -4])\n",
    "np.linalg.norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6556c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "u = tf.constant([3.0, -4.0])\n",
    "tf.norm(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1586ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "u = paddle.to_tensor([3.0, -4.0])\n",
    "paddle.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62945e61",
   "metadata": {},
   "source": [
    "深度学习中更经常地使用$L_2$范数的平方，也会经常遇到[**$L_1$范数，它表示为向量元素的绝对值之和：**]\n",
    "\n",
    "(**$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$**)\n",
    "\n",
    "与$L_2$范数相比，$L_1$范数受异常值的影响较小。\n",
    "为了计算$L_1$范数，我们将绝对值函数和按元素求和组合起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(u).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2bdc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc55f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.reduce_sum(tf.abs(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179feefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e1dfc",
   "metadata": {},
   "source": [
    "$L_2$范数和$L_1$范数都是更一般的$L_p$范数的特例：\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n",
    "\n",
    "类似于向量的$L_2$范数，[**矩阵**]$\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$(**的*Frobenius范数*（Frobenius norm）是矩阵元素平方和的平方根：**)\n",
    "\n",
    "(**$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$**)\n",
    "\n",
    "Frobenius范数满足向量范数的所有性质，它就像是矩阵形向量的$L_2$范数。\n",
    "调用以下函数将计算矩阵的Frobenius范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(np.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb115d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30472c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.norm(tf.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d543cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.norm(paddle.ones(shape=[4, 9], dtype='float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70006db",
   "metadata": {},
   "source": [
    "### 范数和目标\n",
    "\n",
    ":label:`subsec_norms_and_objectives`\n",
    "\n",
    "在深度学习中，我们经常试图解决优化问题：\n",
    "*最大化*分配给观测数据的概率;\n",
    "*最小化*预测和真实观测之间的距离。\n",
    "用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。\n",
    "目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。\n",
    "\n",
    "## 关于线性代数的更多信息\n",
    "\n",
    "仅用一节，我们就教会了阅读本书所需的、用以理解现代深度学习的线性代数。\n",
    "线性代数还有很多，其中很多数学对于机器学习非常有用。\n",
    "例如，矩阵可以分解为因子，这些分解可以显示真实世界数据集中的低维结构。\n",
    "机器学习的整个子领域都侧重于使用矩阵分解及其向高阶张量的泛化，来发现数据集中的结构并解决预测问题。\n",
    "当开始动手尝试并在真实数据集上应用了有效的机器学习模型，你会更倾向于学习更多数学。\n",
    "因此，这一节到此结束，本书将在后面介绍更多数学知识。\n",
    "\n",
    "如果渴望了解有关线性代数的更多信息，可以参考[线性代数运算的在线附录](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html)或其他优秀资源 :cite:`Strang.1993,Kolter.2008,Petersen.Pedersen.ea.2008`。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 标量、向量、矩阵和张量是线性代数中的基本数学对象。\n",
    "* 向量泛化自标量，矩阵泛化自向量。\n",
    "* 标量、向量、矩阵和张量分别具有零、一、二和任意数量的轴。\n",
    "* 一个张量可以通过`sum`和`mean`沿指定的轴降低维度。\n",
    "* 两个矩阵的按元素乘法被称为他们的Hadamard积。它与矩阵乘法不同。\n",
    "* 在深度学习中，我们经常使用范数，如$L_1$范数、$L_2$范数和Frobenius范数。\n",
    "* 我们可以对标量、向量、矩阵和张量执行各种操作。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 证明一个矩阵$\\mathbf{A}$的转置的转置是$\\mathbf{A}$，即$(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$。\n",
    "1. 给出两个矩阵$\\mathbf{A}$和$\\mathbf{B}$，证明“它们转置的和”等于“它们和的转置”，即$\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$。\n",
    "1. 给定任意方阵$\\mathbf{A}$，$\\mathbf{A} + \\mathbf{A}^\\top$总是对称的吗?为什么?\n",
    "1. 本节中定义了形状$(2,3,4)$的张量`X`。`len(X)`的输出结果是什么？\n",
    "1. 对于任意形状的张量`X`,`len(X)`是否总是对应于`X`特定轴的长度?这个轴是什么?\n",
    "1. 运行`A/A.sum(axis=1)`，看看会发生什么。请分析一下原因？\n",
    "1. 考虑一个具有形状$(2,3,4)$的张量，在轴0、1、2上的求和输出是什么形状?\n",
    "1. 为`linalg.norm`函数提供3个或更多轴的张量，并观察其输出。对于任意形状的张量这个函数计算得到什么?\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/1752)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1751)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1753)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11682)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb38ceb",
   "metadata": {},
   "source": [
    "# 微积分\n",
    ":label:`sec_calculus`\n",
    "\n",
    "在2500年前，古希腊人把一个多边形分成三角形，并把它们的面积相加，才找到计算多边形面积的方法。\n",
    "为了求出曲线形状（比如圆）的面积，古希腊人在这样的形状上刻内接多边形。\n",
    "如 :numref:`fig_circle_area`所示，内接多边形的等长边越多，就越接近圆。\n",
    "这个过程也被称为*逼近法*（method of exhaustion）。\n",
    "\n",
    "![用逼近法求圆的面积](../img/polygon-circle.svg)\n",
    ":label:`fig_circle_area`\n",
    "\n",
    "事实上，逼近法就是*积分*（integral calculus）的起源。\n",
    "2000多年后，微积分的另一支，*微分*（differential calculus）被发明出来。\n",
    "在微分学最重要的应用是优化问题，即考虑如何把事情做到最好。\n",
    "正如在 :numref:`subsec_norms_and_objectives`中讨论的那样，\n",
    "这种问题在深度学习中是无处不在的。\n",
    "\n",
    "在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。\n",
    "通常情况下，变得更好意味着最小化一个*损失函数*（loss function），\n",
    "即一个衡量“模型有多糟糕”这个问题的分数。\n",
    "最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。\n",
    "但“训练”模型只能将模型与我们实际能看到的数据相拟合。\n",
    "因此，我们可以将拟合模型的任务分解为两个关键问题：\n",
    "\n",
    "* *优化*（optimization）：用模型拟合观测数据的过程；\n",
    "* *泛化*（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。\n",
    "\n",
    "为了帮助读者在后面的章节中更好地理解优化问题和方法，\n",
    "本节提供了一个非常简短的入门教程，帮助读者快速掌握深度学习中常用的微分知识。\n",
    "\n",
    "## 导数和微分\n",
    "\n",
    "我们首先讨论导数的计算，这是几乎所有深度学习优化算法的关键步骤。\n",
    "在深度学习中，我们通常选择对于模型参数可微的损失函数。\n",
    "简而言之，对于每个参数，\n",
    "如果我们把这个参数*增加*或*减少*一个无穷小的量，可以知道损失会以多快的速度增加或减少，\n",
    "\n",
    "假设我们有一个函数$f: \\mathbb{R} \\rightarrow \\mathbb{R}$，其输入和输出都是标量。\n",
    "(**如果$f$的*导数*存在，这个极限被定义为**)\n",
    "\n",
    "(**$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}.$$**)\n",
    ":eqlabel:`eq_derivative`\n",
    "\n",
    "如果$f'(a)$存在，则称$f$在$a$处是*可微*（differentiable）的。\n",
    "如果$f$在一个区间内的每个数上都是可微的，则此函数在此区间中是可微的。\n",
    "我们可以将 :eqref:`eq_derivative`中的导数$f'(x)$解释为$f(x)$相对于$x$的*瞬时*（instantaneous）变化率。\n",
    "所谓的瞬时变化率是基于$x$中的变化$h$，且$h$接近$0$。\n",
    "\n",
    "为了更好地解释导数，让我们做一个实验。\n",
    "(**定义$u=f(x)=3x^2-4x$**)如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33781008",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from matplotlib_inline import backend_inline\n",
    "from mxnet import np, npx\n",
    "npx.set_np()\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "from matplotlib_inline import backend_inline\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "%matplotlib inline\n",
    "from d2l import tensorflow as d2l\n",
    "from matplotlib_inline import backend_inline\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9434483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "from matplotlib_inline import backend_inline\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6640172",
   "metadata": {},
   "source": [
    "[**通过令$x=1$并让$h$接近$0$，**] :eqref:`eq_derivative`中(**$\\frac{f(x+h)-f(x)}{h}$的数值结果接近$2$**)。\n",
    "虽然这个实验不是一个数学证明，但稍后会看到，当$x=1$时，导数$u'$是$2$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf87ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def numerical_lim(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "h = 0.1\n",
    "for i in range(5):\n",
    "    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n",
    "    h *= 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd3757",
   "metadata": {},
   "source": [
    "让我们熟悉一下导数的几个等价符号。\n",
    "给定$y=f(x)$，其中$x$和$y$分别是函数$f$的自变量和因变量。以下表达式是等价的：\n",
    "\n",
    "$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n",
    "\n",
    "其中符号$\\frac{d}{dx}$和$D$是*微分运算符*，表示*微分*操作。\n",
    "我们可以使用以下规则来对常见函数求微分：\n",
    "\n",
    "* $DC = 0$（$C$是一个常数）\n",
    "* $Dx^n = nx^{n-1}$（*幂律*（power rule），$n$是任意实数）\n",
    "* $De^x = e^x$\n",
    "* $D\\ln(x) = 1/x$\n",
    "\n",
    "为了微分一个由一些常见函数组成的函数，下面的一些法则方便使用。\n",
    "假设函数$f$和$g$都是可微的，$C$是一个常数，则：\n",
    "\n",
    "*常数相乘法则*\n",
    "$$\\frac{d}{dx} [Cf(x)] = C \\frac{d}{dx} f(x),$$\n",
    "\n",
    "*加法法则*\n",
    "\n",
    "$$\\frac{d}{dx} [f(x) + g(x)] = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x),$$\n",
    "\n",
    "*乘法法则*\n",
    "\n",
    "$$\\frac{d}{dx} [f(x)g(x)] = f(x) \\frac{d}{dx} [g(x)] + g(x) \\frac{d}{dx} [f(x)],$$\n",
    "\n",
    "*除法法则*\n",
    "\n",
    "$$\\frac{d}{dx} \\left[\\frac{f(x)}{g(x)}\\right] = \\frac{g(x) \\frac{d}{dx} [f(x)] - f(x) \\frac{d}{dx} [g(x)]}{[g(x)]^2}.$$\n",
    "\n",
    "现在我们可以应用上述几个法则来计算$u'=f'(x)=3\\frac{d}{dx}x^2-4\\frac{d}{dx}x=6x-4$。\n",
    "令$x=1$，我们有$u'=2$：在这个实验中，数值结果接近$2$，\n",
    "这一点得到了在本节前面的实验的支持。\n",
    "当$x=1$时，此导数也是曲线$u=f(x)$切线的斜率。\n",
    "\n",
    "[**为了对导数的这种解释进行可视化，我们将使用`matplotlib`**]，\n",
    "这是一个Python中流行的绘图库。\n",
    "要配置`matplotlib`生成图形的属性，我们需要(**定义几个函数**)。\n",
    "在下面，`use_svg_display`函数指定`matplotlib`软件包输出svg图表以获得更清晰的图像。\n",
    "\n",
    "注意，注释`#@save`是一个特殊的标记，会将对应的函数、类或语句保存在`d2l`包中。\n",
    "因此，以后无须重新定义就可以直接调用它们（例如，`d2l.use_svg_display()`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ebef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def use_svg_display():  #@save\n",
    "    \"\"\"使用svg格式在Jupyter中显示绘图\"\"\"\n",
    "    backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca794e1",
   "metadata": {},
   "source": [
    "我们定义`set_figsize`函数来设置图表大小。\n",
    "注意，这里可以直接使用`d2l.plt`，因为导入语句\n",
    "`from matplotlib import pyplot as plt`已标记为保存到`d2l`包中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b447943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def set_figsize(figsize=(3.5, 2.5)):  #@save\n",
    "    \"\"\"设置matplotlib的图表大小\"\"\"\n",
    "    use_svg_display()\n",
    "    d2l.plt.rcParams['figure.figsize'] = figsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d9950",
   "metadata": {},
   "source": [
    "下面的`set_axes`函数用于设置由`matplotlib`生成图表的轴的属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"设置matplotlib的轴\"\"\"\n",
    "    axes.set_xlabel(xlabel)\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale)\n",
    "    axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim)\n",
    "    axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a14e3f",
   "metadata": {},
   "source": [
    "通过这三个用于图形配置的函数，定义一个`plot`函数来简洁地绘制多条曲线，\n",
    "因为我们需要在整个书中可视化许多曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "         ylim=None, xscale='linear', yscale='linear',\n",
    "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
    "    \"\"\"绘制数据点\"\"\"\n",
    "    if legend is None:\n",
    "        legend = []\n",
    "\n",
    "    set_figsize(figsize)\n",
    "    axes = axes if axes else d2l.plt.gca()\n",
    "\n",
    "    # 如果X有一个轴，输出True\n",
    "    def has_one_axis(X):\n",
    "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
    "                and not hasattr(X[0], \"__len__\"))\n",
    "\n",
    "    if has_one_axis(X):\n",
    "        X = [X]\n",
    "    if Y is None:\n",
    "        X, Y = [[]] * len(X), X\n",
    "    elif has_one_axis(Y):\n",
    "        Y = [Y]\n",
    "    if len(X) != len(Y):\n",
    "        X = X * len(Y)\n",
    "    axes.cla()\n",
    "    for x, y, fmt in zip(X, Y, fmts):\n",
    "        if len(x):\n",
    "            axes.plot(x, y, fmt)\n",
    "        else:\n",
    "            axes.plot(y, fmt)\n",
    "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e9c35",
   "metadata": {},
   "source": [
    "现在我们可以[**绘制函数$u=f(x)$及其在$x=1$处的切线$y=2x-3$**]，\n",
    "其中系数$2$是切线的斜率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aaca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "x = np.arange(0, 3, 0.1)\n",
    "plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fde520",
   "metadata": {},
   "source": [
    "## 偏导数\n",
    "\n",
    "到目前为止，我们只讨论了仅含一个变量的函数的微分。\n",
    "在深度学习中，函数通常依赖于许多变量。\n",
    "因此，我们需要将微分的思想推广到*多元函数*（multivariate function）上。\n",
    "\n",
    "设$y = f(x_1, x_2, \\ldots, x_n)$是一个具有$n$个变量的函数。\n",
    "$y$关于第$i$个参数$x_i$的*偏导数*（partial derivative）为：\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n",
    "\n",
    "为了计算$\\frac{\\partial y}{\\partial x_i}$，\n",
    "我们可以简单地将$x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$看作常数，\n",
    "并计算$y$关于$x_i$的导数。\n",
    "对于偏导数的表示，以下是等价的：\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.$$\n",
    "\n",
    "## 梯度\n",
    ":label:`subsec_calculus-grad`\n",
    "\n",
    "我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的*梯度*（gradient）向量。\n",
    "具体而言，设函数$f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$的输入是\n",
    "一个$n$维向量$\\mathbf{x}=[x_1,x_2,\\ldots,x_n]^\\top$，并且输出是一个标量。\n",
    "函数$f(\\mathbf{x})$相对于$\\mathbf{x}$的梯度是一个包含$n$个偏导数的向量:\n",
    "\n",
    "$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$$\n",
    "\n",
    "其中$\\nabla_{\\mathbf{x}} f(\\mathbf{x})$通常在没有歧义时被$\\nabla f(\\mathbf{x})$取代。\n",
    "\n",
    "假设$\\mathbf{x}$为$n$维向量，在微分多元函数时经常使用以下规则:\n",
    "\n",
    "* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$\n",
    "* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$\n",
    "* 对于所有$\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$，都有$\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$\n",
    "* $\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$\n",
    "\n",
    "同样，对于任何矩阵$\\mathbf{X}$，都有$\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_F^2 = 2\\mathbf{X}$。\n",
    "正如我们之后将看到的，梯度对于设计深度学习中的优化算法有很大用处。\n",
    "\n",
    "## 链式法则\n",
    "\n",
    "然而，上面方法可能很难找到梯度。\n",
    "这是因为在深度学习中，多元函数通常是*复合*（composite）的，\n",
    "所以难以应用上述任何规则来微分这些函数。\n",
    "幸运的是，链式法则可以被用来微分复合函数。\n",
    "\n",
    "让我们先考虑单变量函数。假设函数$y=f(u)$和$u=g(x)$都是可微的，根据链式法则：\n",
    "\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$$\n",
    "\n",
    "现在考虑一个更一般的场景，即函数具有任意数量的变量的情况。\n",
    "假设可微分函数$y$有变量$u_1, u_2, \\ldots, u_m$，其中每个可微分函数$u_i$都有变量$x_1, x_2, \\ldots, x_n$。\n",
    "注意，$y$是$x_1, x_2， \\ldots, x_n$的函数。\n",
    "对于任意$i = 1, 2, \\ldots, n$，链式法则给出：\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial u_1} \\frac{\\partial u_1}{\\partial x_i} + \\frac{\\partial y}{\\partial u_2} \\frac{\\partial u_2}{\\partial x_i} + \\cdots + \\frac{\\partial y}{\\partial u_m} \\frac{\\partial u_m}{\\partial x_i}$$\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 微分和积分是微积分的两个分支，前者可以应用于深度学习中的优化问题。\n",
    "* 导数可以被解释为函数相对于其变量的瞬时变化率，它也是函数曲线的切线的斜率。\n",
    "* 梯度是一个向量，其分量是多变量函数相对于其所有变量的偏导数。\n",
    "* 链式法则可以用来微分复合函数。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 绘制函数$y = f(x) = x^3 - \\frac{1}{x}$和其在$x = 1$处切线的图像。\n",
    "1. 求函数$f(\\mathbf{x}) = 3x_1^2 + 5e^{x_2}$的梯度。\n",
    "1. 函数$f(\\mathbf{x}) = \\|\\mathbf{x}\\|_2$的梯度是什么？\n",
    "1. 尝试写出函数$u = f(x, y, z)$，其中$x = x(a, b)$，$y = y(a, b)$，$z = z(a, b)$的链式法则。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/1755)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1756)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1754)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11684)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a394788",
   "metadata": {},
   "source": [
    "# 自动微分\n",
    ":label:`sec_autograd`\n",
    "\n",
    "正如 :numref:`sec_calculus`中所说，求导是几乎所有深度学习优化算法的关键步骤。\n",
    "虽然求导的计算很简单，只需要一些基本的微积分。\n",
    "但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。\n",
    "\n",
    "深度学习框架通过自动计算导数，即*自动微分*（automatic differentiation）来加快求导。\n",
    "实际中，根据设计好的模型，系统会构建一个*计算图*（computational graph），\n",
    "来跟踪计算是哪些数据通过哪些操作组合起来产生输出。\n",
    "自动微分使系统能够随后反向传播梯度。\n",
    "这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n",
    "\n",
    "## 一个简单的例子\n",
    "\n",
    "作为一个演示例子，(**假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导**)。\n",
    "首先，我们创建变量`x`并为其分配一个初始值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, np, npx\n",
    "npx.set_np()\n",
    "\n",
    "x = np.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f840bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4985a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.range(4, dtype=tf.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import paddle\n",
    "\n",
    "x = paddle.arange(4, dtype='float32')\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae35961c",
   "metadata": {},
   "source": [
    "[**在我们计算$y$关于$\\mathbf{x}$的梯度之前，需要一个地方来存储梯度。**]\n",
    "重要的是，我们不会在每次对一个参数求导时都分配新的内存。\n",
    "因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。\n",
    "注意，一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e601d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过调用attach_grad来为一个张量的梯度分配内存\n",
    "x.attach_grad()\n",
    "# 在计算关于x的梯度后，将能够通过'grad'属性访问它，它的值被初始化为0\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "x.grad  # 默认值是None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac95156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "x = tf.Variable(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ec04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x = paddle.to_tensor(x, stop_gradient=False)\n",
    "x.grad  # 默认值是None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b66ac",
   "metadata": {},
   "source": [
    "(**现在计算$y$。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c358dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把代码放到autograd.record内，以建立计算图\n",
    "with autograd.record():\n",
    "    y = 2 * np.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89322db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c40ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "# 把所有计算记录在磁带上\n",
    "with tf.GradientTape() as t:\n",
    "    y = 2 * tf.tensordot(x, x, axes=1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd2f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "y = 2 * paddle.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199a958",
   "metadata": {},
   "source": [
    "`x`是一个长度为4的向量，计算`x`和`x`的点积，得到了我们赋值给`y`的标量输出。\n",
    "接下来，[**通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度**]，并打印这些梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afccdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4391d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f919a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "x_grad = t.gradient(y, x)\n",
    "x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d71f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2465fc8e",
   "metadata": {},
   "source": [
    "函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。\n",
    "让我们快速验证这个梯度是否计算正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0440ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35488c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb3d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "x_grad == 4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc75df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb74b3",
   "metadata": {},
   "source": [
    "[**现在计算`x`的另一个函数。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f38fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    y = x.sum()\n",
    "y.backward()\n",
    "x.grad  # 被新计算的梯度覆盖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e743204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5538078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "with tf.GradientTape() as t:\n",
    "    y = tf.reduce_sum(x)\n",
    "t.gradient(y, x)  # 被新计算的梯度覆盖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd04dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "# 在默认情况下，PaddlePaddle会累积梯度，我们需要清除之前的值\n",
    "x.clear_gradient()\n",
    "y = paddle.sum(x)\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017db936",
   "metadata": {},
   "source": [
    "## 非标量变量的反向传播\n",
    "\n",
    "当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。\n",
    "对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n",
    "\n",
    "然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[**深度学习中**]），\n",
    "但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。\n",
    "这里(**，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当对向量值变量y（关于x的函数）调用backward时，将通过对y中的元素求和来创建\n",
    "# 一个新的标量变量。然后计算这个标量变量相对于x的梯度\n",
    "with autograd.record():\n",
    "    y = x * x  # y是一个向量\n",
    "y.backward()\n",
    "x.grad  # 等价于y=sum(x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6729a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
    "# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "y.sum().backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827e16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "with tf.GradientTape() as t:\n",
    "    y = x * x\n",
    "t.gradient(y, x)  # 等价于y=tf.reduce_sum(x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x.clear_gradient()\n",
    "y = x * x\n",
    "paddle.sum(y).backward() \n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5cd15",
   "metadata": {},
   "source": [
    "## 分离计算\n",
    "\n",
    "有时，我们希望[**将某些计算移动到记录的计算图之外**]。\n",
    "例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n",
    "想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数，\n",
    "并且只考虑到`x`在`y`被计算后发挥的作用。\n",
    "\n",
    "这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，\n",
    "但丢弃计算图中如何计算`y`的任何信息。\n",
    "换句话说，梯度不会向后流经`u`到`x`。\n",
    "因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，\n",
    "而不是`z=x*x*x`关于`x`的偏导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1236c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    y = x * x\n",
    "    u = y.detach()\n",
    "    z = u * x\n",
    "z.backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "# 设置persistent=True来运行t.gradient多次\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    y = x * x\n",
    "    u = tf.stop_gradient(y)\n",
    "    z = u * x\n",
    "\n",
    "x_grad = t.gradient(z, x)\n",
    "x_grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b84608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x.clear_gradient()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "paddle.sum(z).backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed8fcfb",
   "metadata": {},
   "source": [
    "由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播，\n",
    "得到`y=x*x`关于的`x`的导数，即`2*x`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58019072",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5edd52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "t.gradient(y, x) == 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7af025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x.clear_gradient()\n",
    "paddle.sum(y).backward()\n",
    "x.grad == 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235da5f",
   "metadata": {},
   "source": [
    "## Python控制流的梯度计算\n",
    "\n",
    "使用自动微分的一个好处是：\n",
    "[**即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度**]。\n",
    "在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b = a * 2\n",
    "    while np.linalg.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c658ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while b.norm() < 1000:\n",
    "        b = b * 2\n",
    "    if b.sum() > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886399ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while tf.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if tf.reduce_sum(b) > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7d0f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def f(a):\n",
    "    b = a * 2\n",
    "    while paddle.norm(b) < 1000:\n",
    "        b = b * 2\n",
    "    if paddle.sum(b) > 0:\n",
    "        c = b\n",
    "    else:\n",
    "        c = 100 * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d1e3b",
   "metadata": {},
   "source": [
    "让我们计算梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal()\n",
    "a.attach_grad()\n",
    "with autograd.record():\n",
    "    d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "a = torch.randn(size=(), requires_grad=True)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aeacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "a = tf.Variable(tf.random.normal(shape=()))\n",
    "with tf.GradientTape() as t:\n",
    "    d = f(a)\n",
    "d_grad = t.gradient(d, a)\n",
    "d_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede0501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "a = paddle.to_tensor(paddle.randn(shape=[1]), stop_gradient=False)\n",
    "d = f(a)\n",
    "d.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2832821d",
   "metadata": {},
   "source": [
    "我们现在可以分析上面定义的`f`函数。\n",
    "请注意，它在其输入`a`中是分段线性的。\n",
    "换言之，对于任何`a`，存在某个常量标量`k`，使得`f(a)=k*a`，其中`k`的值取决于输入`a`，因此可以用`d/a`验证梯度是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe6e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "d_grad == d / a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3637223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "a.grad == d / a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eaf7a7",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 为什么计算二阶导数比一阶导数的开销要更大？\n",
    "1. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
    "1. 在控制流的例子中，我们计算`d`关于`a`的导数，如果将变量`a`更改为随机向量或矩阵，会发生什么？\n",
    "1. 重新设计一个求控制流梯度的例子，运行并分析结果。\n",
    "1. 使$f(x)=\\sin(x)$，绘制$f(x)$和$\\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\\cos(x)$。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/1758)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1759)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1757)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11684)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afd6ab",
   "metadata": {},
   "source": [
    "# 概率\n",
    ":label:`sec_prob`\n",
    "\n",
    "简单地说，机器学习就是做出预测。\n",
    "\n",
    "根据病人的临床病史，我们可能想预测他们在下一年心脏病发作的*概率*。\n",
    "在飞机喷气发动机的异常检测中，我们想要评估一组发动机读数为正常运行情况的概率有多大。\n",
    "在强化学习中，我们希望智能体（agent）能在一个环境中智能地行动。\n",
    "这意味着我们需要考虑在每种可行的行为下获得高奖励的概率。\n",
    "当我们建立推荐系统时，我们也需要考虑概率。\n",
    "例如，假设我们为一家大型在线书店工作，我们可能希望估计某些用户购买特定图书的概率。\n",
    "为此，我们需要使用概率学。\n",
    "有完整的课程、专业、论文、职业、甚至院系，都致力于概率学的工作。\n",
    "所以很自然地，我们在这部分的目标不是教授整个科目。\n",
    "相反，我们希望教给读者基础的概率知识，使读者能够开始构建第一个深度学习模型，\n",
    "以便读者可以开始自己探索它。\n",
    "\n",
    "现在让我们更认真地考虑第一个例子：根据照片区分猫和狗。\n",
    "这听起来可能很简单，但对于机器却可能是一个艰巨的挑战。\n",
    "首先，问题的难度可能取决于图像的分辨率。\n",
    "\n",
    "![不同分辨率的图像 ($10 \\times 10$, $20 \\times 20$, $40 \\times 40$, $80 \\times 80$, 和 $160 \\times 160$ pixels)](../img/cat-dog-pixels.png)\n",
    ":width:`300px`\n",
    ":label:`fig_cat_dog`\n",
    "\n",
    "如 :numref:`fig_cat_dog`所示，虽然人类很容易以$160 \\times 160$像素的分辨率识别猫和狗，\n",
    "但它在$40\\times40$像素上变得具有挑战性，而且在$10 \\times 10$像素下几乎是不可能的。\n",
    "换句话说，我们在很远的距离（从而降低分辨率）区分猫和狗的能力可能会变为猜测。\n",
    "概率给了我们一种正式的途径来说明我们的确定性水平。\n",
    "如果我们完全肯定图像是一只猫，我们说标签$y$是\"猫\"的*概率*，表示为$P(y=$\"猫\"$)$等于$1$。\n",
    "如果我们没有证据表明$y=$“猫”或$y=$“狗”，那么我们可以说这两种可能性是相等的，\n",
    "即$P(y=$\"猫\"$)=P(y=$\"狗\"$)=0.5$。\n",
    "如果我们不十分确定图像描绘的是一只猫，我们可以将概率赋值为$0.5<P(y=$\"猫\"$)<1$。\n",
    "\n",
    "现在考虑第二个例子：给出一些天气监测数据，我们想预测明天北京下雨的概率。\n",
    "如果是夏天，下雨的概率是0.5。\n",
    "\n",
    "在这两种情况下，我们都不确定结果，但这两种情况之间有一个关键区别。\n",
    "在第一种情况中，图像实际上是狗或猫二选一。\n",
    "在第二种情况下，结果实际上是一个随机的事件。\n",
    "因此，概率是一种灵活的语言，用于说明我们的确定程度，并且它可以有效地应用于广泛的领域中。\n",
    "\n",
    "## 基本概率论\n",
    "\n",
    "假设我们掷骰子，想知道看到1的几率有多大，而不是看到另一个数字。\n",
    "如果骰子是公平的，那么所有六个结果$\\{1, \\ldots, 6\\}$都有相同的可能发生，\n",
    "因此我们可以说$1$发生的概率为$\\frac{1}{6}$。\n",
    "\n",
    "然而现实生活中，对于我们从工厂收到的真实骰子，我们需要检查它是否有瑕疵。\n",
    "检查骰子的唯一方法是多次投掷并记录结果。\n",
    "对于每个骰子，我们将观察到$\\{1, \\ldots, 6\\}$中的一个值。\n",
    "对于每个值，一种自然的方法是将它出现的次数除以投掷的总次数，\n",
    "即此*事件*（event）概率的*估计值*。\n",
    "*大数定律*（law of large numbers）告诉我们：\n",
    "随着投掷次数的增加，这个估计值会越来越接近真实的潜在概率。\n",
    "让我们用代码试一试！\n",
    "\n",
    "首先，我们导入必要的软件包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "import random\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f03c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch.distributions import multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6814f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "%matplotlib inline\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc4d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ad95f",
   "metadata": {},
   "source": [
    "在统计学中，我们把从概率分布中抽取样本的过程称为*抽样*（sampling）。\n",
    "笼统来说，可以把*分布*（distribution）看作对事件的概率分配，\n",
    "稍后我们将给出的更正式定义。\n",
    "将概率分配给一些离散选择的分布称为*多项分布*（multinomial distribution）。\n",
    "\n",
    "为了抽取一个样本，即掷骰子，我们只需传入一个概率向量。\n",
    "输出是另一个相同长度的向量：它在索引$i$处的值是采样结果中$i$出现的次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_probs = [1.0 / 6] * 6\n",
    "np.random.multinomial(1, fair_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605137ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "fair_probs = torch.ones([6]) / 6\n",
    "multinomial.Multinomial(1, fair_probs).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee700458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "fair_probs = tf.ones(6) / 6\n",
    "tfp.distributions.Multinomial(1, fair_probs).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "fair_probs = [1.0 / 6] * 6\n",
    "paddle.distribution.Multinomial(1, paddle.to_tensor(fair_probs)).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e24cd11",
   "metadata": {},
   "source": [
    "在估计一个骰子的公平性时，我们希望从同一分布中生成多个样本。\n",
    "如果用Python的for循环来完成这个任务，速度会慢得惊人。\n",
    "因此我们使用深度学习框架的函数同时抽取多个样本，得到我们想要的任意形状的独立样本数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.multinomial(10, fair_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36abf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "multinomial.Multinomial(10, fair_probs).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a59a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tfp.distributions.Multinomial(10, fair_probs).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85926bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.distribution.Multinomial(10, paddle.to_tensor(fair_probs)).sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a4e86",
   "metadata": {},
   "source": [
    "现在我们知道如何对骰子进行采样，我们可以模拟1000次投掷。\n",
    "然后，我们可以统计1000次投掷后，每个数字被投中了多少次。\n",
    "具体来说，我们计算相对频率，以作为真实概率的估计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f36a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.random.multinomial(1000, fair_probs).astype(np.float32)\n",
    "counts / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c758a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "# 将结果存储为32位浮点数以进行除法\n",
    "counts = multinomial.Multinomial(1000, fair_probs).sample()\n",
    "counts / 1000  # 相对频率作为估计值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6805947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "counts = tfp.distributions.Multinomial(1000, fair_probs).sample()\n",
    "counts / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "counts = paddle.distribution.Multinomial(1000, paddle.to_tensor(fair_probs)).sample()\n",
    "counts / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb3f19",
   "metadata": {},
   "source": [
    "因为我们是从一个公平的骰子中生成的数据，我们知道每个结果都有真实的概率$\\frac{1}{6}$，\n",
    "大约是$0.167$，所以上面输出的估计值看起来不错。\n",
    "\n",
    "我们也可以看到这些概率如何随着时间的推移收敛到真实概率。\n",
    "让我们进行500组实验，每组抽取10个样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac585d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.random.multinomial(10, fair_probs, size=500)\n",
    "cum_counts = counts.astype(np.float32).cumsum(axis=0)\n",
    "estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "d2l.set_figsize((6, 4.5))\n",
    "for i in range(6):\n",
    "    d2l.plt.plot(estimates[:, i].asnumpy(),\n",
    "                 label=(\"P(die=\" + str(i + 1) + \")\"))\n",
    "d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\n",
    "d2l.plt.gca().set_xlabel('Groups of experiments')\n",
    "d2l.plt.gca().set_ylabel('Estimated probability')\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31597f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "counts = multinomial.Multinomial(10, fair_probs).sample((500,))\n",
    "cum_counts = counts.cumsum(dim=0)\n",
    "estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)\n",
    "\n",
    "d2l.set_figsize((6, 4.5))\n",
    "for i in range(6):\n",
    "    d2l.plt.plot(estimates[:, i].numpy(),\n",
    "                 label=(\"P(die=\" + str(i + 1) + \")\"))\n",
    "d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\n",
    "d2l.plt.gca().set_xlabel('Groups of experiments')\n",
    "d2l.plt.gca().set_ylabel('Estimated probability')\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a0d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "counts = tfp.distributions.Multinomial(10, fair_probs).sample(500)\n",
    "cum_counts = tf.cumsum(counts, axis=0)\n",
    "estimates = cum_counts / tf.reduce_sum(cum_counts, axis=1, keepdims=True)\n",
    "\n",
    "d2l.set_figsize((6, 4.5))\n",
    "for i in range(6):\n",
    "    d2l.plt.plot(estimates[:, i].numpy(),\n",
    "                 label=(\"P(die=\" + str(i + 1) + \")\"))\n",
    "d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\n",
    "d2l.plt.gca().set_xlabel('Groups of experiments')\n",
    "d2l.plt.gca().set_ylabel('Estimated probability')\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37838181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "counts = paddle.distribution.Multinomial(10, paddle.to_tensor(fair_probs)).sample((500,1))\n",
    "cum_counts = counts.cumsum(axis=0)\n",
    "cum_counts = cum_counts.squeeze(axis=1)\n",
    "estimates = cum_counts / cum_counts.sum(axis=1, keepdim=True)\n",
    "\n",
    "d2l.set_figsize((6, 4.5))\n",
    "for i in range(6):\n",
    "    d2l.plt.plot(estimates[:, i],\n",
    "                 label=(\"P(die=\" + str(i + 1) + \")\"))\n",
    "d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\n",
    "d2l.plt.gca().set_xlabel('Groups of experiments')\n",
    "d2l.plt.gca().set_ylabel('Estimated probability')\n",
    "d2l.plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a3a5e",
   "metadata": {},
   "source": [
    "每条实线对应于骰子的6个值中的一个，并给出骰子在每组实验后出现值的估计概率。\n",
    "当我们通过更多的实验获得更多的数据时，这$6$条实体曲线向真实概率收敛。\n",
    "\n",
    "### 概率论公理\n",
    "\n",
    "在处理骰子掷出时，我们将集合$\\mathcal{S} = \\{1, 2, 3, 4, 5, 6\\}$\n",
    "称为*样本空间*（sample space）或*结果空间*（outcome space），\n",
    "其中每个元素都是*结果*（outcome）。\n",
    "*事件*（event）是一组给定样本空间的随机结果。\n",
    "例如，“看到$5$”（$\\{5\\}$）和“看到奇数”（$\\{1, 3, 5\\}$）都是掷出骰子的有效事件。\n",
    "注意，如果一个随机实验的结果在$\\mathcal{A}$中，则事件$\\mathcal{A}$已经发生。\n",
    "也就是说，如果投掷出$3$点，因为$3 \\in \\{1, 3, 5\\}$，我们可以说，“看到奇数”的事件发生了。\n",
    "\n",
    "*概率*（probability）可以被认为是将集合映射到真实值的函数。\n",
    "在给定的样本空间$\\mathcal{S}$中，事件$\\mathcal{A}$的概率，\n",
    "表示为$P(\\mathcal{A})$，满足以下属性：\n",
    "\n",
    "* 对于任意事件$\\mathcal{A}$，其概率从不会是负数，即$P(\\mathcal{A}) \\geq 0$；\n",
    "* 整个样本空间的概率为$1$，即$P(\\mathcal{S}) = 1$；\n",
    "* 对于*互斥*（mutually exclusive）事件（对于所有$i \\neq j$都有$\\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset$）的任意一个可数序列$\\mathcal{A}_1, \\mathcal{A}_2, \\ldots$，序列中任意一个事件发生的概率等于它们各自发生的概率之和，即$P(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i) = \\sum_{i=1}^{\\infty} P(\\mathcal{A}_i)$。\n",
    "\n",
    "以上也是概率论的公理，由科尔莫戈罗夫于1933年提出。\n",
    "有了这个公理系统，我们可以避免任何关于随机性的哲学争论；\n",
    "相反，我们可以用数学语言严格地推理。\n",
    "例如，假设事件$\\mathcal{A}_1$为整个样本空间，\n",
    "且当所有$i > 1$时的$\\mathcal{A}_i = \\emptyset$，\n",
    "那么我们可以证明$P(\\emptyset) = 0$，即不可能发生事件的概率是$0$。\n",
    "\n",
    "### 随机变量\n",
    "\n",
    "在我们掷骰子的随机实验中，我们引入了*随机变量*（random variable）的概念。\n",
    "随机变量几乎可以是任何数量，并且它可以在随机实验的一组可能性中取一个值。\n",
    "考虑一个随机变量$X$，其值在掷骰子的样本空间$\\mathcal{S}=\\{1,2,3,4,5,6\\}$中。\n",
    "我们可以将事件“看到一个$5$”表示为$\\{X=5\\}$或$X=5$，\n",
    "其概率表示为$P(\\{X=5\\})$或$P(X=5)$。\n",
    "通过$P(X=a)$，我们区分了随机变量$X$和$X$可以采取的值（例如$a$）。\n",
    "然而，这可能会导致繁琐的表示。\n",
    "为了简化符号，一方面，我们可以将$P(X)$表示为随机变量$X$上的*分布*（distribution）：\n",
    "分布告诉我们$X$获得某一值的概率。\n",
    "另一方面，我们可以简单用$P(a)$表示随机变量取值$a$的概率。\n",
    "由于概率论中的事件是来自样本空间的一组结果，因此我们可以为随机变量指定值的可取范围。\n",
    "例如，$P(1 \\leq X \\leq 3)$表示事件$\\{1 \\leq X \\leq 3\\}$，\n",
    "即$\\{X = 1, 2, \\text{or}, 3\\}$的概率。\n",
    "等价地，$P(1 \\leq X \\leq 3)$表示随机变量$X$从$\\{1, 2, 3\\}$中取值的概率。\n",
    "\n",
    "请注意，*离散*（discrete）随机变量（如骰子的每一面）\n",
    "和*连续*（continuous）随机变量（如人的体重和身高）之间存在微妙的区别。\n",
    "现实生活中，测量两个人是否具有完全相同的身高没有太大意义。\n",
    "如果我们进行足够精确的测量，最终会发现这个星球上没有两个人具有完全相同的身高。\n",
    "在这种情况下，询问某人的身高是否落入给定的区间，比如是否在1.79米和1.81米之间更有意义。\n",
    "在这些情况下，我们将这个看到某个数值的可能性量化为*密度*（density）。\n",
    "高度恰好为1.80米的概率为0，但密度不是0。\n",
    "在任何两个不同高度之间的区间，我们都有非零的概率。\n",
    "在本节的其余部分中，我们将考虑离散空间中的概率。\n",
    "连续随机变量的概率可以参考深度学习数学附录中[随机变量](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/random-variables.html)\n",
    "的一节。\n",
    "\n",
    "## 处理多个随机变量\n",
    "\n",
    "很多时候，我们会考虑多个随机变量。\n",
    "比如，我们可能需要对疾病和症状之间的关系进行建模。\n",
    "给定一个疾病和一个症状，比如“流感”和“咳嗽”，以某个概率存在或不存在于某个患者身上。\n",
    "我们需要估计这些概率以及概率之间的关系，以便我们可以运用我们的推断来实现更好的医疗服务。\n",
    "\n",
    "再举一个更复杂的例子：图像包含数百万像素，因此有数百万个随机变量。\n",
    "在许多情况下，图像会附带一个*标签*（label），标识图像中的对象。\n",
    "我们也可以将标签视为一个随机变量。\n",
    "我们甚至可以将所有元数据视为随机变量，例如位置、时间、光圈、焦距、ISO、对焦距离和相机类型。\n",
    "所有这些都是联合发生的随机变量。\n",
    "当我们处理多个随机变量时，会有若干个变量是我们感兴趣的。\n",
    "\n",
    "### 联合概率\n",
    "\n",
    "第一个被称为*联合概率*（joint probability）$P(A=a,B=b)$。\n",
    "给定任意值$a$和$b$，联合概率可以回答：$A=a$和$B=b$同时满足的概率是多少？\n",
    "请注意，对于任何$a$和$b$的取值，$P(A = a, B=b) \\leq P(A=a)$。\n",
    "这点是确定的，因为要同时发生$A=a$和$B=b$，$A=a$就必须发生，$B=b$也必须发生（反之亦然）。因此，$A=a$和$B=b$同时发生的可能性不大于$A=a$或是$B=b$单独发生的可能性。\n",
    "\n",
    "### 条件概率\n",
    "\n",
    "联合概率的不等式带给我们一个有趣的比率：\n",
    "$0 \\leq \\frac{P(A=a, B=b)}{P(A=a)} \\leq 1$。\n",
    "我们称这个比率为*条件概率*（conditional probability），\n",
    "并用$P(B=b \\mid A=a)$表示它：它是$B=b$的概率，前提是$A=a$已发生。\n",
    "\n",
    "### 贝叶斯定理\n",
    "\n",
    "使用条件概率的定义，我们可以得出统计学中最有用的方程之一：\n",
    "*Bayes定理*（Bayes' theorem）。\n",
    "根据*乘法法则*（multiplication rule ）可得到$P(A, B) = P(B \\mid A) P(A)$。\n",
    "根据对称性，可得到$P(A, B) = P(A \\mid B) P(B)$。\n",
    "假设$P(B)>0$，求解其中一个条件变量，我们得到\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}.$$\n",
    "\n",
    "请注意，这里我们使用紧凑的表示法：\n",
    "其中$P(A, B)$是一个*联合分布*（joint distribution），\n",
    "$P(A \\mid B)$是一个*条件分布*（conditional distribution）。\n",
    "这种分布可以在给定值$A = a, B=b$上进行求值。\n",
    "\n",
    "### 边际化\n",
    "\n",
    "为了能进行事件概率求和，我们需要*求和法则*（sum rule），\n",
    "即$B$的概率相当于计算$A$的所有可能选择，并将所有选择的联合概率聚合在一起：\n",
    "\n",
    "$$P(B) = \\sum_{A} P(A, B),$$\n",
    "\n",
    "这也称为*边际化*（marginalization）。\n",
    "边际化结果的概率或分布称为*边际概率*（marginal probability）\n",
    "或*边际分布*（marginal distribution）。\n",
    "\n",
    "### 独立性\n",
    "\n",
    "另一个有用属性是*依赖*（dependence）与*独立*（independence）。\n",
    "如果两个随机变量$A$和$B$是独立的，意味着事件$A$的发生跟$B$事件的发生无关。\n",
    "在这种情况下，统计学家通常将这一点表述为$A \\perp  B$。\n",
    "根据贝叶斯定理，马上就能同样得到$P(A \\mid B) = P(A)$。\n",
    "在所有其他情况下，我们称$A$和$B$依赖。\n",
    "比如，两次连续抛出一个骰子的事件是相互独立的。\n",
    "相比之下，灯开关的位置和房间的亮度并不是（因为可能存在灯泡坏掉、电源故障，或者开关故障）。\n",
    "\n",
    "由于$P(A \\mid B) = \\frac{P(A, B)}{P(B)} = P(A)$等价于$P(A, B) = P(A)P(B)$，\n",
    "因此两个随机变量是独立的，当且仅当两个随机变量的联合分布是其各自分布的乘积。\n",
    "同样地，给定另一个随机变量$C$时，两个随机变量$A$和$B$是*条件独立的*（conditionally independent），\n",
    "当且仅当$P(A, B \\mid C) = P(A \\mid C)P(B \\mid C)$。\n",
    "这个情况表示为$A \\perp B \\mid C$。\n",
    "\n",
    "### 应用\n",
    ":label:`subsec_probability_hiv_app`\n",
    "\n",
    "我们实战演练一下！\n",
    "假设一个医生对患者进行艾滋病病毒（HIV）测试。\n",
    "这个测试是相当准确的，如果患者健康但测试显示他患病，这个概率只有1%；\n",
    "如果患者真正感染HIV，它永远不会检测不出。\n",
    "我们使用$D_1$来表示诊断结果（如果阳性，则为$1$，如果阴性，则为$0$），\n",
    "$H$来表示感染艾滋病病毒的状态（如果阳性，则为$1$，如果阴性，则为$0$）。\n",
    "在 :numref:`conditional_prob_D1`中列出了这样的条件概率。\n",
    "\n",
    ":条件概率为$P(D_1 \\mid H)$\n",
    "\n",
    "| 条件概率 | $H=1$ | $H=0$ |\n",
    "|---|---|---|\n",
    "|$P(D_1 = 1 \\mid H)$|            1 |         0.01 |\n",
    "|$P(D_1 = 0 \\mid H)$|            0 |         0.99 |\n",
    ":label:`conditional_prob_D1`\n",
    "\n",
    "请注意，每列的加和都是1（但每行的加和不是），因为条件概率需要总和为1，就像概率一样。\n",
    "让我们计算如果测试出来呈阳性，患者感染HIV的概率，即$P(H = 1 \\mid D_1 = 1)$。\n",
    "显然，这将取决于疾病有多常见，因为它会影响错误警报的数量。\n",
    "假设人口总体是相当健康的，例如，$P(H=1) = 0.0015$。\n",
    "为了应用贝叶斯定理，我们需要运用边际化和乘法法则来确定\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(D_1 = 1) \\\\\n",
    "=& P(D_1=1, H=0) + P(D_1=1, H=1)  \\\\\n",
    "=& P(D_1=1 \\mid H=0) P(H=0) + P(D_1=1 \\mid H=1) P(H=1) \\\\\n",
    "=& 0.011485.\n",
    "\\end{aligned}\n",
    "$$\n",
    "因此，我们得到\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(H = 1 \\mid D_1 = 1)\\\\ =& \\frac{P(D_1=1 \\mid H=1) P(H=1)}{P(D_1=1)} \\\\ =& 0.1306 \\end{aligned}.$$\n",
    "\n",
    "换句话说，尽管使用了非常准确的测试，患者实际上患有艾滋病的几率只有13.06%。\n",
    "正如我们所看到的，概率可能是违反直觉的。\n",
    "\n",
    "患者在收到这样可怕的消息后应该怎么办？\n",
    "很可能，患者会要求医生进行另一次测试来确定病情。\n",
    "第二个测试具有不同的特性，它不如第一个测试那么精确，\n",
    "如 :numref:`conditional_prob_D2`所示。\n",
    "\n",
    ":条件概率为$P(D_2 \\mid H)$\n",
    "\n",
    "| 条件概率 | $H=1$ | $H=0$ |\n",
    "|---|---|---|\n",
    "|$P(D_2 = 1 \\mid H)$|            0.98 |         0.03 |\n",
    "|$P(D_2 = 0 \\mid H)$|            0.02 |         0.97 |\n",
    ":label:`conditional_prob_D2`\n",
    "\n",
    "不幸的是，第二次测试也显示阳性。让我们通过假设条件独立性来计算出应用Bayes定理的必要概率：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(D_1 = 1, D_2 = 1 \\mid H = 0) \\\\\n",
    "=& P(D_1 = 1 \\mid H = 0) P(D_2 = 1 \\mid H = 0)  \\\\\n",
    "=& 0.0003,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(D_1 = 1, D_2 = 1 \\mid H = 1) \\\\\n",
    "=& P(D_1 = 1 \\mid H = 1) P(D_2 = 1 \\mid H = 1)  \\\\\n",
    "=& 0.98.\n",
    "\\end{aligned}\n",
    "$$\n",
    "现在我们可以应用边际化和乘法规则：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(D_1 = 1, D_2 = 1) \\\\\n",
    "=& P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1)  \\\\\n",
    "=& P(D_1 = 1, D_2 = 1 \\mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 \\mid H = 1)P(H=1)\\\\\n",
    "=& 0.00176955.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "最后，鉴于存在两次阳性检测，患者患有艾滋病的概率为\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(H = 1 \\mid D_1 = 1, D_2 = 1)\\\\\n",
    "=& \\frac{P(D_1 = 1, D_2 = 1 \\mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)} \\\\\n",
    "=& 0.8307.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "也就是说，第二次测试使我们能够对患病的情况获得更高的信心。\n",
    "尽管第二次检验比第一次检验的准确性要低得多，但它仍然显著提高我们的预测概率。\n",
    "\n",
    "## 期望和方差\n",
    "\n",
    "为了概括概率分布的关键特征，我们需要一些测量方法。\n",
    "一个随机变量$X$的*期望*（expectation，或平均值（average））表示为\n",
    "\n",
    "$$E[X] = \\sum_{x} x P(X = x).$$\n",
    "\n",
    "当函数$f(x)$的输入是从分布$P$中抽取的随机变量时，$f(x)$的期望值为\n",
    "\n",
    "$$E_{x \\sim P}[f(x)] = \\sum_x f(x) P(x).$$\n",
    "\n",
    "在许多情况下，我们希望衡量随机变量$X$与其期望值的偏置。这可以通过方差来量化\n",
    "\n",
    "$$\\mathrm{Var}[X] = E\\left[(X - E[X])^2\\right] =\n",
    "E[X^2] - E[X]^2.$$\n",
    "\n",
    "方差的平方根被称为*标准差*（standard deviation）。\n",
    "随机变量函数的方差衡量的是：当从该随机变量分布中采样不同值$x$时，\n",
    "函数值偏离该函数的期望的程度：\n",
    "\n",
    "$$\\mathrm{Var}[f(x)] = E\\left[\\left(f(x) - E[f(x)]\\right)^2\\right].$$\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 我们可以从概率分布中采样。\n",
    "* 我们可以使用联合分布、条件分布、Bayes定理、边缘化和独立性假设来分析多个随机变量。\n",
    "* 期望和方差为概率分布的关键特征的概括提供了实用的度量形式。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 进行$m=500$组实验，每组抽取$n=10$个样本。改变$m$和$n$，观察和分析实验结果。\n",
    "2. 给定两个概率为$P(\\mathcal{A})$和$P(\\mathcal{B})$的事件，计算$P(\\mathcal{A} \\cup \\mathcal{B})$和$P(\\mathcal{A} \\cap \\mathcal{B})$的上限和下限。（提示：使用[友元图](https://en.wikipedia.org/wiki/Venn_diagram)来展示这些情况。)\n",
    "3. 假设我们有一系列随机变量，例如$A$、$B$和$C$，其中$B$只依赖于$A$，而$C$只依赖于$B$，能简化联合概率$P(A, B, C)$吗？（提示：这是一个[马尔可夫链](https://en.wikipedia.org/wiki/Markov_chain)。)\n",
    "4. 在 :numref:`subsec_probability_hiv_app`中，第一个测试更准确。为什么不运行第一个测试两次，而是同时运行第一个和第二个测试?\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/1761)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1762)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1760)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11685)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb969fee",
   "metadata": {},
   "source": [
    "# 查阅文档\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "由于篇幅限制，本书不可能介绍每一个MXNet函数和类。\n",
    "API文档、其他教程和示例提供了本书之外的大量文档。\n",
    "本节提供了一些查看MXNet API的指导。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "由于篇幅限制，本书不可能介绍每一个PyTorch函数和类。\n",
    "API文档、其他教程和示例提供了本书之外的大量文档。\n",
    "本节提供了一些查看PyTorch API的指导。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "由于篇幅限制，本书不可能介绍每一个TensorFlow函数和类。\n",
    "API文档、其他教程和示例提供了本书之外的大量文档。\n",
    "本节提供了一些查TensorFlow API的指导。\n",
    ":end_tab:\n",
    "\n",
    "## 查找模块中的所有函数和类\n",
    "\n",
    "为了知道模块中可以调用哪些函数和类，可以调用`dir`函数。\n",
    "例如，我们可以(**查询随机数生成模块中的所有属性：**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f25970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np\n",
    "print(dir(np.random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e094098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import torch\n",
    "print(dir(torch.distributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import tensorflow as tf\n",
    "print(dir(tf.random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import paddle\n",
    "print(dir(paddle.distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3882b4",
   "metadata": {},
   "source": [
    "通常可以忽略以“`__`”（双下划线）开始和结束的函数，它们是Python中的特殊对象，\n",
    "或以单个“`_`”（单下划线）开始的函数，它们通常是内部函数。\n",
    "根据剩余的函数名或属性名，我们可能会猜测这个模块提供了各种生成随机数的方法，\n",
    "包括从均匀分布（`uniform`）、正态分布（`normal`）和多项分布（`multinomial`）中采样。\n",
    "\n",
    "## 查找特定函数和类的用法\n",
    "\n",
    "有关如何使用给定函数或类的更具体说明，可以调用`help`函数。\n",
    "例如，我们来[**查看张量`ones`函数的用法。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcdcd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "help(torch.ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "help(tf.ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "help(paddle.ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe268c79",
   "metadata": {},
   "source": [
    "从文档中，我们可以看到`ones`函数创建一个具有指定形状的新张量，并将所有元素值设置为1。\n",
    "下面来[**运行一个快速测试**]来确认这一解释："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "torch.ones(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4182f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.ones(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc89ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.ones([4], dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a066e1ac",
   "metadata": {},
   "source": [
    "在Jupyter记事本中，我们可以使用`?`指令在另一个浏览器窗口中显示文档。\n",
    "例如，`list?`指令将创建与`help(list)`指令几乎相同的内容，并在新的浏览器窗口中显示它。\n",
    "此外，如果我们使用两个问号，如`list??`，将显示实现该函数的Python代码。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 官方文档提供了本书之外的大量描述和示例。\n",
    "* 可以通过调用`dir`和`help`函数或在Jupyter记事本中使用`?`和`??`查看API的用法文档。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 在深度学习框架中查找任何函数或类的文档。请尝试在这个框架的官方网站上找到文档。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/1764)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1765)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1763)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11686)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

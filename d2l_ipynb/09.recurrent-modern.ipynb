{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53819a96",
   "metadata": {},
   "source": [
    "# 现代循环神经网络\n",
    ":label:`chap_modern_rnn`\n",
    "\n",
    "前一章中我们介绍了循环神经网络的基础知识，\n",
    "这种网络可以更好地处理序列数据。\n",
    "我们在文本数据上实现了基于循环神经网络的语言模型，\n",
    "但是对于当今各种各样的序列学习问题，这些技术可能并不够用。\n",
    "\n",
    "例如，循环神经网络在实践中一个常见问题是数值不稳定性。\n",
    "尽管我们已经应用了梯度裁剪等技巧来缓解这个问题，\n",
    "但是仍需要通过设计更复杂的序列模型来进一步处理它。\n",
    "具体来说，我们将引入两个广泛使用的网络，\n",
    "即*门控循环单元*（gated recurrent units，GRU）和\n",
    "*长短期记忆网络*（long short-term memory，LSTM）。\n",
    "然后，我们将基于一个单向隐藏层来扩展循环神经网络架构。\n",
    "我们将描述具有多个隐藏层的深层架构，\n",
    "并讨论基于前向和后向循环计算的双向设计。\n",
    "现代循环网络经常采用这种扩展。\n",
    "在解释这些循环神经网络的变体时，\n",
    "我们将继续考虑 :numref:`chap_rnn`中的语言建模问题。\n",
    "\n",
    "事实上，语言建模只揭示了序列学习能力的冰山一角。\n",
    "在各种序列学习问题中，如自动语音识别、文本到语音转换和机器翻译，\n",
    "输入和输出都是任意长度的序列。\n",
    "为了阐述如何拟合这种类型的数据，\n",
    "我们将以机器翻译为例介绍基于循环神经网络的\n",
    "“编码器－解码器”架构和束搜索，并用它们来生成序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19bec1c",
   "metadata": {
    "attributes": {
     "classes": [
      "toc"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    ":maxdepth: 2\n",
    "\n",
    "gru\n",
    "lstm\n",
    "deep-rnn\n",
    "bi-rnn\n",
    "machine-translation-and-dataset\n",
    "encoder-decoder\n",
    "seq2seq\n",
    "beam-search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ef9d20",
   "metadata": {},
   "source": [
    "# 门控循环单元（GRU）\n",
    ":label:`sec_gru`\n",
    "\n",
    "在 :numref:`sec_bptt`中，\n",
    "我们讨论了如何在循环神经网络中计算梯度，\n",
    "以及矩阵连续乘积可以导致梯度消失或梯度爆炸的问题。\n",
    "下面我们简单思考一下这种梯度异常在实践中的意义：\n",
    "\n",
    "* 我们可能会遇到这样的情况：早期观测值对预测所有未来观测值具有非常重要的意义。\n",
    "  考虑一个极端情况，其中第一个观测值包含一个校验和，\n",
    "  目标是在序列的末尾辨别校验和是否正确。\n",
    "  在这种情况下，第一个词元的影响至关重要。\n",
    "  我们希望有某些机制能够在一个记忆元里存储重要的早期信息。\n",
    "  如果没有这样的机制，我们将不得不给这个观测值指定一个非常大的梯度，\n",
    "  因为它会影响所有后续的观测值。\n",
    "* 我们可能会遇到这样的情况：一些词元没有相关的观测值。\n",
    "  例如，在对网页内容进行情感分析时，\n",
    "  可能有一些辅助HTML代码与网页传达的情绪无关。\n",
    "  我们希望有一些机制来*跳过*隐状态表示中的此类词元。\n",
    "* 我们可能会遇到这样的情况：序列的各个部分之间存在逻辑中断。\n",
    "  例如，书的章节之间可能会有过渡存在，\n",
    "  或者证券的熊市和牛市之间可能会有过渡存在。\n",
    "  在这种情况下，最好有一种方法来*重置*我们的内部状态表示。\n",
    "\n",
    "在学术界已经提出了许多方法来解决这类问题。\n",
    "其中最早的方法是\"长短期记忆\"（long-short-term memory，LSTM）\n",
    " :cite:`Hochreiter.Schmidhuber.1997`，\n",
    "我们将在 :numref:`sec_lstm`中讨论。\n",
    "门控循环单元（gated recurrent unit，GRU）\n",
    " :cite:`Cho.Van-Merrienboer.Bahdanau.ea.2014`\n",
    "是一个稍微简化的变体，通常能够提供同等的效果，\n",
    "并且计算 :cite:`Chung.Gulcehre.Cho.ea.2014`的速度明显更快。\n",
    "由于门控循环单元更简单，我们从它开始解读。\n",
    "\n",
    "## 门控隐状态\n",
    "\n",
    "门控循环单元与普通的循环神经网络之间的关键区别在于：\n",
    "前者支持隐状态的门控。\n",
    "这意味着模型有专门的机制来确定应该何时更新隐状态，\n",
    "以及应该何时重置隐状态。\n",
    "这些机制是可学习的，并且能够解决了上面列出的问题。\n",
    "例如，如果第一个词元非常重要，\n",
    "模型将学会在第一次观测之后不更新隐状态。\n",
    "同样，模型也可以学会跳过不相关的临时观测。\n",
    "最后，模型还将学会在需要的时候重置隐状态。\n",
    "下面我们将详细讨论各类门控。\n",
    "\n",
    "### 重置门和更新门\n",
    "\n",
    "我们首先介绍*重置门*（reset gate）和*更新门*（update gate）。\n",
    "我们把它们设计成$(0, 1)$区间中的向量，\n",
    "这样我们就可以进行凸组合。\n",
    "重置门允许我们控制“可能还想记住”的过去状态的数量；\n",
    "更新门将允许我们控制新状态中有多少个是旧状态的副本。\n",
    "\n",
    "我们从构造这些门控开始。 :numref:`fig_gru_1`\n",
    "描述了门控循环单元中的重置门和更新门的输入，\n",
    "输入是由当前时间步的输入和前一时间步的隐状态给出。\n",
    "两个门的输出是由使用sigmoid激活函数的两个全连接层给出。\n",
    "\n",
    "![在门控循环单元模型中计算重置门和更新门](../img/gru-1.svg)\n",
    ":label:`fig_gru_1`\n",
    "\n",
    "我们来看一下门控循环单元的数学表达。\n",
    "对于给定的时间步$t$，假设输入是一个小批量\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$\n",
    "（样本个数$n$，输入个数$d$），\n",
    "上一个时间步的隐状态是\n",
    "$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$\n",
    "（隐藏单元个数$h$）。\n",
    "那么，重置门$\\mathbf{R}_t \\in \\mathbb{R}^{n \\times h}$和\n",
    "更新门$\\mathbf{Z}_t \\in \\mathbb{R}^{n \\times h}$的计算如下所示：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r),\\\\\n",
    "\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$\\mathbf{W}_{xr}, \\mathbf{W}_{xz} \\in \\mathbb{R}^{d \\times h}$\n",
    "和$\\mathbf{W}_{hr}, \\mathbf{W}_{hz} \\in \\mathbb{R}^{h \\times h}$是权重参数，\n",
    "$\\mathbf{b}_r, \\mathbf{b}_z \\in \\mathbb{R}^{1 \\times h}$是偏置参数。\n",
    "请注意，在求和过程中会触发广播机制\n",
    "（请参阅 :numref:`subsec_broadcasting`）。\n",
    "我们使用sigmoid函数（如 :numref:`sec_mlp`中介绍的）\n",
    "将输入值转换到区间$(0, 1)$。\n",
    "\n",
    "### 候选隐状态\n",
    "\n",
    "接下来，让我们将重置门$\\mathbf{R}_t$\n",
    "与 :eqref:`rnn_h_with_state`\n",
    "中的常规隐状态更新机制集成，\n",
    "得到在时间步$t$的*候选隐状态*（candidate hidden state）\n",
    "$\\tilde{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}$。\n",
    "\n",
    "$$\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{hh} + \\mathbf{b}_h),$$\n",
    ":eqlabel:`gru_tilde_H`\n",
    "\n",
    "其中$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}$\n",
    "和$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$是权重参数，\n",
    "$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$是偏置项，\n",
    "符号$\\odot$是Hadamard积（按元素乘积）运算符。\n",
    "在这里，我们使用tanh非线性激活函数来确保候选隐状态中的值保持在区间$(-1, 1)$中。\n",
    "\n",
    "与 :eqref:`rnn_h_with_state`相比，\n",
    " :eqref:`gru_tilde_H`中的$\\mathbf{R}_t$和$\\mathbf{H}_{t-1}$\n",
    "的元素相乘可以减少以往状态的影响。\n",
    "每当重置门$\\mathbf{R}_t$中的项接近$1$时，\n",
    "我们恢复一个如 :eqref:`rnn_h_with_state`中的普通的循环神经网络。\n",
    "对于重置门$\\mathbf{R}_t$中所有接近$0$的项，\n",
    "候选隐状态是以$\\mathbf{X}_t$作为输入的多层感知机的结果。\n",
    "因此，任何预先存在的隐状态都会被*重置*为默认值。\n",
    "\n",
    " :numref:`fig_gru_2`说明了应用重置门之后的计算流程。\n",
    "\n",
    "![在门控循环单元模型中计算候选隐状态](../img/gru-2.svg)\n",
    ":label:`fig_gru_2`\n",
    "\n",
    "### 隐状态\n",
    "\n",
    "上述的计算结果只是候选隐状态，我们仍然需要结合更新门$\\mathbf{Z}_t$的效果。\n",
    "这一步确定新的隐状态$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$\n",
    "在多大程度上来自旧的状态$\\mathbf{H}_{t-1}$和\n",
    "新的候选状态$\\tilde{\\mathbf{H}}_t$。\n",
    "更新门$\\mathbf{Z}_t$仅需要在\n",
    "$\\mathbf{H}_{t-1}$和$\\tilde{\\mathbf{H}}_t$\n",
    "之间进行按元素的凸组合就可以实现这个目标。\n",
    "这就得出了门控循环单元的最终更新公式：\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.$$\n",
    "\n",
    "每当更新门$\\mathbf{Z}_t$接近$1$时，模型就倾向只保留旧状态。\n",
    "此时，来自$\\mathbf{X}_t$的信息基本上被忽略，\n",
    "从而有效地跳过了依赖链条中的时间步$t$。\n",
    "相反，当$\\mathbf{Z}_t$接近$0$时，\n",
    "新的隐状态$\\mathbf{H}_t$就会接近候选隐状态$\\tilde{\\mathbf{H}}_t$。\n",
    "这些设计可以帮助我们处理循环神经网络中的梯度消失问题，\n",
    "并更好地捕获时间步距离很长的序列的依赖关系。\n",
    "例如，如果整个子序列的所有时间步的更新门都接近于$1$，\n",
    "则无论序列的长度如何，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。\n",
    "\n",
    " :numref:`fig_gru_3`说明了更新门起作用后的计算流。\n",
    "\n",
    "![计算门控循环单元模型中的隐状态](../img/gru-3.svg)\n",
    ":label:`fig_gru_3`\n",
    "\n",
    "总之，门控循环单元具有以下两个显著特征：\n",
    "\n",
    "* 重置门有助于捕获序列中的短期依赖关系；\n",
    "* 更新门有助于捕获序列中的长期依赖关系。\n",
    "\n",
    "## 从零开始实现\n",
    "\n",
    "为了更好地理解门控循环单元模型，我们从零开始实现它。\n",
    "首先，我们读取 :numref:`sec_rnn_scratch`中使用的时间机器数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c5305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "from mxnet.gluon import rnn\n",
    "npx.set_np()\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b03dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a5ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddle import nn\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b224601",
   "metadata": {},
   "source": [
    "### [**初始化模型参数**]\n",
    "\n",
    "下一步是初始化模型参数。\n",
    "我们从标准差为$0.01$的高斯分布中提取权重，\n",
    "并将偏置项设为$0$，超参数`num_hiddens`定义隐藏单元的数量，\n",
    "实例化与更新门、重置门、候选隐状态和输出层相关的所有权重和偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debf543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return np.random.normal(scale=0.01, size=shape, ctx=device)\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                np.zeros(num_hiddens, ctx=device))\n",
    "\n",
    "    W_xz, W_hz, b_z = three()  # 更新门参数\n",
    "    W_xr, W_hr, b_r = three()  # 重置门参数\n",
    "    W_xh, W_hh, b_h = three()  # 候选隐状态参数\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = np.zeros(num_outputs, ctx=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device)*0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                d2l.zeros(num_hiddens, device=device))\n",
    "\n",
    "    W_xz, W_hz, b_z = three()  # 更新门参数\n",
    "    W_xr, W_hr, b_r = three()  # 重置门参数\n",
    "    W_xh, W_hh, b_h = three()  # 候选隐状态参数\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = d2l.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def get_params(vocab_size, num_hiddens):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return d2l.normal(shape=shape,stddev=0.01,mean=0,dtype=tf.float32)\n",
    "\n",
    "    def three():\n",
    "        return (tf.Variable(normal((num_inputs, num_hiddens)), dtype=tf.float32),\n",
    "                tf.Variable(normal((num_hiddens, num_hiddens)), dtype=tf.float32),\n",
    "                tf.Variable(d2l.zeros(num_hiddens), dtype=tf.float32))\n",
    "\n",
    "    W_xz, W_hz, b_z = three()  # 更新门参数\n",
    "    W_xr, W_hr, b_r = three()  # 重置门参数\n",
    "    W_xh, W_hh, b_h = three()  # 候选隐状态参数\n",
    "    # 输出层参数\n",
    "    W_hq = tf.Variable(normal((num_hiddens, num_outputs)), dtype=tf.float32)\n",
    "    b_q = tf.Variable(d2l.zeros(num_outputs), dtype=tf.float32)\n",
    "    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def get_params(vocab_size, num_hiddens):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return paddle.randn(shape=shape)*0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                paddle.zeros([num_hiddens]))\n",
    "\n",
    "    W_xz, W_hz, b_z = three()  # 更新门参数\n",
    "    W_xr, W_hr, b_r = three()  # 重置门参数\n",
    "    W_xh, W_hh, b_h = three()  # 候选隐状态参数\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = paddle.zeros([num_outputs])\n",
    "    # 附加梯度\n",
    "    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.stop_gradient = False\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937d81a",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "\n",
    "现在我们将[**定义隐状态的初始化函数**]`init_gru_state`。\n",
    "与 :numref:`sec_rnn_scratch`中定义的`init_rnn_state`函数一样，\n",
    "此函数返回一个形状为（批量大小，隐藏单元个数）的张量，张量的值全部为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gru_state(batch_size, num_hiddens, device):\n",
    "    return (np.zeros(shape=(batch_size, num_hiddens), ctx=device), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b8a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def init_gru_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a62e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def init_gru_state(batch_size, num_hiddens):\n",
    "    return (d2l.zeros((batch_size, num_hiddens)), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc442ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def init_gru_state(batch_size, num_hiddens):\n",
    "    return (paddle.zeros([batch_size, num_hiddens]), )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3379c",
   "metadata": {},
   "source": [
    "现在我们准备[**定义门控循环单元模型**]，\n",
    "模型的架构与基本的循环神经网络单元是相同的，\n",
    "只是权重更新公式更为复杂。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(inputs, state, params):\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        Z = npx.sigmoid(np.dot(X, W_xz) + np.dot(H, W_hz) + b_z)\n",
    "        R = npx.sigmoid(np.dot(X, W_xr) + np.dot(H, W_hr) + b_r)\n",
    "        H_tilda = np.tanh(np.dot(X, W_xh) + np.dot(R * H, W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = np.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return np.concatenate(outputs, axis=0), (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f126f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def gru(inputs, state, params):\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        Z = torch.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)\n",
    "        R = torch.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)\n",
    "        H_tilda = torch.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = H @ W_hq + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76274c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def gru(inputs, state, params):\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        X = tf.reshape(X,[-1,W_xh.shape[0]])\n",
    "        Z = tf.sigmoid(tf.matmul(X, W_xz) + tf.matmul(H, W_hz) + b_z)\n",
    "        R = tf.sigmoid(tf.matmul(X, W_xr) + tf.matmul(H, W_hr) + b_r)\n",
    "        H_tilda = tf.tanh(tf.matmul(X, W_xh) + tf.matmul(R * H, W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = tf.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return tf.concat(outputs, axis=0), (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc3019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def gru(inputs, state, params):\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H,*_ = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        Z = F.sigmoid((X @ W_xz) + (H @ W_hz) + b_z)\n",
    "        R = F.sigmoid((X @ W_xr) + (H @ W_hr) + b_r)\n",
    "        H_tilda = paddle.tanh((X @ W_xh) + ((R * H) @ W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = H @ W_hq + b_q\n",
    "        outputs.append(Y)\n",
    "    return paddle.concat(outputs, axis=0), (H,*_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b63ca",
   "metadata": {},
   "source": [
    "### [**训练**]与预测\n",
    "\n",
    "训练和预测的工作方式与 :numref:`sec_rnn_scratch`完全相同。\n",
    "训练结束后，我们分别打印输出训练集的困惑度，\n",
    "以及前缀“time traveler”和“traveler”的预测序列上的困惑度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b73305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()\n",
    "num_epochs, lr = 500, 1\n",
    "model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_params,\n",
    "                            init_gru_state, gru)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5568b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "vocab_size, num_hiddens, device_name = len(vocab), 256, d2l.try_gpu()._device_name\n",
    "# 定义训练策略\n",
    "strategy = tf.distribute.OneDeviceStrategy(device_name)\n",
    "num_epochs, lr = 500, 1\n",
    "with strategy.scope():\n",
    "    model = d2l.RNNModelScratch(len(vocab), num_hiddens, init_gru_state, gru, get_params)\n",
    "\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()\n",
    "num_epochs, lr = 500, 1.0\n",
    "model = d2l.RNNModelScratch(len(vocab), num_hiddens, get_params,\n",
    "                            init_gru_state, gru)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd7cb5f",
   "metadata": {},
   "source": [
    "## [**简洁实现**]\n",
    "\n",
    "高级API包含了前文介绍的所有配置细节，\n",
    "所以我们可以直接实例化门控循环单元模型。\n",
    "这段代码的运行速度要快得多，\n",
    "因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4786f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_layer = rnn.GRU(num_hiddens)\n",
    "model = d2l.RNNModel(gru_layer, len(vocab))\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6191337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "num_inputs = vocab_size\n",
    "gru_layer = nn.GRU(num_inputs, num_hiddens)\n",
    "model = d2l.RNNModel(gru_layer, len(vocab))\n",
    "model = model.to(device)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90eeb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "gru_cell = tf.keras.layers.GRUCell(num_hiddens,\n",
    "    kernel_initializer='glorot_uniform')\n",
    "gru_layer = tf.keras.layers.RNN(gru_cell, time_major=True,\n",
    "    return_sequences=True, return_state=True)\n",
    "\n",
    "device_name = d2l.try_gpu()._device_name\n",
    "strategy = tf.distribute.OneDeviceStrategy(device_name)\n",
    "with strategy.scope():\n",
    "    model = d2l.RNNModel(gru_layer, vocab_size=len(vocab))\n",
    "\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c923ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "num_inputs = vocab_size\n",
    "gru_layer = nn.GRU(num_inputs, num_hiddens, time_major=True)\n",
    "model = d2l.RNNModel(gru_layer, len(vocab))\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2680c8",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 门控循环神经网络可以更好地捕获时间步距离很长的序列上的依赖关系。\n",
    "* 重置门有助于捕获序列中的短期依赖关系。\n",
    "* 更新门有助于捕获序列中的长期依赖关系。\n",
    "* 重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 假设我们只想使用时间步$t'$的输入来预测时间步$t > t'$的输出。对于每个时间步，重置门和更新门的最佳值是什么？\n",
    "1. 调整和分析超参数对运行时间、困惑度和输出顺序的影响。\n",
    "1. 比较`rnn.RNN`和`rnn.GRU`的不同实现对运行时间、困惑度和输出字符串的影响。\n",
    "1. 如果仅仅实现门控循环单元的一部分，例如，只有一个重置门或一个更新门会怎样？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2764)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2763)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11812)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f8606",
   "metadata": {},
   "source": [
    "# 长短期记忆网络（LSTM）\n",
    ":label:`sec_lstm`\n",
    "\n",
    "长期以来，隐变量模型存在着长期信息保存和短期输入缺失的问题。\n",
    "解决这一问题的最早方法之一是长短期存储器（long short-term memory，LSTM）\n",
    " :cite:`Hochreiter.Schmidhuber.1997`。\n",
    "它有许多与门控循环单元（ :numref:`sec_gru`）一样的属性。\n",
    "有趣的是，长短期记忆网络的设计比门控循环单元稍微复杂一些，\n",
    "却比门控循环单元早诞生了近20年。\n",
    "\n",
    "## 门控记忆元\n",
    "\n",
    "可以说，长短期记忆网络的设计灵感来自于计算机的逻辑门。\n",
    "长短期记忆网络引入了*记忆元*（memory cell），或简称为*单元*（cell）。\n",
    "有些文献认为记忆元是隐状态的一种特殊类型，\n",
    "它们与隐状态具有相同的形状，其设计目的是用于记录附加的信息。\n",
    "为了控制记忆元，我们需要许多门。\n",
    "其中一个门用来从单元中输出条目，我们将其称为*输出门*（output gate）。\n",
    "另外一个门用来决定何时将数据读入单元，我们将其称为*输入门*（input gate）。\n",
    "我们还需要一种机制来重置单元的内容，由*遗忘门*（forget gate）来管理，\n",
    "这种设计的动机与门控循环单元相同，\n",
    "能够通过专用机制决定什么时候记忆或忽略隐状态中的输入。\n",
    "让我们看看这在实践中是如何运作的。\n",
    "\n",
    "### 输入门、忘记门和输出门\n",
    "\n",
    "就如在门控循环单元中一样，\n",
    "当前时间步的输入和前一个时间步的隐状态\n",
    "作为数据送入长短期记忆网络的门中，\n",
    "如 :numref:`lstm_0`所示。\n",
    "它们由三个具有sigmoid激活函数的全连接层处理，\n",
    "以计算输入门、遗忘门和输出门的值。\n",
    "因此，这三个门的值都在$(0, 1)$的范围内。\n",
    "\n",
    "![长短期记忆模型中的输入门、遗忘门和输出门](../img/lstm-0.svg)\n",
    ":label:`lstm_0`\n",
    "\n",
    "我们来细化一下长短期记忆网络的数学表达。\n",
    "假设有$h$个隐藏单元，批量大小为$n$，输入数为$d$。\n",
    "因此，输入为$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$，\n",
    "前一时间步的隐状态为$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$。\n",
    "相应地，时间步$t$的门被定义如下：\n",
    "输入门是$\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}$，\n",
    "遗忘门是$\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}$，\n",
    "输出门是$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}$。\n",
    "它们的计算方法如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{I}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i),\\\\\n",
    "\\mathbf{F}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f),\\\\\n",
    "\\mathbf{O}_t &= \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$\\mathbf{W}_{xi}, \\mathbf{W}_{xf}, \\mathbf{W}_{xo} \\in \\mathbb{R}^{d \\times h}$\n",
    "和$\\mathbf{W}_{hi}, \\mathbf{W}_{hf}, \\mathbf{W}_{ho} \\in \\mathbb{R}^{h \\times h}$是权重参数，\n",
    "$\\mathbf{b}_i, \\mathbf{b}_f, \\mathbf{b}_o \\in \\mathbb{R}^{1 \\times h}$是偏置参数。\n",
    "\n",
    "### 候选记忆元\n",
    "\n",
    "由于还没有指定各种门的操作，所以先介绍*候选记忆元*（candidate memory cell）\n",
    "$\\tilde{\\mathbf{C}}_t \\in \\mathbb{R}^{n \\times h}$。\n",
    "它的计算与上面描述的三个门的计算类似，\n",
    "但是使用$\\tanh$函数作为激活函数，函数的值范围为$(-1, 1)$。\n",
    "下面导出在时间步$t$处的方程：\n",
    "\n",
    "$$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c),$$\n",
    "\n",
    "其中$\\mathbf{W}_{xc} \\in \\mathbb{R}^{d \\times h}$和\n",
    "$\\mathbf{W}_{hc} \\in \\mathbb{R}^{h \\times h}$是权重参数，\n",
    "$\\mathbf{b}_c \\in \\mathbb{R}^{1 \\times h}$是偏置参数。\n",
    "\n",
    "候选记忆元的如 :numref:`lstm_1`所示。\n",
    "\n",
    "![长短期记忆模型中的候选记忆元](../img/lstm-1.svg)\n",
    ":label:`lstm_1`\n",
    "\n",
    "### 记忆元\n",
    "\n",
    "在门控循环单元中，有一种机制来控制输入和遗忘（或跳过）。\n",
    "类似地，在长短期记忆网络中，也有两个门用于这样的目的：\n",
    "输入门$\\mathbf{I}_t$控制采用多少来自$\\tilde{\\mathbf{C}}_t$的新数据，\n",
    "而遗忘门$\\mathbf{F}_t$控制保留多少过去的\n",
    "记忆元$\\mathbf{C}_{t-1} \\in \\mathbb{R}^{n \\times h}$的内容。\n",
    "使用按元素乘法，得出：\n",
    "\n",
    "$$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.$$\n",
    "\n",
    "如果遗忘门始终为$1$且输入门始终为$0$，\n",
    "则过去的记忆元$\\mathbf{C}_{t-1}$\n",
    "将随时间被保存并传递到当前时间步。\n",
    "引入这种设计是为了缓解梯度消失问题，\n",
    "并更好地捕获序列中的长距离依赖关系。\n",
    "\n",
    "这样我们就得到了计算记忆元的流程图，如 :numref:`lstm_2`。\n",
    "\n",
    "![在长短期记忆网络模型中计算记忆元](../img/lstm-2.svg)\n",
    "\n",
    ":label:`lstm_2`\n",
    "\n",
    "### 隐状态\n",
    "\n",
    "最后，我们需要定义如何计算隐状态\n",
    "$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$，\n",
    "这就是输出门发挥作用的地方。\n",
    "在长短期记忆网络中，它仅仅是记忆元的$\\tanh$的门控版本。\n",
    "这就确保了$\\mathbf{H}_t$的值始终在区间$(-1, 1)$内：\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).$$\n",
    "\n",
    "只要输出门接近$1$，我们就能够有效地将所有记忆信息传递给预测部分，\n",
    "而对于输出门接近$0$，我们只保留记忆元内的所有信息，而不需要更新隐状态。\n",
    "\n",
    " :numref:`lstm_3`提供了数据流的图形化演示。\n",
    "\n",
    "![在长短期记忆模型中计算隐状态](../img/lstm-3.svg)\n",
    ":label:`lstm_3`\n",
    "\n",
    "## 从零开始实现\n",
    "\n",
    "现在，我们从零开始实现长短期记忆网络。\n",
    "与 :numref:`sec_rnn_scratch`中的实验相同，\n",
    "我们首先加载时光机器数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96874d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "from mxnet.gluon import rnn\n",
    "npx.set_np()\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849eba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "from paddle import nn\n",
    "import paddle.nn.functional as Function\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a99a5e",
   "metadata": {},
   "source": [
    "### [**初始化模型参数**]\n",
    "\n",
    "接下来，我们需要定义和初始化模型参数。\n",
    "如前所述，超参数`num_hiddens`定义隐藏单元的数量。\n",
    "我们按照标准差$0.01$的高斯分布初始化权重，并将偏置项设为$0$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7516ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return np.random.normal(scale=0.01, size=shape, ctx=device)\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                np.zeros(num_hiddens, ctx=device))\n",
    "\n",
    "    W_xi, W_hi, b_i = three()  # 输入门参数\n",
    "    W_xf, W_hf, b_f = three()  # 遗忘门参数\n",
    "    W_xo, W_ho, b_o = three()  # 输出门参数\n",
    "    W_xc, W_hc, b_c = three()  # 候选记忆元参数\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = np.zeros(num_outputs, ctx=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,\n",
    "              b_c, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80983983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_lstm_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device)*0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                d2l.zeros(num_hiddens, device=device))\n",
    "\n",
    "    W_xi, W_hi, b_i = three()  # 输入门参数\n",
    "    W_xf, W_hf, b_f = three()  # 遗忘门参数\n",
    "    W_xo, W_ho, b_o = three()  # 输出门参数\n",
    "    W_xc, W_hc, b_c = three()  # 候选记忆元参数\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = d2l.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,\n",
    "              b_c, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baefc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def get_lstm_params(vocab_size, num_hiddens):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return tf.Variable(tf.random.normal(shape=shape, stddev=0.01,\n",
    "                                            mean=0, dtype=tf.float32))\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32))\n",
    "\n",
    "    W_xi, W_hi, b_i = three()  # 输入门参数\n",
    "    W_xf, W_hf, b_f = three()  # 遗忘门参数\n",
    "    W_xo, W_ho, b_o = three()  # 输出门参数\n",
    "    W_xc, W_hc, b_c = three()  # 候选记忆元参数\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)\n",
    "    # 附加梯度\n",
    "    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,\n",
    "              b_c, W_hq, b_q]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e28ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def get_lstm_params(vocab_size, num_hiddens):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return paddle.randn(shape=shape)*0.01\n",
    "\n",
    "    def three():\n",
    "        return (normal((num_inputs, num_hiddens)),\n",
    "                normal((num_hiddens, num_hiddens)),\n",
    "                d2l.zeros([num_hiddens]))\n",
    "\n",
    "    W_xi, W_hi, b_i = three()  # 输入门参数\n",
    "    W_xf, W_hf, b_f = three()  # 遗忘门参数\n",
    "    W_xo, W_ho, b_o = three()  # 输出门参数\n",
    "    W_xc, W_hc, b_c = three()  # 候选记忆元参数\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = d2l.zeros([num_outputs])\n",
    "    # 附加梯度\n",
    "    params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc,\n",
    "              b_c, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.stop_gradient = False\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b88f80c",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "\n",
    "在[**初始化函数**]中，\n",
    "长短期记忆网络的隐状态需要返回一个*额外*的记忆元，\n",
    "单元的值为0，形状为（批量大小，隐藏单元数）。\n",
    "因此，我们得到以下的状态初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b902f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm_state(batch_size, num_hiddens, device):\n",
    "    return (np.zeros((batch_size, num_hiddens), ctx=device),\n",
    "            np.zeros((batch_size, num_hiddens), ctx=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d05527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def init_lstm_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device),\n",
    "            torch.zeros((batch_size, num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def init_lstm_state(batch_size, num_hiddens):\n",
    "    return (tf.zeros(shape=(batch_size, num_hiddens)),\n",
    "            tf.zeros(shape=(batch_size, num_hiddens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d8e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def init_lstm_state(batch_size, num_hiddens):\n",
    "    return (paddle.zeros([batch_size, num_hiddens]),\n",
    "            paddle.zeros([batch_size, num_hiddens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4fb519",
   "metadata": {},
   "source": [
    "[**实际模型**]的定义与我们前面讨论的一样：\n",
    "提供三个门和一个额外的记忆元。\n",
    "请注意，只有隐状态才会传递到输出层，\n",
    "而记忆元$\\mathbf{C}_t$不直接参与输出计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d70ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(inputs, state, params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "     W_hq, b_q] = params\n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = npx.sigmoid(np.dot(X, W_xi) + np.dot(H, W_hi) + b_i)\n",
    "        F = npx.sigmoid(np.dot(X, W_xf) + np.dot(H, W_hf) + b_f)\n",
    "        O = npx.sigmoid(np.dot(X, W_xo) + np.dot(H, W_ho) + b_o)\n",
    "        C_tilda = np.tanh(np.dot(X, W_xc) + np.dot(H, W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * np.tanh(C)\n",
    "        Y = np.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return np.concatenate(outputs, axis=0), (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8540260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def lstm(inputs, state, params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "     W_hq, b_q] = params\n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)\n",
    "        F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)\n",
    "        O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)\n",
    "        C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * torch.tanh(C)\n",
    "        Y = (H @ W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39769b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def lstm(inputs, state, params):\n",
    "    W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q = params\n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        X=tf.reshape(X,[-1,W_xi.shape[0]])\n",
    "        I = tf.sigmoid(tf.matmul(X, W_xi) + tf.matmul(H, W_hi) + b_i)\n",
    "        F = tf.sigmoid(tf.matmul(X, W_xf) + tf.matmul(H, W_hf) + b_f)\n",
    "        O = tf.sigmoid(tf.matmul(X, W_xo) + tf.matmul(H, W_ho) + b_o)\n",
    "        C_tilda = tf.tanh(tf.matmul(X, W_xc) + tf.matmul(H, W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * tf.tanh(C)\n",
    "        Y = tf.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return tf.concat(outputs, axis=0), (H,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307604dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def lstm(inputs, state, params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "     W_hq, b_q] = params\n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = Function.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)\n",
    "        F = Function.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)\n",
    "        O = Function.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)\n",
    "        C_tilda = paddle.tanh((X @ W_xc) + (H @ W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * paddle.tanh(C)\n",
    "        Y = (H @ W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return paddle.concat(outputs, axis=0), (H, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec2c51",
   "metadata": {},
   "source": [
    "### [**训练**]和预测\n",
    "\n",
    "让我们通过实例化 :numref:`sec_rnn_scratch`中\n",
    "引入的`RNNModelScratch`类来训练一个长短期记忆网络，\n",
    "就如我们在 :numref:`sec_gru`中所做的一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()\n",
    "num_epochs, lr = 500, 1\n",
    "model = d2l.RNNModelScratch(len(vocab), num_hiddens, device, get_lstm_params,\n",
    "                            init_lstm_state, lstm)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d478a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "vocab_size, num_hiddens, device_name = len(vocab), 256, d2l.try_gpu()._device_name\n",
    "num_epochs, lr = 500, 1\n",
    "strategy = tf.distribute.OneDeviceStrategy(device_name)\n",
    "with strategy.scope():\n",
    "    model = d2l.RNNModelScratch(len(vocab), num_hiddens, init_lstm_state, lstm, get_lstm_params)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11dcfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "vocab_size, num_hiddens, device = len(vocab), 256, d2l.try_gpu()\n",
    "num_epochs, lr = 500, 1.0\n",
    "model = d2l.RNNModelScratch(len(vocab), num_hiddens, get_lstm_params,\n",
    "                            init_lstm_state, lstm)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4369c",
   "metadata": {},
   "source": [
    "## [**简洁实现**]\n",
    "\n",
    "使用高级API，我们可以直接实例化`LSTM`模型。\n",
    "高级API封装了前文介绍的所有配置细节。\n",
    "这段代码的运行速度要快得多，\n",
    "因为它使用的是编译好的运算符而不是Python来处理之前阐述的许多细节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer = rnn.LSTM(num_hiddens)\n",
    "model = d2l.RNNModel(lstm_layer, len(vocab))\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3abbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "num_inputs = vocab_size\n",
    "lstm_layer = nn.LSTM(num_inputs, num_hiddens)\n",
    "model = d2l.RNNModel(lstm_layer, len(vocab))\n",
    "model = model.to(device)\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83de5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "lstm_cell = tf.keras.layers.LSTMCell(num_hiddens,\n",
    "    kernel_initializer='glorot_uniform')\n",
    "lstm_layer = tf.keras.layers.RNN(lstm_cell, time_major=True,\n",
    "    return_sequences=True, return_state=True)\n",
    "device_name = d2l.try_gpu()._device_name\n",
    "strategy = tf.distribute.OneDeviceStrategy(device_name)\n",
    "with strategy.scope():\n",
    "    model = d2l.RNNModel(lstm_layer, vocab_size=len(vocab))\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ec5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "num_inputs = vocab_size\n",
    "lstm_layer = nn.LSTM(num_inputs, num_hiddens, time_major=True)\n",
    "model = d2l.RNNModel(lstm_layer, len(vocab))\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e15e5",
   "metadata": {},
   "source": [
    "长短期记忆网络是典型的具有重要状态控制的隐变量自回归模型。\n",
    "多年来已经提出了其许多变体，例如，多层、残差连接、不同类型的正则化。\n",
    "然而，由于序列的长距离依赖性，训练长短期记忆网络\n",
    "和其他序列模型（例如门控循环单元）的成本是相当高的。\n",
    "在后面的内容中，我们将讲述更高级的替代模型，如Transformer。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 长短期记忆网络有三种类型的门：输入门、遗忘门和输出门。\n",
    "* 长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。\n",
    "* 长短期记忆网络可以缓解梯度消失和梯度爆炸。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 调整和分析超参数对运行时间、困惑度和输出顺序的影响。\n",
    "1. 如何更改模型以生成适当的单词，而不是字符序列？\n",
    "1. 在给定隐藏层维度的情况下，比较门控循环单元、长短期记忆网络和常规循环神经网络的计算成本。要特别注意训练和推断成本。\n",
    "1. 既然候选记忆元通过使用$\\tanh$函数来确保值范围在$(-1,1)$之间，那么为什么隐状态需要再次使用$\\tanh$函数来确保输出值范围在$(-1,1)$之间呢？\n",
    "1. 实现一个能够基于时间序列进行预测而不是基于字符序列进行预测的长短期记忆网络模型。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2766)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2768)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11833)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cbe053",
   "metadata": {},
   "source": [
    "# 深度循环神经网络\n",
    "\n",
    ":label:`sec_deep_rnn`\n",
    "\n",
    "到目前为止，我们只讨论了具有一个单向隐藏层的循环神经网络。\n",
    "其中，隐变量和观测值与具体的函数形式的交互方式是相当随意的。\n",
    "只要交互类型建模具有足够的灵活性，这就不是一个大问题。\n",
    "然而，对一个单层来说，这可能具有相当的挑战性。\n",
    "之前在线性模型中，我们通过添加更多的层来解决这个问题。\n",
    "而在循环神经网络中，我们首先需要确定如何添加更多的层，\n",
    "以及在哪里添加额外的非线性，因此这个问题有点棘手。\n",
    "\n",
    "事实上，我们可以将多层循环神经网络堆叠在一起，\n",
    "通过对几个简单层的组合，产生了一个灵活的机制。\n",
    "特别是，数据可能与不同层的堆叠有关。\n",
    "例如，我们可能希望保持有关金融市场状况\n",
    "（熊市或牛市）的宏观数据可用，\n",
    "而微观数据只记录较短期的时间动态。\n",
    "\n",
    " :numref:`fig_deep_rnn`描述了一个具有$L$个隐藏层的深度循环神经网络，\n",
    "每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步。\n",
    "\n",
    "![深度循环神经网络结构](../img/deep-rnn.svg)\n",
    ":label:`fig_deep_rnn`\n",
    "\n",
    "## 函数依赖关系\n",
    "\n",
    "我们可以将深度架构中的函数依赖关系形式化，\n",
    "这个架构是由 :numref:`fig_deep_rnn`中描述了$L$个隐藏层构成。\n",
    "后续的讨论主要集中在经典的循环神经网络模型上，\n",
    "但是这些讨论也适应于其他序列模型。\n",
    "\n",
    "假设在时间步$t$有一个小批量的输入数据\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$\n",
    "（样本数：$n$，每个样本中的输入数：$d$）。\n",
    "同时，将$l^\\mathrm{th}$隐藏层（$l=1,\\ldots,L$）\n",
    "的隐状态设为$\\mathbf{H}_t^{(l)}  \\in \\mathbb{R}^{n \\times h}$\n",
    "（隐藏单元数：$h$），\n",
    "输出层变量设为$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$\n",
    "（输出数：$q$）。\n",
    "设置$\\mathbf{H}_t^{(0)} = \\mathbf{X}_t$，\n",
    "第$l$个隐藏层的隐状态使用激活函数$\\phi_l$，则：\n",
    "\n",
    "$$\\mathbf{H}_t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{xh}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{hh}^{(l)}  + \\mathbf{b}_h^{(l)}),$$\n",
    ":eqlabel:`eq_deep_rnn_H`\n",
    "\n",
    "其中，权重$\\mathbf{W}_{xh}^{(l)} \\in \\mathbb{R}^{h \\times h}$，\n",
    "$\\mathbf{W}_{hh}^{(l)} \\in \\mathbb{R}^{h \\times h}$和\n",
    "偏置$\\mathbf{b}_h^{(l)} \\in \\mathbb{R}^{1 \\times h}$\n",
    "都是第$l$个隐藏层的模型参数。\n",
    "\n",
    "最后，输出层的计算仅基于第$l$个隐藏层最终的隐状态：\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t^{(L)} \\mathbf{W}_{hq} + \\mathbf{b}_q,$$\n",
    "\n",
    "其中，权重$\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$和偏置$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$都是输出层的模型参数。\n",
    "\n",
    "与多层感知机一样，隐藏层数目$L$和隐藏单元数目$h$都是超参数。\n",
    "也就是说，它们可以由我们调整的。\n",
    "另外，用门控循环单元或长短期记忆网络的隐状态\n",
    "来代替 :eqref:`eq_deep_rnn_H`中的隐状态进行计算，\n",
    "可以很容易地得到深度门控循环神经网络或深度长短期记忆神经网络。\n",
    "\n",
    "## 简洁实现\n",
    "\n",
    "实现多层循环神经网络所需的许多逻辑细节在高级API中都是现成的。\n",
    "简单起见，我们仅示范使用此类内置函数的实现方式。\n",
    "以长短期记忆网络模型为例，\n",
    "该代码与之前在 :numref:`sec_lstm`中使用的代码非常相似，\n",
    "实际上唯一的区别是我们指定了层的数量，\n",
    "而不是使用单一层这个默认值。\n",
    "像往常一样，我们从加载数据集开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import npx\n",
    "from mxnet.gluon import rnn\n",
    "npx.set_np()\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df14c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "from paddle import nn\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f93d2",
   "metadata": {},
   "source": [
    "像选择超参数这类架构决策也跟 :numref:`sec_lstm`中的决策非常相似。\n",
    "因为我们有不同的词元，所以输入和输出都选择相同数量，即`vocab_size`。\n",
    "隐藏单元的数量仍然是$256$。\n",
    "唯一的区别是，我们现在(**通过`num_layers`的值来设定隐藏层数**)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, num_hiddens, num_layers = len(vocab), 256, 2\n",
    "device = d2l.try_gpu()\n",
    "lstm_layer = rnn.LSTM(num_hiddens, num_layers)\n",
    "model = d2l.RNNModel(lstm_layer, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05672e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "vocab_size, num_hiddens, num_layers = len(vocab), 256, 2\n",
    "num_inputs = vocab_size\n",
    "device = d2l.try_gpu()\n",
    "lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)\n",
    "model = d2l.RNNModel(lstm_layer, len(vocab))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d6b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "vocab_size, num_hiddens, num_layers = len(vocab), 256, 2\n",
    "num_inputs = vocab_size\n",
    "device = d2l.try_gpu()\n",
    "lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, time_major=True)\n",
    "model = d2l.RNNModel(lstm_layer, len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c70ff8",
   "metadata": {},
   "source": [
    "## [**训练**]与预测\n",
    "\n",
    "由于使用了长短期记忆网络模型来实例化两个层，因此训练速度被大大降低了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ec85db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "num_epochs, lr = 500, 2\n",
    "d2l.train_ch8(model, train_iter, vocab, lr*1.0, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4735a",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 在深度循环神经网络中，隐状态的信息被传递到当前层的下一时间步和下一层的当前时间步。\n",
    "* 有许多不同风格的深度循环神经网络，\n",
    "  如长短期记忆网络、门控循环单元、或经典循环神经网络。\n",
    "  这些模型在深度学习框架的高级API中都有涵盖。\n",
    "* 总体而言，深度循环神经网络需要大量的调参（如学习率和修剪）\n",
    "  来确保合适的收敛，模型的初始化也需要谨慎。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 基于我们在 :numref:`sec_rnn_scratch`中讨论的单层实现，\n",
    "   尝试从零开始实现两层循环神经网络。\n",
    "1. 在本节训练模型中，比较使用门控循环单元替换长短期记忆网络后模型的精确度和训练速度。\n",
    "1. 如果增加训练数据，能够将困惑度降到多低？\n",
    "1. 在为文本建模时，是否可以将不同作者的源数据合并？有何优劣呢？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2771)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2770)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11834)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73605f5",
   "metadata": {},
   "source": [
    "# 双向循环神经网络\n",
    ":label:`sec_bi_rnn`\n",
    "\n",
    "在序列学习中，我们以往假设的目标是：\n",
    "在给定观测的情况下\n",
    "（例如，在时间序列的上下文中或在语言模型的上下文中），\n",
    "对下一个输出进行建模。\n",
    "虽然这是一个典型情景，但不是唯一的。\n",
    "还可能发生什么其它的情况呢？\n",
    "我们考虑以下三个在文本序列中填空的任务。\n",
    "\n",
    "* 我`___`。\n",
    "* 我`___`饿了。\n",
    "* 我`___`饿了，我可以吃半头猪。\n",
    "\n",
    "根据可获得的信息量，我们可以用不同的词填空，\n",
    "如“很高兴”（\"happy\"）、“不”（\"not\"）和“非常”（\"very\"）。\n",
    "很明显，每个短语的“下文”传达了重要信息（如果有的话），\n",
    "而这些信息关乎到选择哪个词来填空，\n",
    "所以无法利用这一点的序列模型将在相关任务上表现不佳。\n",
    "例如，如果要做好命名实体识别\n",
    "（例如，识别“Green”指的是“格林先生”还是绿色），\n",
    "不同长度的上下文范围重要性是相同的。\n",
    "为了获得一些解决问题的灵感，让我们先迂回到概率图模型。\n",
    "\n",
    "## 隐马尔可夫模型中的动态规划\n",
    "\n",
    "这一小节是用来说明动态规划问题的，\n",
    "具体的技术细节对于理解深度学习模型并不重要，\n",
    "但它有助于我们思考为什么要使用深度学习，\n",
    "以及为什么要选择特定的架构。\n",
    "\n",
    "如果我们想用概率图模型来解决这个问题，\n",
    "可以设计一个隐变量模型：\n",
    "在任意时间步$t$，假设存在某个隐变量$h_t$，\n",
    "通过概率$P(x_t \\mid h_t)$控制我们观测到的$x_t$。\n",
    "此外，任何$h_t \\to h_{t+1}$转移\n",
    "都是由一些状态转移概率$P(h_{t+1} \\mid h_{t})$给出。\n",
    "这个概率图模型就是一个*隐马尔可夫模型*（hidden Markov model，HMM），\n",
    "如 :numref:`fig_hmm`所示。\n",
    "\n",
    "![隐马尔可夫模型](../img/hmm.svg)\n",
    ":label:`fig_hmm`\n",
    "\n",
    "因此，对于有$T$个观测值的序列，\n",
    "我们在观测状态和隐状态上具有以下联合概率分布：\n",
    "\n",
    "$$P(x_1, \\ldots, x_T, h_1, \\ldots, h_T) = \\prod_{t=1}^T P(h_t \\mid h_{t-1}) P(x_t \\mid h_t), \\text{ where } P(h_1 \\mid h_0) = P(h_1).$$\n",
    ":eqlabel:`eq_hmm_jointP`\n",
    "\n",
    "现在，假设我们观测到所有的$x_i$，除了$x_j$，\n",
    "并且我们的目标是计算$P(x_j \\mid x_{-j})$，\n",
    "其中$x_{-j} = (x_1, \\ldots, x_{j-1}, x_{j+1}, \\ldots, x_{T})$。\n",
    "由于$P(x_j \\mid x_{-j})$中没有隐变量，\n",
    "因此我们考虑对$h_1, \\ldots, h_T$选择构成的\n",
    "所有可能的组合进行求和。\n",
    "如果任何$h_i$可以接受$k$个不同的值（有限的状态数），\n",
    "这意味着我们需要对$k^T$个项求和，\n",
    "这个任务显然难于登天。\n",
    "幸运的是，有个巧妙的解决方案：*动态规划*（dynamic programming）。\n",
    "\n",
    "要了解动态规划的工作方式，\n",
    "我们考虑对隐变量$h_1, \\ldots, h_T$的依次求和。\n",
    "根据 :eqref:`eq_hmm_jointP`，将得出：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    &P(x_1, \\ldots, x_T) \\\\\n",
    "    =& \\sum_{h_1, \\ldots, h_T} P(x_1, \\ldots, x_T, h_1, \\ldots, h_T) \\\\\n",
    "    =& \\sum_{h_1, \\ldots, h_T} \\prod_{t=1}^T P(h_t \\mid h_{t-1}) P(x_t \\mid h_t) \\\\\n",
    "    =& \\sum_{h_2, \\ldots, h_T} \\underbrace{\\left[\\sum_{h_1} P(h_1) P(x_1 \\mid h_1) P(h_2 \\mid h_1)\\right]}_{\\pi_2(h_2) \\stackrel{\\mathrm{def}}{=}}\n",
    "    P(x_2 \\mid h_2) \\prod_{t=3}^T P(h_t \\mid h_{t-1}) P(x_t \\mid h_t) \\\\\n",
    "    =& \\sum_{h_3, \\ldots, h_T} \\underbrace{\\left[\\sum_{h_2} \\pi_2(h_2) P(x_2 \\mid h_2) P(h_3 \\mid h_2)\\right]}_{\\pi_3(h_3)\\stackrel{\\mathrm{def}}{=}}\n",
    "    P(x_3 \\mid h_3) \\prod_{t=4}^T P(h_t \\mid h_{t-1}) P(x_t \\mid h_t)\\\\\n",
    "    =& \\dots \\\\\n",
    "    =& \\sum_{h_T} \\pi_T(h_T) P(x_T \\mid h_T).\n",
    "\\end{aligned}$$\n",
    "\n",
    "通常，我们将*前向递归*（forward recursion）写为：\n",
    "\n",
    "$$\\pi_{t+1}(h_{t+1}) = \\sum_{h_t} \\pi_t(h_t) P(x_t \\mid h_t) P(h_{t+1} \\mid h_t).$$\n",
    "\n",
    "递归被初始化为$\\pi_1(h_1) = P(h_1)$。\n",
    "符号简化，也可以写成$\\pi_{t+1} = f(\\pi_t, x_t)$，\n",
    "其中$f$是一些可学习的函数。\n",
    "这看起来就像我们在循环神经网络中讨论的隐变量模型中的更新方程。\n",
    "\n",
    "与前向递归一样，我们也可以使用后向递归对同一组隐变量求和。这将得到：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    & P(x_1, \\ldots, x_T) \\\\\n",
    "     =& \\sum_{h_1, \\ldots, h_T} P(x_1, \\ldots, x_T, h_1, \\ldots, h_T) \\\\\n",
    "    =& \\sum_{h_1, \\ldots, h_T} \\prod_{t=1}^{T-1} P(h_t \\mid h_{t-1}) P(x_t \\mid h_t) \\cdot P(h_T \\mid h_{T-1}) P(x_T \\mid h_T) \\\\\n",
    "    =& \\sum_{h_1, \\ldots, h_{T-1}} \\prod_{t=1}^{T-1} P(h_t \\mid h_{t-1}) P(x_t \\mid h_t) \\cdot\n",
    "    \\underbrace{\\left[\\sum_{h_T} P(h_T \\mid h_{T-1}) P(x_T \\mid h_T)\\right]}_{\\rho_{T-1}(h_{T-1})\\stackrel{\\mathrm{def}}{=}} \\\\\n",
    "    =& \\sum_{h_1, \\ldots, h_{T-2}} \\prod_{t=1}^{T-2} P(h_t \\mid h_{t-1}) P(x_t \\mid h_t) \\cdot\n",
    "    \\underbrace{\\left[\\sum_{h_{T-1}} P(h_{T-1} \\mid h_{T-2}) P(x_{T-1} \\mid h_{T-1}) \\rho_{T-1}(h_{T-1}) \\right]}_{\\rho_{T-2}(h_{T-2})\\stackrel{\\mathrm{def}}{=}} \\\\\n",
    "    =& \\ldots \\\\\n",
    "    =& \\sum_{h_1} P(h_1) P(x_1 \\mid h_1)\\rho_{1}(h_{1}).\n",
    "\\end{aligned}$$\n",
    "\n",
    "因此，我们可以将*后向递归*（backward recursion）写为：\n",
    "\n",
    "$$\\rho_{t-1}(h_{t-1})= \\sum_{h_{t}} P(h_{t} \\mid h_{t-1}) P(x_{t} \\mid h_{t}) \\rho_{t}(h_{t}),$$\n",
    "\n",
    "初始化$\\rho_T(h_T) = 1$。\n",
    "前向和后向递归都允许我们对$T$个隐变量在$\\mathcal{O}(kT)$\n",
    "（线性而不是指数）时间内对$(h_1, \\ldots, h_T)$的所有值求和。\n",
    "这是使用图模型进行概率推理的巨大好处之一。\n",
    "它也是通用消息传递算法 :cite:`Aji.McEliece.2000`的一个非常特殊的例子。\n",
    "结合前向和后向递归，我们能够计算\n",
    "\n",
    "$$P(x_j \\mid x_{-j}) \\propto \\sum_{h_j} \\pi_j(h_j) \\rho_j(h_j) P(x_j \\mid h_j).$$\n",
    "\n",
    "因为符号简化的需要，后向递归也可以写为$\\rho_{t-1} = g(\\rho_t, x_t)$，\n",
    "其中$g$是一个可以学习的函数。\n",
    "同样，这看起来非常像一个更新方程，\n",
    "只是不像我们在循环神经网络中看到的那样前向运算，而是后向计算。\n",
    "事实上，知道未来数据何时可用对隐马尔可夫模型是有益的。\n",
    "信号处理学家将是否知道未来观测这两种情况区分为内插和外推，\n",
    "有关更多详细信息，请参阅 :cite:`Doucet.De-Freitas.Gordon.2001`。\n",
    "\n",
    "## 双向模型\n",
    "\n",
    "如果我们希望在循环神经网络中拥有一种机制，\n",
    "使之能够提供与隐马尔可夫模型类似的前瞻能力，\n",
    "我们就需要修改循环神经网络的设计。\n",
    "幸运的是，这在概念上很容易，\n",
    "只需要增加一个“从最后一个词元开始从后向前运行”的循环神经网络，\n",
    "而不是只有一个在前向模式下“从第一个词元开始运行”的循环神经网络。\n",
    "*双向循环神经网络*（bidirectional RNNs）\n",
    "添加了反向传递信息的隐藏层，以便更灵活地处理此类信息。\n",
    " :numref:`fig_birnn`描述了具有单个隐藏层的双向循环神经网络的架构。\n",
    "\n",
    "![双向循环神经网络架构](../img/birnn.svg)\n",
    ":label:`fig_birnn`\n",
    "\n",
    "事实上，这与隐马尔可夫模型中的动态规划的前向和后向递归没有太大区别。\n",
    "其主要区别是，在隐马尔可夫模型中的方程具有特定的统计意义。\n",
    "双向循环神经网络没有这样容易理解的解释，\n",
    "我们只能把它们当作通用的、可学习的函数。\n",
    "这种转变集中体现了现代深度网络的设计原则：\n",
    "首先使用经典统计模型的函数依赖类型，然后将其参数化为通用形式。\n",
    "\n",
    "### 定义\n",
    "\n",
    "双向循环神经网络是由 :cite:`Schuster.Paliwal.1997`提出的，\n",
    "关于各种架构的详细讨论请参阅 :cite:`Graves.Schmidhuber.2005`。\n",
    "让我们看看这样一个网络的细节。\n",
    "\n",
    "对于任意时间步$t$，给定一个小批量的输入数据\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$\n",
    "（样本数$n$，每个示例中的输入数$d$），\n",
    "并且令隐藏层激活函数为$\\phi$。\n",
    "在双向架构中，我们设该时间步的前向和反向隐状态分别为\n",
    "$\\overrightarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$和\n",
    "$\\overleftarrow{\\mathbf{H}}_t  \\in \\mathbb{R}^{n \\times h}$，\n",
    "其中$h$是隐藏单元的数目。\n",
    "前向和反向隐状态的更新如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\overrightarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(f)} + \\overrightarrow{\\mathbf{H}}_{t-1} \\mathbf{W}_{hh}^{(f)}  + \\mathbf{b}_h^{(f)}),\\\\\n",
    "\\overleftarrow{\\mathbf{H}}_t &= \\phi(\\mathbf{X}_t \\mathbf{W}_{xh}^{(b)} + \\overleftarrow{\\mathbf{H}}_{t+1} \\mathbf{W}_{hh}^{(b)}  + \\mathbf{b}_h^{(b)}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，权重$\\mathbf{W}_{xh}^{(f)} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh}^{(f)} \\in \\mathbb{R}^{h \\times h}, \\mathbf{W}_{xh}^{(b)} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh}^{(b)} \\in \\mathbb{R}^{h \\times h}$\n",
    "和偏置$\\mathbf{b}_h^{(f)} \\in \\mathbb{R}^{1 \\times h}, \\mathbf{b}_h^{(b)} \\in \\mathbb{R}^{1 \\times h}$都是模型参数。\n",
    "\n",
    "接下来，将前向隐状态$\\overrightarrow{\\mathbf{H}}_t$\n",
    "和反向隐状态$\\overleftarrow{\\mathbf{H}}_t$连接起来，\n",
    "获得需要送入输出层的隐状态$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times 2h}$。\n",
    "在具有多个隐藏层的深度双向循环神经网络中，\n",
    "该信息作为输入传递到下一个双向层。\n",
    "最后，输出层计算得到的输出为\n",
    "$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$（$q$是输出单元的数目）：\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n",
    "\n",
    "这里，权重矩阵$\\mathbf{W}_{hq} \\in \\mathbb{R}^{2h \\times q}$\n",
    "和偏置$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$\n",
    "是输出层的模型参数。\n",
    "事实上，这两个方向可以拥有不同数量的隐藏单元。\n",
    "\n",
    "### 模型的计算代价及其应用\n",
    "\n",
    "双向循环神经网络的一个关键特性是：使用来自序列两端的信息来估计输出。\n",
    "也就是说，我们使用来自过去和未来的观测信息来预测当前的观测。\n",
    "但是在对下一个词元进行预测的情况中，这样的模型并不是我们所需的。\n",
    "因为在预测下一个词元时，我们终究无法知道下一个词元的下文是什么，\n",
    "所以将不会得到很好的精度。\n",
    "具体地说，在训练期间，我们能够利用过去和未来的数据来估计现在空缺的词；\n",
    "而在测试期间，我们只有过去的数据，因此精度将会很差。\n",
    "下面的实验将说明这一点。\n",
    "\n",
    "另一个严重问题是，双向循环神经网络的计算速度非常慢。\n",
    "其主要原因是网络的前向传播需要在双向层中进行前向和后向递归，\n",
    "并且网络的反向传播还依赖于前向传播的结果。\n",
    "因此，梯度求解将有一个非常长的链。\n",
    "\n",
    "双向层的使用在实践中非常少，并且仅仅应用于部分场合。\n",
    "例如，填充缺失的单词、词元注释（例如，用于命名实体识别）\n",
    "以及作为序列处理流水线中的一个步骤对序列进行编码（例如，用于机器翻译）。\n",
    "在 :numref:`sec_bert`和 :numref:`sec_sentiment_rnn`中，\n",
    "我们将介绍如何使用双向循环神经网络编码文本序列。\n",
    "\n",
    "## (**双向循环神经网络的错误应用**)\n",
    "\n",
    "由于双向循环神经网络使用了过去的和未来的数据，\n",
    "所以我们不能盲目地将这一语言模型应用于任何预测任务。\n",
    "尽管模型产出的困惑度是合理的，\n",
    "该模型预测未来词元的能力却可能存在严重缺陷。\n",
    "我们用下面的示例代码引以为戒，以防在错误的环境中使用它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df876a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import npx\n",
    "from mxnet.gluon import rnn\n",
    "npx.set_np()\n",
    "\n",
    "# 加载数据\n",
    "batch_size, num_steps, device = 32, 35, d2l.try_gpu()\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "# 通过设置“bidirective=True”来定义双向LSTM模型\n",
    "vocab_size, num_hiddens, num_layers = len(vocab), 256, 2\n",
    "lstm_layer = rnn.LSTM(num_hiddens, num_layers, bidirectional=True)\n",
    "model = d2l.RNNModel(lstm_layer, len(vocab))\n",
    "# 训练模型\n",
    "num_epochs, lr = 500, 1\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd08584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 加载数据\n",
    "batch_size, num_steps, device = 32, 35, d2l.try_gpu()\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "# 通过设置“bidirective=True”来定义双向LSTM模型\n",
    "vocab_size, num_hiddens, num_layers = len(vocab), 256, 2\n",
    "num_inputs = vocab_size\n",
    "lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)\n",
    "model = d2l.RNNModel(lstm_layer, len(vocab))\n",
    "model = model.to(device)\n",
    "# 训练模型\n",
    "num_epochs, lr = 500, 1\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37431504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "from paddle import nn\n",
    "\n",
    "#加载数据\n",
    "batch_size, num_steps, device  = 32, 35, d2l.try_gpu()\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n",
    "#通过设置“direction='bidirect'”来定义双向LSTM模型\n",
    "vocab_size, num_hiddens, num_layers = len(vocab), 256, 2\n",
    "num_inputs = vocab_size\n",
    "lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, direction='bidirect', time_major=True)\n",
    "model = d2l.RNNModel(lstm_layer, len(vocab))\n",
    "#训练模型\n",
    "num_epochs, lr = 500, 1.0\n",
    "d2l.train_ch8(model, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677313d3",
   "metadata": {},
   "source": [
    "上述结果显然令人瞠目结舌。\n",
    "关于如何更有效地使用双向循环神经网络的讨论，\n",
    "请参阅 :numref:`sec_sentiment_rnn`中的情感分类应用。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 在双向循环神经网络中，每个时间步的隐状态由当前时间步的前后数据同时决定。\n",
    "* 双向循环神经网络与概率图模型中的“前向-后向”算法具有相似性。\n",
    "* 双向循环神经网络主要用于序列编码和给定双向上下文的观测估计。\n",
    "* 由于梯度链更长，因此双向循环神经网络的训练代价非常高。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果不同方向使用不同数量的隐藏单位，$\\mathbf{H_t}$的形状会发生怎样的变化？\n",
    "1. 设计一个具有多个隐藏层的双向循环神经网络。\n",
    "1. 在自然语言中一词多义很常见。例如，“bank”一词在不同的上下文“i went to the bank to deposit cash”和“i went to the bank to sit down”中有不同的含义。如何设计一个神经网络模型，使其在给定上下文序列和单词的情况下，返回该单词在此上下文中的向量表示？哪种类型的神经网络架构更适合处理一词多义？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2774)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2773)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11835)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642efce",
   "metadata": {},
   "source": [
    "# 机器翻译与数据集\n",
    ":label:`sec_machine_translation`\n",
    "\n",
    "语言模型是自然语言处理的关键，\n",
    "而*机器翻译*是语言模型最成功的基准测试。\n",
    "因为机器翻译正是将输入序列转换成输出序列的\n",
    "*序列转换模型*（sequence transduction）的核心问题。\n",
    "序列转换模型在各类现代人工智能应用中发挥着至关重要的作用，\n",
    "因此我们将其做为本章剩余部分和 :numref:`chap_attention`的重点。\n",
    "为此，本节将介绍机器翻译问题及其后文需要使用的数据集。\n",
    "\n",
    "*机器翻译*（machine translation）指的是\n",
    "将序列从一种语言自动翻译成另一种语言。\n",
    "事实上，这个研究领域可以追溯到数字计算机发明后不久的20世纪40年代，\n",
    "特别是在第二次世界大战中使用计算机破解语言编码。\n",
    "几十年来，在使用神经网络进行端到端学习的兴起之前，\n",
    "统计学方法在这一领域一直占据主导地位\n",
    " :cite:`Brown.Cocke.Della-Pietra.ea.1988,Brown.Cocke.Della-Pietra.ea.1990`。\n",
    "因为*统计机器翻译*（statistical machine translation）涉及了\n",
    "翻译模型和语言模型等组成部分的统计分析，\n",
    "因此基于神经网络的方法通常被称为\n",
    "*神经机器翻译*（neural machine translation），\n",
    "用于将两种翻译模型区分开来。\n",
    "\n",
    "本书的关注点是神经网络机器翻译方法，强调的是端到端的学习。\n",
    "与 :numref:`sec_language_model`中的语料库\n",
    "是单一语言的语言模型问题存在不同，\n",
    "机器翻译的数据集是由源语言和目标语言的文本序列对组成的。\n",
    "因此，我们需要一种完全不同的方法来预处理机器翻译数据集，\n",
    "而不是复用语言模型的预处理程序。\n",
    "下面，我们看一下如何将预处理后的数据加载到小批量中用于训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538551ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "import os\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae3916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ce0c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d007ade5",
   "metadata": {},
   "source": [
    "## [**下载和预处理数据集**]\n",
    "\n",
    "首先，下载一个由[Tatoeba项目的双语句子对](http://www.manythings.org/anki/)\n",
    "组成的“英－法”数据集，数据集中的每一行都是制表符分隔的文本序列对，\n",
    "序列对由英文文本序列和翻译后的法语文本序列组成。\n",
    "请注意，每个文本序列可以是一个句子，\n",
    "也可以是包含多个句子的一个段落。\n",
    "在这个将英语翻译成法语的机器翻译问题中，\n",
    "英语是*源语言*（source language），\n",
    "法语是*目标语言*（target language）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45ff30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "d2l.DATA_HUB['fra-eng'] = (d2l.DATA_URL + 'fra-eng.zip',\n",
    "                           '94646ad1522d915e7b0f9296181140edcf86a4f5')\n",
    "\n",
    "#@save\n",
    "def read_data_nmt():\n",
    "    \"\"\"载入“英语－法语”数据集\"\"\"\n",
    "    data_dir = d2l.download_extract('fra-eng')\n",
    "    with open(os.path.join(data_dir, 'fra.txt'), 'r', \n",
    "             encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "raw_text = read_data_nmt()\n",
    "print(raw_text[:75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b846c99",
   "metadata": {},
   "source": [
    "下载数据集后，原始文本数据需要经过[**几个预处理步骤**]。\n",
    "例如，我们用空格代替*不间断空格*（non-breaking space），\n",
    "使用小写字母替换大写字母，并在单词和标点符号之间插入空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def preprocess_nmt(text):\n",
    "    \"\"\"预处理“英语－法语”数据集\"\"\"\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    # 使用空格替换不间断空格\n",
    "    # 使用小写字母替换大写字母\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    # 在单词和标点符号之间插入空格\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "print(text[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc45efc",
   "metadata": {},
   "source": [
    "## [**词元化**]\n",
    "\n",
    "与 :numref:`sec_language_model`中的字符级词元化不同，\n",
    "在机器翻译中，我们更喜欢单词级词元化\n",
    "（最先进的模型可能使用更高级的词元化技术）。\n",
    "下面的`tokenize_nmt`函数对前`num_examples`个文本序列对进行词元，\n",
    "其中每个词元要么是一个词，要么是一个标点符号。\n",
    "此函数返回两个词元列表：`source`和`target`：\n",
    "`source[i]`是源语言（这里是英语）第$i$个文本序列的词元列表，\n",
    "`target[i]`是目标语言（这里是法语）第$i$个文本序列的词元列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22259e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def tokenize_nmt(text, num_examples=None):\n",
    "    \"\"\"词元化“英语－法语”数据数据集\"\"\"\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "source, target = tokenize_nmt(text)\n",
    "source[:6], target[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26797a0b",
   "metadata": {},
   "source": [
    "让我们[**绘制每个文本序列所包含的词元数量的直方图**]。\n",
    "在这个简单的“英－法”数据集中，大多数文本序列的词元数量少于$20$个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b93c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist):\n",
    "    \"\"\"绘制列表长度对的直方图\"\"\"\n",
    "    d2l.set_figsize()\n",
    "    _, _, patches = d2l.plt.hist(\n",
    "        [[len(l) for l in xlist], [len(l) for l in ylist]])\n",
    "    d2l.plt.xlabel(xlabel)\n",
    "    d2l.plt.ylabel(ylabel)\n",
    "    for patch in patches[1].patches:\n",
    "        patch.set_hatch('/')\n",
    "    d2l.plt.legend(legend)\n",
    "\n",
    "show_list_len_pair_hist(['source', 'target'], '# tokens per sequence',\n",
    "                        'count', source, target);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8bda5",
   "metadata": {},
   "source": [
    "## [**词表**]\n",
    "\n",
    "由于机器翻译数据集由语言对组成，\n",
    "因此我们可以分别为源语言和目标语言构建两个词表。\n",
    "使用单词级词元化时，词表大小将明显大于使用字符级词元化时的词表大小。\n",
    "为了缓解这一问题，这里我们将出现次数少于2次的低频率词元\n",
    "视为相同的未知（“&lt;unk&gt;”）词元。\n",
    "除此之外，我们还指定了额外的特定词元，\n",
    "例如在小批量时用于将序列填充到相同长度的填充词元（“&lt;pad&gt;”），\n",
    "以及序列的开始词元（“&lt;bos&gt;”）和结束词元（“&lt;eos&gt;”）。\n",
    "这些特殊词元在自然语言处理任务中比较常用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517fb1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "src_vocab = d2l.Vocab(source, min_freq=2,\n",
    "                      reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "len(src_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588c5cd",
   "metadata": {},
   "source": [
    "## 加载数据集\n",
    ":label:`subsec_mt_data_loading`\n",
    "\n",
    "回想一下，语言模型中的[**序列样本都有一个固定的长度**]，\n",
    "无论这个样本是一个句子的一部分还是跨越了多个句子的一个片断。\n",
    "这个固定长度是由 :numref:`sec_language_model`中的\n",
    "`num_steps`（时间步数或词元数量）参数指定的。\n",
    "在机器翻译中，每个样本都是由源和目标组成的文本序列对，\n",
    "其中的每个文本序列可能具有不同的长度。\n",
    "\n",
    "为了提高计算效率，我们仍然可以通过*截断*（truncation）和\n",
    "*填充*（padding）方式实现一次只处理一个小批量的文本序列。\n",
    "假设同一个小批量中的每个序列都应该具有相同的长度`num_steps`，\n",
    "那么如果文本序列的词元数目少于`num_steps`时，\n",
    "我们将继续在其末尾添加特定的“&lt;pad&gt;”词元，\n",
    "直到其长度达到`num_steps`；\n",
    "反之，我们将截断文本序列时，只取其前`num_steps` 个词元，\n",
    "并且丢弃剩余的词元。这样，每个文本序列将具有相同的长度，\n",
    "以便以相同形状的小批量进行加载。\n",
    "\n",
    "如前所述，下面的`truncate_pad`函数将(**截断或填充文本序列**)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"截断或填充文本序列\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # 截断\n",
    "    return line + [padding_token] * (num_steps - len(line))  # 填充\n",
    "\n",
    "truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc19762",
   "metadata": {},
   "source": [
    "现在我们定义一个函数，可以将文本序列\n",
    "[**转换成小批量数据集用于训练**]。\n",
    "我们将特定的“&lt;eos&gt;”词元添加到所有序列的末尾，\n",
    "用于表示序列的结束。\n",
    "当模型通过一个词元接一个词元地生成序列进行预测时，\n",
    "生成的“&lt;eos&gt;”词元说明完成了序列输出工作。\n",
    "此外，我们还记录了每个文本序列的长度，\n",
    "统计长度时排除了填充词元，\n",
    "在稍后将要介绍的一些模型会需要这个长度信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15825e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def build_array_nmt(lines, vocab, num_steps):\n",
    "    \"\"\"将机器翻译的文本序列转换成小批量\"\"\"\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = d2l.tensor([truncate_pad(\n",
    "        l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = d2l.reduce_sum(\n",
    "        d2l.astype(array != vocab['<pad>'], d2l.int32), 1)\n",
    "    return array, valid_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222268d",
   "metadata": {},
   "source": [
    "## [**训练模型**]\n",
    "\n",
    "最后，我们定义`load_data_nmt`函数来返回数据迭代器，\n",
    "以及源语言和目标语言的两种词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def load_data_nmt(batch_size, num_steps, num_examples=600):\n",
    "    \"\"\"返回翻译数据集的迭代器和词表\"\"\"\n",
    "    text = preprocess_nmt(read_data_nmt())\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "    src_vocab = d2l.Vocab(source, min_freq=2,\n",
    "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = d2l.Vocab(target, min_freq=2,\n",
    "                          reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = d2l.load_array(data_arrays, batch_size)\n",
    "    return data_iter, src_vocab, tgt_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a375c80",
   "metadata": {},
   "source": [
    "下面我们[**读出“英语－法语”数据集中的第一个小批量数据**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e5279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size=2, num_steps=8)\n",
    "for X, X_valid_len, Y, Y_valid_len in train_iter:\n",
    "    print('X:', d2l.astype(X, d2l.int32))\n",
    "    print('X的有效长度:', X_valid_len)\n",
    "    print('Y:', d2l.astype(Y, d2l.int32))\n",
    "    print('Y的有效长度:', Y_valid_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b4501",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 机器翻译指的是将文本序列从一种语言自动翻译成另一种语言。\n",
    "* 使用单词级词元化时的词表大小，将明显大于使用字符级词元化时的词表大小。为了缓解这一问题，我们可以将低频词元视为相同的未知词元。\n",
    "* 通过截断和填充文本序列，可以保证所有的文本序列都具有相同的长度，以便以小批量的方式加载。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 在`load_data_nmt`函数中尝试不同的`num_examples`参数值。这对源语言和目标语言的词表大小有何影响？\n",
    "1. 某些语言（例如中文和日语）的文本没有单词边界指示符（例如空格）。对于这种情况，单词级词元化仍然是个好主意吗？为什么？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2777)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2776)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11836)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471a795",
   "metadata": {},
   "source": [
    "# 编码器-解码器架构\n",
    ":label:`sec_encoder-decoder`\n",
    "\n",
    "正如我们在 :numref:`sec_machine_translation`中所讨论的，\n",
    "机器翻译是序列转换模型的一个核心问题，\n",
    "其输入和输出都是长度可变的序列。\n",
    "为了处理这种类型的输入和输出，\n",
    "我们可以设计一个包含两个主要组件的架构：\n",
    "第一个组件是一个*编码器*（encoder）：\n",
    "它接受一个长度可变的序列作为输入，\n",
    "并将其转换为具有固定形状的编码状态。\n",
    "第二个组件是*解码器*（decoder）：\n",
    "它将固定形状的编码状态映射到长度可变的序列。\n",
    "这被称为*编码器-解码器*（encoder-decoder）架构，\n",
    "如 :numref:`fig_encoder_decoder` 所示。\n",
    "\n",
    "![编码器-解码器架构](../img/encoder-decoder.svg)\n",
    ":label:`fig_encoder_decoder`\n",
    "\n",
    "我们以英语到法语的机器翻译为例：\n",
    "给定一个英文的输入序列：“They”“are”“watching”“.”。\n",
    "首先，这种“编码器－解码器”架构将长度可变的输入序列编码成一个“状态”，\n",
    "然后对该状态进行解码，\n",
    "一个词元接着一个词元地生成翻译后的序列作为输出：\n",
    "“Ils”“regordent”“.”。\n",
    "由于“编码器－解码器”架构是形成后续章节中不同序列转换模型的基础，\n",
    "因此本节将把这个架构转换为接口方便后面的代码实现。\n",
    "\n",
    "## (**编码器**)\n",
    "\n",
    "在编码器接口中，我们只指定长度可变的序列作为编码器的输入`X`。\n",
    "任何继承这个`Encoder`基类的模型将完成代码实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "\n",
    "#@save\n",
    "class Encoder(nn.Block):\n",
    "    \"\"\"编码器-解码器架构的基本编码器接口\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb361d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from torch import nn\n",
    "\n",
    "#@save\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本编码器接口\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c24cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#@save\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"编码器-解码器架构的基本编码器接口\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, X, *args, **kwargs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from paddle import nn\n",
    "\n",
    "#@save\n",
    "class Encoder(nn.Layer):\n",
    "    \"\"\"编码器-解码器架构的基本编码器接口\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19934641",
   "metadata": {},
   "source": [
    "## [**解码器**]\n",
    "\n",
    "在下面的解码器接口中，我们新增一个`init_state`函数，\n",
    "用于将编码器的输出（`enc_outputs`）转换为编码后的状态。\n",
    "注意，此步骤可能需要额外的输入，例如：输入序列的有效长度，\n",
    "这在 :numref:`subsec_mt_data_loading`中进行了解释。\n",
    "为了逐个地生成长度可变的词元序列，\n",
    "解码器在每个时间步都会将输入\n",
    "（例如：在前一时间步生成的词元）和编码后的状态\n",
    "映射成当前时间步的输出词元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ded638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class Decoder(nn.Block):\n",
    "    \"\"\"编码器-解码器架构的基本解码器接口\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0678cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本解码器接口\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb71a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "#@save\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"编码器-解码器架构的基本解码器接口\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def call(self, X, state, **kwargs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "class Decoder(nn.Layer):\n",
    "    \"\"\"编码器-解码器架构的基本解码器接口\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03809603",
   "metadata": {},
   "source": [
    "## [**合并编码器和解码器**]\n",
    "\n",
    "总而言之，“编码器-解码器”架构包含了一个编码器和一个解码器，\n",
    "并且还拥有可选的额外的参数。\n",
    "在前向传播中，编码器的输出用于生成编码状态，\n",
    "这个状态又被解码器作为其输入的一部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd196c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class EncoderDecoder(nn.Block):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e016a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29df264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "#@save\n",
    "class EncoderDecoder(tf.keras.Model):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, enc_X, dec_X, *args, **kwargs):\n",
    "        enc_outputs = self.encoder(enc_X, *args, **kwargs)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5089a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "class EncoderDecoder(nn.Layer):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba8654f",
   "metadata": {},
   "source": [
    "“编码器－解码器”体系架构中的术语*状态*\n",
    "会启发人们使用具有状态的神经网络来实现该架构。\n",
    "在下一节中，我们将学习如何应用循环神经网络，\n",
    "来设计基于“编码器－解码器”架构的序列转换模型。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* “编码器－解码器”架构可以将长度可变的序列作为输入和输出，因此适用于机器翻译等序列转换问题。\n",
    "* 编码器将长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。\n",
    "* 解码器将具有固定形状的编码状态映射为长度可变的序列。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 假设我们使用神经网络来实现“编码器－解码器”架构，那么编码器和解码器必须是同一类型的神经网络吗？\n",
    "1. 除了机器翻译，还有其它可以适用于”编码器－解码器“架构的应用吗？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2780)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2779)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11837)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090b8a2",
   "metadata": {},
   "source": [
    "#  序列到序列学习（seq2seq）\n",
    ":label:`sec_seq2seq`\n",
    "\n",
    "正如我们在 :numref:`sec_machine_translation`中看到的，\n",
    "机器翻译中的输入序列和输出序列都是长度可变的。\n",
    "为了解决这类问题，我们在 :numref:`sec_encoder-decoder`中\n",
    "设计了一个通用的”编码器－解码器“架构。\n",
    "本节，我们将使用两个循环神经网络的编码器和解码器，\n",
    "并将其应用于*序列到序列*（sequence to sequence，seq2seq）类的学习任务\n",
    " :cite:`Sutskever.Vinyals.Le.2014,Cho.Van-Merrienboer.Gulcehre.ea.2014`。\n",
    "\n",
    "遵循编码器－解码器架构的设计原则，\n",
    "循环神经网络编码器使用长度可变的序列作为输入，\n",
    "将其转换为固定形状的隐状态。\n",
    "换言之，输入序列的信息被*编码*到循环神经网络编码器的隐状态中。\n",
    "为了连续生成输出序列的词元，\n",
    "独立的循环神经网络解码器是基于输入序列的编码信息\n",
    "和输出序列已经看见的或者生成的词元来预测下一个词元。\n",
    " :numref:`fig_seq2seq`演示了\n",
    "如何在机器翻译中使用两个循环神经网络进行序列到序列学习。\n",
    "\n",
    "![使用循环神经网络编码器和循环神经网络解码器的序列到序列学习](../img/seq2seq.svg)\n",
    ":label:`fig_seq2seq`\n",
    "\n",
    "在 :numref:`fig_seq2seq`中，\n",
    "特定的“&lt;eos&gt;”表示序列结束词元。\n",
    "一旦输出序列生成此词元，模型就会停止预测。\n",
    "在循环神经网络解码器的初始化时间步，有两个特定的设计决定：\n",
    "首先，特定的“&lt;bos&gt;”表示序列开始词元，它是解码器的输入序列的第一个词元。\n",
    "其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。\n",
    "例如，在 :cite:`Sutskever.Vinyals.Le.2014`的设计中，\n",
    "正是基于这种设计将输入序列的编码信息送入到解码器中来生成输出序列的。\n",
    "在其他一些设计中 :cite:`Cho.Van-Merrienboer.Gulcehre.ea.2014`，\n",
    "如 :numref:`fig_seq2seq`所示，\n",
    "编码器最终的隐状态在每一个时间步都作为解码器的输入序列的一部分。\n",
    "类似于 :numref:`sec_language_model`中语言模型的训练，\n",
    "可以允许标签成为原始的输出序列，\n",
    "从源序列词元“&lt;bos&gt;”“Ils”“regardent”“.”\n",
    "到新序列词元\n",
    "“Ils”“regardent”“.”“&lt;eos&gt;”来移动预测的位置。\n",
    "\n",
    "下面，我们动手构建 :numref:`fig_seq2seq`的设计，\n",
    "并将基于 :numref:`sec_machine_translation`中\n",
    "介绍的“英－法”数据集来训练这个机器翻译模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1473b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from d2l import mxnet as d2l\n",
    "import math\n",
    "from mxnet import np, npx, init, gluon, autograd\n",
    "from mxnet.gluon import nn, rnn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b99f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import collections\n",
    "from d2l import torch as d2l\n",
    "import math\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfc422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import collections\n",
    "from d2l import tensorflow as d2l\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c8e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import collections\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "import paddle\n",
    "from paddle import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdaeee2",
   "metadata": {},
   "source": [
    "## 编码器\n",
    "\n",
    "从技术上讲，编码器将长度可变的输入序列转换成\n",
    "形状固定的上下文变量$\\mathbf{c}$，\n",
    "并且将输入序列的信息在该上下文变量中进行编码。\n",
    "如 :numref:`fig_seq2seq`所示，可以使用循环神经网络来设计编码器。\n",
    "\n",
    "考虑由一个序列组成的样本（批量大小是$1$）。\n",
    "假设输入序列是$x_1, \\ldots, x_T$，\n",
    "其中$x_t$是输入文本序列中的第$t$个词元。\n",
    "在时间步$t$，循环神经网络将词元$x_t$的输入特征向量\n",
    "$\\mathbf{x}_t$和$\\mathbf{h} _{t-1}$（即上一时间步的隐状态）\n",
    "转换为$\\mathbf{h}_t$（即当前步的隐状态）。\n",
    "使用一个函数$f$来描述循环神经网络的循环层所做的变换：\n",
    "\n",
    "$$\\mathbf{h}_t = f(\\mathbf{x}_t, \\mathbf{h}_{t-1}). $$\n",
    "\n",
    "总之，编码器通过选定的函数$q$，\n",
    "将所有时间步的隐状态转换为上下文变量：\n",
    "\n",
    "$$\\mathbf{c} =  q(\\mathbf{h}_1, \\ldots, \\mathbf{h}_T).$$\n",
    "\n",
    "比如，当选择$q(\\mathbf{h}_1, \\ldots, \\mathbf{h}_T) = \\mathbf{h}_T$时\n",
    "（就像 :numref:`fig_seq2seq`中一样），\n",
    "上下文变量仅仅是输入序列在最后时间步的隐状态$\\mathbf{h}_T$。\n",
    "\n",
    "到目前为止，我们使用的是一个单向循环神经网络来设计编码器，\n",
    "其中隐状态只依赖于输入子序列，\n",
    "这个子序列是由输入序列的开始位置到隐状态所在的时间步的位置\n",
    "（包括隐状态所在的时间步）组成。\n",
    "我们也可以使用双向循环神经网络构造编码器，\n",
    "其中隐状态依赖于两个输入子序列，\n",
    "两个子序列是由隐状态所在的时间步的位置之前的序列和之后的序列\n",
    "（包括隐状态所在的时间步），\n",
    "因此隐状态对整个序列的信息都进行了编码。\n",
    "\n",
    "现在，让我们[**实现循环神经网络编码器**]。\n",
    "注意，我们使用了*嵌入层*（embedding layer）\n",
    "来获得输入序列中每个词元的特征向量。\n",
    "嵌入层的权重是一个矩阵，\n",
    "其行数等于输入词表的大小（`vocab_size`），\n",
    "其列数等于特征向量的维度（`embed_size`）。\n",
    "对于任意输入词元的索引$i$，\n",
    "嵌入层获取权重矩阵的第$i$行（从$0$开始）以返回其特征向量。\n",
    "另外，本文选择了一个多层门控循环单元来实现编码器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbd78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络编码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X)\n",
    "        # 在循环神经网络模型中，第一个轴对应于时间步\n",
    "        X = X.swapaxes(0, 1)\n",
    "        state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.ctx)\n",
    "        output, state = self.rnn(X, state)\n",
    "        # output的形状:(num_steps,batch_size,num_hiddens)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb97f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络编码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X)\n",
    "        # 在循环神经网络模型中，第一个轴对应于时间步\n",
    "        X = X.permute(1, 0, 2)\n",
    "        # 如果未提及状态，则默认为0\n",
    "        output, state = self.rnn(X)\n",
    "        # output的形状:(num_steps,batch_size,num_hiddens)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fccdf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "#@save\n",
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络编码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): \n",
    "        super().__init__(*kwargs)\n",
    "        # 嵌入层\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells(\n",
    "            [tf.keras.layers.GRUCell(num_hiddens, dropout=dropout)\n",
    "             for _ in range(num_layers)]), return_sequences=True,\n",
    "                                       return_state=True)\n",
    "    \n",
    "    def call(self, X, *args, **kwargs):\n",
    "        # 输入'X'的形状：(batch_size,num_steps)\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X)\n",
    "        output = self.rnn(X, **kwargs)\n",
    "        state = output[1:]\n",
    "        return output[0], state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68045dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络编码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        weight_ih_attr = paddle.ParamAttr(initializer=nn.initializer.XavierUniform())\n",
    "        weight_hh_attr = paddle.ParamAttr(initializer=nn.initializer.XavierUniform())\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout,\n",
    "                          time_major=True, weight_ih_attr=weight_ih_attr, weight_hh_attr=weight_hh_attr)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X)\n",
    "        # 在循环神经网络模型中，第一个轴对应于时间步\n",
    "        X = X.transpose([1, 0, 2])\n",
    "        # 如果未提及状态，则默认为0\n",
    "        output, state = self.rnn(X)\n",
    "        # PaddlePaddle的GRU层output的形状:(batch_size,time_steps,num_directions * num_hiddens),\n",
    "        # 需设定time_major=True,指定input的第一个维度为time_steps\n",
    "        # state[0]的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634e4b5",
   "metadata": {},
   "source": [
    "循环层返回变量的说明可以参考 :numref:`sec_rnn-concise`。\n",
    "\n",
    "下面，我们实例化[**上述编码器的实现**]：\n",
    "我们使用一个两层门控循环单元编码器，其隐藏单元数为$16$。\n",
    "给定一小批量的输入序列`X`（批量大小为$4$，时间步为$7$）。\n",
    "在完成所有时间步后，\n",
    "最后一层的隐状态的输出是一个张量（`output`由编码器的循环层返回），\n",
    "其形状为（时间步数，批量大小，隐藏单元数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "encoder.initialize()\n",
    "X = d2l.zeros((4, 7))\n",
    "output, state = encoder(X)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e706fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "encoder.eval()\n",
    "X = d2l.zeros((4, 7), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "X = tf.zeros((4, 7))\n",
    "output, state = encoder(X, training=False)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "encoder.eval()\n",
    "X = d2l.zeros((4, 7), dtype=paddle.int64)\n",
    "output, state = encoder(X)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1aa45d",
   "metadata": {},
   "source": [
    "由于这里使用的是门控循环单元，\n",
    "所以在最后一个时间步的多层隐状态的形状是\n",
    "（隐藏层的数量，批量大小，隐藏单元的数量）。\n",
    "如果使用长短期记忆网络，`state`中还将包含记忆单元信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f959e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(state), state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae49c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0301a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "len(state), [element.shape for element in state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e64ac",
   "metadata": {},
   "source": [
    "## [**解码器**]\n",
    ":label:`sec_seq2seq_decoder`\n",
    "\n",
    "正如上文提到的，编码器输出的上下文变量$\\mathbf{c}$\n",
    "对整个输入序列$x_1, \\ldots, x_T$进行编码。\n",
    "来自训练数据集的输出序列$y_1, y_2, \\ldots, y_{T'}$，\n",
    "对于每个时间步$t'$（与输入序列或编码器的时间步$t$不同），\n",
    "解码器输出$y_{t'}$的概率取决于先前的输出子序列\n",
    "$y_1, \\ldots, y_{t'-1}$和上下文变量$\\mathbf{c}$，\n",
    "即$P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c})$。\n",
    "\n",
    "为了在序列上模型化这种条件概率，\n",
    "我们可以使用另一个循环神经网络作为解码器。\n",
    "在输出序列上的任意时间步$t^\\prime$，\n",
    "循环神经网络将来自上一时间步的输出$y_{t^\\prime-1}$\n",
    "和上下文变量$\\mathbf{c}$作为其输入，\n",
    "然后在当前时间步将它们和上一隐状态\n",
    "$\\mathbf{s}_{t^\\prime-1}$转换为\n",
    "隐状态$\\mathbf{s}_{t^\\prime}$。\n",
    "因此，可以使用函数$g$来表示解码器的隐藏层的变换：\n",
    "\n",
    "$$\\mathbf{s}_{t^\\prime} = g(y_{t^\\prime-1}, \\mathbf{c}, \\mathbf{s}_{t^\\prime-1}).$$\n",
    ":eqlabel:`eq_seq2seq_s_t`\n",
    "\n",
    "在获得解码器的隐状态之后，\n",
    "我们可以使用输出层和softmax操作\n",
    "来计算在时间步$t^\\prime$时输出$y_{t^\\prime}$的条件概率分布\n",
    "$P(y_{t^\\prime} \\mid y_1, \\ldots, y_{t^\\prime-1}, \\mathbf{c})$。\n",
    "\n",
    "根据 :numref:`fig_seq2seq`，当实现解码器时，\n",
    "我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。\n",
    "这就要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。\n",
    "为了进一步包含经过编码的输入序列的信息，\n",
    "上下文变量在所有的时间步与解码器的输入进行拼接（concatenate）。\n",
    "为了预测输出词元的概率分布，\n",
    "在循环神经网络解码器的最后一层使用全连接层来变换隐状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88003901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络解码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Dense(vocab_size, flatten=False)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X).swapaxes(0, 1)\n",
    "        # context的形状:(batch_size,num_hiddens)\n",
    "        context = state[0][-1]\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        context = np.broadcast_to(context, (\n",
    "            X.shape[0], context.shape[0], context.shape[1]))\n",
    "        X_and_context = d2l.concat((X, context), 2)\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).swapaxes(0, 1)\n",
    "        # output的形状:(batch_size,num_steps,vocab_size)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络解码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        X_and_context = d2l.concat((X, context), 2)\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # output的形状:(batch_size,num_steps,vocab_size)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01a28e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络解码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells(\n",
    "            [tf.keras.layers.GRUCell(num_hiddens, dropout=dropout)\n",
    "             for _ in range(num_layers)]), return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "    \n",
    "    def call(self, X, state, **kwargs):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X)\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        context = tf.repeat(tf.expand_dims(state[-1], axis=1), repeats=X.shape[1], axis=1)\n",
    "        X_and_context = tf.concat((X, context), axis=2)\n",
    "        rnn_output = self.rnn(X_and_context, state, **kwargs)\n",
    "        output = self.dense(rnn_output[0])\n",
    "        # output的形状:(batch_size,num_steps,vocab_size)\n",
    "        # state是一个包含num_layers个元素的列表，每个元素的形状:(batch_size,num_hiddens)\n",
    "        return output, rnn_output[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711eac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络解码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        weight_attr = paddle.ParamAttr(initializer=nn.initializer.XavierUniform())\n",
    "        weight_ih_attr = paddle.ParamAttr(initializer=nn.initializer.XavierUniform())\n",
    "        weight_hh_attr = paddle.ParamAttr(initializer=nn.initializer.XavierUniform())\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout,\n",
    "                          time_major=True, weight_ih_attr=weight_ih_attr,weight_hh_attr=weight_hh_attr)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size,weight_attr=weight_attr)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X).transpose([1, 0, 2])\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        context = state[-1].tile([X.shape[0], 1, 1])\n",
    "        X_and_context = d2l.concat((X, context), 2)\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).transpose([1, 0, 2])\n",
    "        # output的形状:(batch_size,num_steps,vocab_size)\n",
    "        # state[0]的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42d71c",
   "metadata": {},
   "source": [
    "下面，我们用与前面提到的编码器中相同的超参数来[**实例化解码器**]。\n",
    "如我们所见，解码器的输出形状变为（批量大小，时间步数，词表大小），\n",
    "其中张量的最后一个维度存储预测的词元分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36583565",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.initialize()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, len(state), state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f345c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state, training=False)\n",
    "output.shape, len(state), state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1eeef",
   "metadata": {},
   "source": [
    "总之，上述循环神经网络“编码器－解码器”模型中的各层如\n",
    " :numref:`fig_seq2seq_details`所示。\n",
    "\n",
    "![循环神经网络编码器-解码器模型中的层](../img/seq2seq-details.svg)\n",
    ":label:`fig_seq2seq_details`\n",
    "\n",
    "## 损失函数\n",
    "\n",
    "在每个时间步，解码器预测了输出词元的概率分布。\n",
    "类似于语言模型，可以使用softmax来获得分布，\n",
    "并通过计算交叉熵损失函数来进行优化。\n",
    "回想一下 :numref:`sec_machine_translation`中，\n",
    "特定的填充词元被添加到序列的末尾，\n",
    "因此不同长度的序列可以以相同形状的小批量加载。\n",
    "但是，我们应该将填充词元的预测排除在损失函数的计算之外。\n",
    "\n",
    "为此，我们可以使用下面的`sequence_mask`函数\n",
    "[**通过零值化屏蔽不相关的项**]，\n",
    "以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。\n",
    "例如，如果两个序列的有效长度（不包括填充词元）分别为$1$和$2$，\n",
    "则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "npx.sequence_mask(X, np.array([1, 2]), True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8c3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "sequence_mask(X, torch.tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396cb25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "#@save\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    maxlen = X.shape[1]\n",
    "    mask = tf.range(start=0, limit=maxlen, dtype=tf.float32)[\n",
    "        None, :] < tf.cast(valid_len[:, None], dtype=tf.float32)\n",
    "    \n",
    "    if len(X.shape) == 3:\n",
    "        return tf.where(tf.expand_dims(mask, axis=-1), X, value)\n",
    "    else:\n",
    "        return tf.where(mask, X, value)\n",
    "    \n",
    "X = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "sequence_mask(X, tf.constant([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976fb2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    maxlen = X.shape[1]\n",
    "    mask = paddle.arange((maxlen), dtype=paddle.float32)[None, :] < valid_len[:, None]\n",
    "    Xtype = X.dtype\n",
    "    X = X.astype(paddle.float32)\n",
    "    X[~mask] = float(value)\n",
    "    return X.astype(Xtype)\n",
    "\n",
    "X = paddle.to_tensor([[1, 2, 3], [4, 5, 6]])\n",
    "sequence_mask(X, paddle.to_tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a43f925",
   "metadata": {},
   "source": [
    "(**我们还可以使用此函数屏蔽最后几个轴上的所有项。**)如果愿意，也可以使用指定的非零值来替换这些项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541df8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d2l.ones((2, 3, 4))\n",
    "npx.sequence_mask(X, np.array([1, 2]), True, value=-1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849af66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "X = d2l.ones(2, 3, 4)\n",
    "sequence_mask(X, torch.tensor([1, 2]), value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b88da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "X = tf.ones((2,3,4))\n",
    "sequence_mask(X, tf.constant([1, 2]), value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "X = d2l.ones([2, 3, 4])\n",
    "sequence_mask(X, paddle.to_tensor([1, 2]), value=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d2c400",
   "metadata": {},
   "source": [
    "现在，我们可以[**通过扩展softmax交叉熵损失函数来遮蔽不相关的预测**]。\n",
    "最初，所有预测词元的掩码都设置为1。\n",
    "一旦给定了有效长度，与填充词元对应的掩码将被设置为0。\n",
    "最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class MaskedSoftmaxCELoss(gluon.loss.SoftmaxCELoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    # pred的形状：(batch_size,num_steps,vocab_size)\n",
    "    # label的形状：(batch_size,num_steps)\n",
    "    # valid_len的形状：(batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        # weights的形状：(batch_size,num_steps,1)\n",
    "        weights = np.expand_dims(np.ones_like(label), axis=-1)\n",
    "        weights = npx.sequence_mask(weights, valid_len, True, axis=1)\n",
    "        return super(MaskedSoftmaxCELoss, self).forward(pred, label, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee77380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    # pred的形状：(batch_size,num_steps,vocab_size)\n",
    "    # label的形状：(batch_size,num_steps)\n",
    "    # valid_len的形状：(batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d85bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "#@save\n",
    "class MaskedSoftmaxCELoss(tf.keras.losses.Loss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    def __init__(self, valid_len):\n",
    "        super().__init__(reduction='none')\n",
    "        self.valid_len = valid_len\n",
    "    \n",
    "    # pred的形状：(batch_size,num_steps,vocab_size)\n",
    "    # label的形状：(batch_size,num_steps)\n",
    "    # valid_len的形状：(batch_size,)\n",
    "    def call(self, label, pred):\n",
    "        weights = tf.ones_like(label, dtype=tf.float32)\n",
    "        weights = sequence_mask(weights, self.valid_len)\n",
    "        label_one_hot = tf.one_hot(label, depth=pred.shape[-1])\n",
    "        unweighted_loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')(label_one_hot, pred)\n",
    "        weighted_loss = tf.reduce_mean((unweighted_loss*weights), axis=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfed8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    # pred的形状：(batch_size,num_steps,vocab_size)\n",
    "    # label的形状：(batch_size,num_steps)\n",
    "    # valid_len的形状：(batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = paddle.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred, label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(axis=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66eb8c8",
   "metadata": {},
   "source": [
    "我们可以创建三个相同的序列来进行[**代码健全性检查**]，\n",
    "然后分别指定这些序列的有效长度为$4$、$2$和$0$。\n",
    "结果就是，第一个序列的损失应为第二个序列的两倍，而第三个序列的损失应为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48eb427",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(d2l.ones((3, 4, 10)), d2l.ones((3, 4)), np.array([4, 2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d3206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(d2l.ones(3, 4, 10), d2l.ones((3, 4), dtype=torch.long),\n",
    "     torch.tensor([4, 2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31faa4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "loss = MaskedSoftmaxCELoss(tf.constant([4, 2, 0]))\n",
    "loss(tf.ones((3,4), dtype = tf.int32), tf.ones((3, 4, 10))).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1799a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(d2l.ones([3, 4, 10]), d2l.ones((3, 4), dtype=paddle.int64),\n",
    "     paddle.to_tensor([4, 2, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82da55",
   "metadata": {},
   "source": [
    "## [**训练**]\n",
    ":label:`sec_seq2seq_training`\n",
    "\n",
    "在下面的循环训练过程中，如 :numref:`fig_seq2seq`所示，\n",
    "特定的序列开始词元（“&lt;bos&gt;”）和\n",
    "原始的输出序列（不包括序列结束词元“&lt;eos&gt;”）\n",
    "拼接在一起作为解码器的输入。\n",
    "这被称为*强制教学*（teacher forcing），\n",
    "因为原始的输出序列（词元的标签）被送入解码器。\n",
    "或者，将来自上一个时间步的*预测*得到的词元作为解码器的当前输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69993fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    net.initialize(init.Xavier(), force_reinit=True, ctx=device)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': lr})\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[10, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # 训练损失求和，词元数量\n",
    "        for batch in data_iter:\n",
    "            X, X_valid_len, Y, Y_valid_len = [\n",
    "                x.as_in_ctx(device) for x in batch]\n",
    "            bos = np.array([tgt_vocab['<bos>']] * Y.shape[0], \n",
    "                       ctx=device).reshape(-1, 1)\n",
    "            dec_input = np.concatenate([bos, Y[:, :-1]], 1)  # 强制教学\n",
    "            with autograd.record():\n",
    "                Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "                l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.backward()\n",
    "            d2l.grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            trainer.step(num_tokens)\n",
    "            metric.add(l.sum(), num_tokens)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "        f'tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a82b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                     xlim=[10, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                          device=device).reshape(-1, 1)\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            l.sum().backward()\t# 损失函数的标量进行“反向传播”\n",
    "            d2l.grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "        f'tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44934df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "#@save\n",
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    animator = d2l.Animator(xlabel=\"epoch\", ylabel=\"loss\",\n",
    "                            xlim=[10, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量\n",
    "        for batch in data_iter:\n",
    "            X, X_valid_len, Y, Y_valid_len = [x for x in batch]\n",
    "            bos = tf.reshape(tf.constant([tgt_vocab['<bos>']] * Y.shape[0]),\n",
    "                             shape=(-1, 1))\n",
    "            dec_input = tf.concat([bos, Y[:, :-1]], 1)  # 强制教学\n",
    "            with tf.GradientTape() as tape:\n",
    "                Y_hat, _ = net(X, dec_input, X_valid_len, training=True)\n",
    "                l = MaskedSoftmaxCELoss(Y_valid_len)(Y, Y_hat)\n",
    "            gradients = tape.gradient(l, net.trainable_variables)\n",
    "            gradients = d2l.grad_clipping(gradients, 1)\n",
    "            optimizer.apply_gradients(zip(gradients, net.trainable_variables))\n",
    "            num_tokens = tf.reduce_sum(Y_valid_len).numpy()\n",
    "            metric.add(tf.reduce_sum(l), num_tokens)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "          f'tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86efa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=net.parameters())\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    net.train()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                     xlim=[10, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # 训练损失总和，词元数量\n",
    "        for batch in data_iter:\n",
    "            optimizer.clear_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [paddle.to_tensor(x, place=device) for x in batch]\n",
    "            bos = paddle.to_tensor([tgt_vocab['<bos>']] * Y.shape[0]).reshape([-1, 1])\n",
    "            dec_input = paddle.concat([bos, Y[:, :-1]], 1)  # 强制教学\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len.squeeze())\n",
    "            l = loss(Y_hat, Y, Y_valid_len.squeeze())\n",
    "            l.backward()\t# 损失函数的标量进行“反向传播”\n",
    "            d2l.grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            with paddle.no_grad():\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "        f'tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd83a8",
   "metadata": {},
   "source": [
    "现在，在机器翻译数据集上，我们可以\n",
    "[**创建和训练一个循环神经网络“编码器－解码器”模型**]用于序列到序列的学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6e8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()\n",
    "\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d085d50",
   "metadata": {},
   "source": [
    "## [**预测**]\n",
    "\n",
    "为了采用一个接着一个词元的方式预测输出序列，\n",
    "每个解码器当前时间步的输入都将来自于前一时间步的预测词元。\n",
    "与训练类似，序列开始词元（“&lt;bos&gt;”）\n",
    "在初始时间步被输入到解码器中。\n",
    "该预测过程如 :numref:`fig_seq2seq_predict`所示，\n",
    "当输出序列的预测遇到序列结束词元（“&lt;eos&gt;”）时，预测就结束了。\n",
    "\n",
    "![使用循环神经网络编码器-解码器逐词元地预测输出序列。](../img/seq2seq-predict.svg)\n",
    ":label:`fig_seq2seq_predict`\n",
    "\n",
    "我们将在 :numref:`sec_beam-search`中介绍不同的序列生成策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba80338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"序列到序列模型的预测\"\"\"\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n",
    "        src_vocab['<eos>']]\n",
    "    enc_valid_len = np.array([len(src_tokens)], ctx=device)\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # 添加批量轴\n",
    "    enc_X = np.expand_dims(np.array(src_tokens, ctx=device), axis=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # 添加批量轴\n",
    "    dec_X = np.expand_dims(np.array([tgt_vocab['<bos>']], ctx=device), \n",
    "                           axis=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入\n",
    "        dec_X = Y.argmax(axis=2)\n",
    "        pred = dec_X.squeeze(axis=0).astype('int32').item()\n",
    "        # 保存注意力权重（稍后讨论）\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        # 一旦序列结束词元被预测，输出序列的生成就完成了\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcccb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"序列到序列模型的预测\"\"\"\n",
    "    # 在预测时将net设置为评估模式\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n",
    "        src_vocab['<eos>']]\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # 添加批量轴\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # 添加批量轴\n",
    "    dec_X = torch.unsqueeze(torch.tensor(\n",
    "        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # 保存注意力权重（稍后讨论）\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        # 一旦序列结束词元被预测，输出序列的生成就完成了\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219aeed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "#@save\n",
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    save_attention_weights=False):\n",
    "    \"\"\"序列到序列模型的预测\"\"\"\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n",
    "        src_vocab['<eos>']]\n",
    "    enc_valid_len = tf.constant([len(src_tokens)])\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # 添加批量轴\n",
    "    enc_X = tf.expand_dims(src_tokens, axis=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len, training=False)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # 添加批量轴\n",
    "    dec_X = tf.expand_dims(tf.constant([tgt_vocab['<bos>']]), axis=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state, training=False)\n",
    "        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入\n",
    "        dec_X = tf.argmax(Y, axis=2)\n",
    "        pred = tf.squeeze(dec_X, axis=0)\n",
    "        # 保存注意力权重\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        # 一旦序列结束词元被预测，输出序列的生成就完成了\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred.numpy())\n",
    "    return ' '.join(tgt_vocab.to_tokens(tf.reshape(output_seq, \n",
    "        shape = -1).numpy().tolist())), attention_weight_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05191bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"序列到序列模型的预测\"\"\"\n",
    "    # 在预测时将net设置为评估模式\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n",
    "        src_vocab['<eos>']]\n",
    "    enc_valid_len = paddle.to_tensor([len(src_tokens)], place=device)\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    # 添加批量轴\n",
    "    enc_X = paddle.unsqueeze(\n",
    "        paddle.to_tensor(src_tokens, dtype=paddle.int64, place=device), axis=0)\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    # 添加批量轴\n",
    "    dec_X = paddle.unsqueeze(paddle.to_tensor(\n",
    "        [tgt_vocab['<bos>']], dtype=paddle.int64, place=device), axis=0)\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入\n",
    "        dec_X = Y.argmax(axis=2)\n",
    "        pred = dec_X.squeeze(axis=0).astype(paddle.int32).item()\n",
    "        # 保存注意力权重（稍后讨论）\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights)\n",
    "        # 一旦序列结束词元被预测，输出序列的生成就完成了\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5acc4d",
   "metadata": {},
   "source": [
    "## 预测序列的评估\n",
    "\n",
    "我们可以通过与真实的标签序列进行比较来评估预测序列。\n",
    "虽然 :cite:`Papineni.Roukos.Ward.ea.2002`\n",
    "提出的BLEU（bilingual evaluation understudy）\n",
    "最先是用于评估机器翻译的结果，\n",
    "但现在它已经被广泛用于测量许多应用的输出序列的质量。\n",
    "原则上说，对于预测序列中的任意$n$元语法（n-grams），\n",
    "BLEU的评估都是这个$n$元语法是否出现在标签序列中。\n",
    "\n",
    "我们将BLEU定义为：\n",
    "\n",
    "$$ \\exp\\left(\\min\\left(0, 1 - \\frac{\\mathrm{len}_{\\text{label}}}{\\mathrm{len}_{\\text{pred}}}\\right)\\right) \\prod_{n=1}^k p_n^{1/2^n},$$\n",
    ":eqlabel:`eq_bleu`\n",
    "\n",
    "其中$\\mathrm{len}_{\\text{label}}$表示标签序列中的词元数和\n",
    "$\\mathrm{len}_{\\text{pred}}$表示预测序列中的词元数，\n",
    "$k$是用于匹配的最长的$n$元语法。\n",
    "另外，用$p_n$表示$n$元语法的精确度，它是两个数量的比值：\n",
    "第一个是预测序列与标签序列中匹配的$n$元语法的数量，\n",
    "第二个是预测序列中$n$元语法的数量的比率。\n",
    "具体地说，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$\n",
    "和预测序列$A$、$B$、$B$、$C$、$D$，\n",
    "我们有$p_1 = 4/5$、$p_2 = 3/4$、$p_3 = 1/3$和$p_4 = 0$。\n",
    "\n",
    "根据 :eqref:`eq_bleu`中BLEU的定义，\n",
    "当预测序列与标签序列完全相同时，BLEU为$1$。\n",
    "此外，由于$n$元语法越长则匹配难度越大，\n",
    "所以BLEU为更长的$n$元语法的精确度分配更大的权重。\n",
    "具体来说，当$p_n$固定时，$p_n^{1/2^n}$\n",
    "会随着$n$的增长而增加（原始论文使用$p_n^{1/n}$）。\n",
    "而且，由于预测的序列越短获得的$p_n$值越高，\n",
    "所以 :eqref:`eq_bleu`中乘法项之前的系数用于惩罚较短的预测序列。\n",
    "例如，当$k=2$时，给定标签序列$A$、$B$、$C$、$D$、$E$、$F$\n",
    "和预测序列$A$、$B$，尽管$p_1 = p_2 = 1$，\n",
    "惩罚因子$\\exp(1-6/2) \\approx 0.14$会降低BLEU。\n",
    "\n",
    "[**BLEU的代码实现**]如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def bleu(pred_seq, label_seq, k):  #@save\n",
    "    \"\"\"计算BLEU\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, k + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af1521",
   "metadata": {},
   "source": [
    "最后，利用训练好的循环神经网络“编码器－解码器”模型，\n",
    "[**将几个英语句子翻译成法语**]，并计算BLEU的最终结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e49a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch, paddle\n",
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, attention_weight_seq = predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd80a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, attention_weight_seq = predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps)\n",
    "    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d248ce",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 根据“编码器-解码器”架构的设计，\n",
    "  我们可以使用两个循环神经网络来设计一个序列到序列学习的模型。\n",
    "* 在实现编码器和解码器时，我们可以使用多层循环神经网络。\n",
    "* 我们可以使用遮蔽来过滤不相关的计算，例如在计算损失时。\n",
    "* 在“编码器－解码器”训练中，强制教学方法将原始输出序列（而非预测结果）输入解码器。\n",
    "* BLEU是一种常用的评估方法，它通过测量预测序列和标签序列之间的$n$元语法的匹配度来评估预测。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 试着通过调整超参数来改善翻译效果。\n",
    "1. 重新运行实验并在计算损失时不使用遮蔽，可以观察到什么结果？为什么会有这个结果？\n",
    "1. 如果编码器和解码器的层数或者隐藏单元数不同，那么如何初始化解码器的隐状态？\n",
    "1. 在训练中，如果用前一时间步的预测输入到解码器来代替强制教学，对性能有何影响？\n",
    "1. 用长短期记忆网络替换门控循环单元重新运行实验。\n",
    "1. 有没有其他方法来设计解码器的输出层？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2783)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2782)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11838)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f66754",
   "metadata": {},
   "source": [
    "# 束搜索\n",
    ":label:`sec_beam-search`\n",
    "\n",
    "在 :numref:`sec_seq2seq`中，我们逐个预测输出序列，\n",
    "直到预测序列中出现特定的序列结束词元“&lt;eos&gt;”。\n",
    "本节将首先介绍*贪心搜索*（greedy search）策略，\n",
    "并探讨其存在的问题，然后对比其他替代策略：\n",
    "*穷举搜索*（exhaustive search）和*束搜索*（beam search）。\n",
    "\n",
    "在正式介绍贪心搜索之前，我们使用与 :numref:`sec_seq2seq`中\n",
    "相同的数学符号定义搜索问题。\n",
    "在任意时间步$t'$，解码器输出$y_{t'}$的概率取决于\n",
    "时间步$t'$之前的输出子序列$y_1, \\ldots, y_{t'-1}$\n",
    "和对输入序列的信息进行编码得到的上下文变量$\\mathbf{c}$。\n",
    "为了量化计算代价，用$\\mathcal{Y}$表示输出词表，\n",
    "其中包含“&lt;eos&gt;”，\n",
    "所以这个词汇集合的基数$\\left|\\mathcal{Y}\\right|$就是词表的大小。\n",
    "我们还将输出序列的最大词元数指定为$T'$。\n",
    "因此，我们的目标是从所有$\\mathcal{O}(\\left|\\mathcal{Y}\\right|^{T'})$个\n",
    "可能的输出序列中寻找理想的输出。\n",
    "当然，对于所有输出序列，在“&lt;eos&gt;”之后的部分（非本句）\n",
    "将在实际输出中丢弃。\n",
    "\n",
    "## 贪心搜索\n",
    "\n",
    "首先，让我们看看一个简单的策略：*贪心搜索*，\n",
    "该策略已用于 :numref:`sec_seq2seq`的序列预测。\n",
    "对于输出序列的每一时间步$t'$，\n",
    "我们都将基于贪心搜索从$\\mathcal{Y}$中找到具有最高条件概率的词元，即：\n",
    "\n",
    "$$y_{t'} = \\operatorname*{argmax}_{y \\in \\mathcal{Y}} P(y \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c})$$\n",
    "\n",
    "一旦输出序列包含了“&lt;eos&gt;”或者达到其最大长度$T'$，则输出完成。\n",
    "\n",
    "![在每个时间步，贪心搜索选择具有最高条件概率的词元](../img/s2s-prob1.svg)\n",
    ":label:`fig_s2s-prob1`\n",
    "\n",
    "如 :numref:`fig_s2s-prob1`中，\n",
    "假设输出中有四个词元“A”“B”“C”和“&lt;eos&gt;”。\n",
    "每个时间步下的四个数字分别表示在该时间步\n",
    "生成“A”“B”“C”和“&lt;eos&gt;”的条件概率。\n",
    "在每个时间步，贪心搜索选择具有最高条件概率的词元。\n",
    "因此，将在 :numref:`fig_s2s-prob1`中\n",
    "预测输出序列“A”“B”“C”和“&lt;eos&gt;”。\n",
    "这个输出序列的条件概率是\n",
    "$0.5\\times0.4\\times0.4\\times0.6 = 0.048$。\n",
    "\n",
    "那么贪心搜索存在的问题是什么呢？\n",
    "现实中，*最优序列*（optimal sequence）应该是最大化\n",
    "$\\prod_{t'=1}^{T'} P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c})$\n",
    "值的输出序列，这是基于输入序列生成输出序列的条件概率。\n",
    "然而，贪心搜索无法保证得到最优序列。\n",
    "\n",
    "![在时间步2，选择具有第二高条件概率的词元“C”（而非最高条件概率的词元）](../img/s2s-prob2.svg)\n",
    ":label:`fig_s2s-prob2`\n",
    "\n",
    " :numref:`fig_s2s-prob2`中的另一个例子阐述了这个问题。\n",
    "与 :numref:`fig_s2s-prob1`不同，在时间步$2$中，\n",
    "我们选择 :numref:`fig_s2s-prob2`中的词元“C”，\n",
    "它具有*第二*高的条件概率。\n",
    "由于时间步$3$所基于的时间步$1$和$2$处的输出子序列已从\n",
    " :numref:`fig_s2s-prob1`中的“A”和“B”改变为\n",
    " :numref:`fig_s2s-prob2`中的“A”和“C”，\n",
    "因此时间步$3$处的每个词元的条件概率也在 :numref:`fig_s2s-prob2`中改变。\n",
    "假设我们在时间步$3$选择词元“B”，\n",
    "于是当前的时间步$4$基于前三个时间步的输出子序列“A”“C”和“B”为条件，\n",
    "这与 :numref:`fig_s2s-prob1`中的“A”“B”和“C”不同。\n",
    "因此，在 :numref:`fig_s2s-prob2`中的时间步$4$生成\n",
    "每个词元的条件概率也不同于 :numref:`fig_s2s-prob1`中的条件概率。\n",
    "结果， :numref:`fig_s2s-prob2`中的输出序列\n",
    "“A”“C”“B”和“&lt;eos&gt;”的条件概率为\n",
    "$0.5\\times0.3 \\times0.6\\times0.6=0.054$，\n",
    "这大于 :numref:`fig_s2s-prob1`中的贪心搜索的条件概率。\n",
    "这个例子说明：贪心搜索获得的输出序列\n",
    "“A”“B”“C”和“&lt;eos&gt;”\n",
    "不一定是最佳序列。\n",
    "\n",
    "## 穷举搜索\n",
    "\n",
    "如果目标是获得最优序列，\n",
    "我们可以考虑使用*穷举搜索*（exhaustive search）：\n",
    "穷举地列举所有可能的输出序列及其条件概率，\n",
    "然后计算输出条件概率最高的一个。\n",
    "\n",
    "虽然我们可以使用穷举搜索来获得最优序列，\n",
    "但其计算量$\\mathcal{O}(\\left|\\mathcal{Y}\\right|^{T'})$可能高的惊人。\n",
    "例如，当$|\\mathcal{Y}|=10000$和$T'=10$时，\n",
    "我们需要评估$10000^{10} = 10^{40}$序列，\n",
    "这是一个极大的数，现有的计算机几乎不可能计算它。\n",
    "然而，贪心搜索的计算量\n",
    "$\\mathcal{O}(\\left|\\mathcal{Y}\\right|T')$\n",
    "通它要显著地小于穷举搜索。\n",
    "例如，当$|\\mathcal{Y}|=10000$和$T'=10$时，\n",
    "我们只需要评估$10000\\times10=10^5$个序列。\n",
    "\n",
    "## 束搜索\n",
    "\n",
    "那么该选取哪种序列搜索策略呢？\n",
    "如果精度最重要，则显然是穷举搜索。\n",
    "如果计算成本最重要，则显然是贪心搜索。\n",
    "而束搜索的实际应用则介于这两个极端之间。\n",
    "\n",
    "*束搜索*（beam search）是贪心搜索的一个改进版本。\n",
    "它有一个超参数，名为*束宽*（beam size）$k$。\n",
    "在时间步$1$，我们选择具有最高条件概率的$k$个词元。\n",
    "这$k$个词元将分别是$k$个候选输出序列的第一个词元。\n",
    "在随后的每个时间步，基于上一时间步的$k$个候选输出序列，\n",
    "我们将继续从$k\\left|\\mathcal{Y}\\right|$个可能的选择中\n",
    "挑出具有最高条件概率的$k$个候选输出序列。\n",
    "\n",
    "![束搜索过程（束宽：2，输出序列的最大长度：3）。候选输出序列是$A$、$C$、$AB$、$CE$、$ABD$和$CED$](../img/beam-search.svg)\n",
    ":label:`fig_beam-search`\n",
    "\n",
    " :numref:`fig_beam-search`演示了束搜索的过程。\n",
    "假设输出的词表只包含五个元素：\n",
    "$\\mathcal{Y} = \\{A, B, C, D, E\\}$，\n",
    "其中有一个是“&lt;eos&gt;”。\n",
    "设置束宽为$2$，输出序列的最大长度为$3$。\n",
    "在时间步$1$，假设具有最高条件概率\n",
    "$P(y_1 \\mid \\mathbf{c})$的词元是$A$和$C$。\n",
    "在时间步$2$，我们计算所有$y_2 \\in \\mathcal{Y}$为：\n",
    "\n",
    "$$\\begin{aligned}P(A, y_2 \\mid \\mathbf{c}) = P(A \\mid \\mathbf{c})P(y_2 \\mid A, \\mathbf{c}),\\\\ P(C, y_2 \\mid \\mathbf{c}) = P(C \\mid \\mathbf{c})P(y_2 \\mid C, \\mathbf{c}),\\end{aligned}$$  \n",
    "\n",
    "从这十个值中选择最大的两个，\n",
    "比如$P(A, B \\mid \\mathbf{c})$和$P(C, E \\mid \\mathbf{c})$。\n",
    "然后在时间步$3$，我们计算所有$y_3 \\in \\mathcal{Y}$为：\n",
    "\n",
    "$$\\begin{aligned}P(A, B, y_3 \\mid \\mathbf{c}) = P(A, B \\mid \\mathbf{c})P(y_3 \\mid A, B, \\mathbf{c}),\\\\P(C, E, y_3 \\mid \\mathbf{c}) = P(C, E \\mid \\mathbf{c})P(y_3 \\mid C, E, \\mathbf{c}),\\end{aligned}$$ \n",
    "\n",
    "从这十个值中选择最大的两个，\n",
    "即$P(A, B, D \\mid \\mathbf{c})$和$P(C, E, D \\mid  \\mathbf{c})$，\n",
    "我们会得到六个候选输出序列：\n",
    "（1）$A$；（2）$C$；（3）$A,B$；（4）$C,E$；（5）$A,B,D$；（6）$C,E,D$。\n",
    "\n",
    "最后，基于这六个序列（例如，丢弃包括“&lt;eos&gt;”和之后的部分），\n",
    "我们获得最终候选输出序列集合。\n",
    "然后我们选择其中条件概率乘积最高的序列作为输出序列：\n",
    "\n",
    "$$ \\frac{1}{L^\\alpha} \\log P(y_1, \\ldots, y_{L}\\mid \\mathbf{c}) = \\frac{1}{L^\\alpha} \\sum_{t'=1}^L \\log P(y_{t'} \\mid y_1, \\ldots, y_{t'-1}, \\mathbf{c}),$$\n",
    ":eqlabel:`eq_beam-search-score`\n",
    "\n",
    "其中$L$是最终候选序列的长度，\n",
    "$\\alpha$通常设置为$0.75$。\n",
    "因为一个较长的序列在 :eqref:`eq_beam-search-score`\n",
    "的求和中会有更多的对数项，\n",
    "因此分母中的$L^\\alpha$用于惩罚长序列。\n",
    "\n",
    "束搜索的计算量为$\\mathcal{O}(k\\left|\\mathcal{Y}\\right|T')$，\n",
    "这个结果介于贪心搜索和穷举搜索之间。\n",
    "实际上，贪心搜索可以看作一种束宽为$1$的特殊类型的束搜索。\n",
    "通过灵活地选择束宽，束搜索可以在正确率和计算代价之间进行权衡。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 序列搜索策略包括贪心搜索、穷举搜索和束搜索。\n",
    "* 贪心搜索所选取序列的计算量最小，但精度相对较低。\n",
    "* 穷举搜索所选取序列的精度最高，但计算量最大。\n",
    "* 束搜索通过灵活选择束宽，在正确率和计算代价之间进行权衡。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 我们可以把穷举搜索看作一种特殊的束搜索吗？为什么？\n",
    "1. 在 :numref:`sec_seq2seq`的机器翻译问题中应用束搜索。\n",
    "   束宽是如何影响预测的速度和结果的？\n",
    "1. 在 :numref:`sec_rnn_scratch`中，我们基于用户提供的前缀，\n",
    "   通过使用语言模型来生成文本。这个例子中使用了哪种搜索策略？可以改进吗？\n",
    "\n",
    "[Discussions](https://discuss.d2l.ai/t/5768)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fc2f10",
   "metadata": {},
   "source": [
    "# 循环神经网络\n",
    ":label:`chap_rnn`\n",
    "\n",
    "到目前为止，我们遇到过两种类型的数据：表格数据和图像数据。\n",
    "对于图像数据，我们设计了专门的卷积神经网络架构来为这类特殊的数据结构建模。\n",
    "换句话说，如果我们拥有一张图像，我们需要有效地利用其像素位置，\n",
    "假若我们对图像中的像素位置进行重排，就会对图像中内容的推断造成极大的困难。\n",
    "\n",
    "最重要的是，到目前为止我们默认数据都来自于某种分布，\n",
    "并且所有样本都是独立同分布的\n",
    "（independently and identically distributed，i.i.d.）。\n",
    "然而，大多数的数据并非如此。\n",
    "例如，文章中的单词是按顺序写的，如果顺序被随机地重排，就很难理解文章原始的意思。\n",
    "同样，视频中的图像帧、对话中的音频信号以及网站上的浏览行为都是有顺序的。\n",
    "因此，针对此类数据而设计特定模型，可能效果会更好。\n",
    "\n",
    "另一个问题来自这样一个事实：\n",
    "我们不仅仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续。\n",
    "例如，一个任务可以是继续预测$2, 4, 6, 8, 10, \\ldots$。\n",
    "这在时间序列分析中是相当常见的，可以用来预测股市的波动、\n",
    "患者的体温曲线或者赛车所需的加速度。\n",
    "同理，我们需要能够处理这些数据的特定模型。\n",
    "\n",
    "简言之，如果说卷积神经网络可以有效地处理空间信息，\n",
    "那么本章的*循环神经网络*（recurrent neural network，RNN）则可以更好地处理序列信息。\n",
    "循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。\n",
    "\n",
    "许多使用循环网络的例子都是基于文本数据的，因此我们将在本章中重点介绍语言模型。\n",
    "在对序列数据进行更详细的回顾之后，我们将介绍文本预处理的实用技术。\n",
    "然后，我们将讨论语言模型的基本概念，并将此讨论作为循环神经网络设计的灵感。\n",
    "最后，我们描述了循环神经网络的梯度计算方法，以探讨训练此类网络时可能遇到的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b5326",
   "metadata": {
    "attributes": {
     "classes": [
      "toc"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    ":maxdepth: 2\n",
    "\n",
    "sequence\n",
    "text-preprocessing\n",
    "language-models-and-dataset\n",
    "rnn\n",
    "rnn-scratch\n",
    "rnn-concise\n",
    "bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87738ad",
   "metadata": {},
   "source": [
    "# 序列模型\n",
    ":label:`sec_sequence`\n",
    "\n",
    "想象一下有人正在看网飞（Netflix，一个国外的视频网站）上的电影。\n",
    "一名忠实的用户会对每一部电影都给出评价，\n",
    "毕竟一部好电影需要更多的支持和认可。\n",
    "然而事实证明，事情并不那么简单。\n",
    "随着时间的推移，人们对电影的看法会发生很大的变化。\n",
    "事实上，心理学家甚至对这些现象起了名字：\n",
    "\n",
    "* *锚定*（anchoring）效应：基于其他人的意见做出评价。\n",
    "  例如，奥斯卡颁奖后，受到关注的电影的评分会上升，尽管它还是原来那部电影。\n",
    "  这种影响将持续几个月，直到人们忘记了这部电影曾经获得的奖项。\n",
    "  结果表明（ :cite:`Wu.Ahmed.Beutel.ea.2017`），这种效应会使评分提高半个百分点以上。\n",
    "* *享乐适应*（hedonic adaption）：人们迅速接受并且适应一种更好或者更坏的情况\n",
    "  作为新的常态。\n",
    "  例如，在看了很多好电影之后，人们会强烈期望下部电影会更好。\n",
    "  因此，在许多精彩的电影被看过之后，即使是一部普通的也可能被认为是糟糕的。\n",
    "* *季节性*（seasonality）：少有观众喜欢在八月看圣诞老人的电影。\n",
    "* 有时，电影会由于导演或演员在制作中的不当行为变得不受欢迎。\n",
    "* 有些电影因为其极度糟糕只能成为小众电影。*Plan9from Outer Space*和*Troll2*就因为这个原因而臭名昭著的。\n",
    "\n",
    "简而言之，电影评分决不是固定不变的。\n",
    "因此，使用时间动力学可以得到更准确的电影推荐 :cite:`Koren.2009`。\n",
    "当然，序列数据不仅仅是关于电影评分的。\n",
    "下面给出了更多的场景。\n",
    "\n",
    "* 在使用程序时，许多用户都有很强的特定习惯。\n",
    "  例如，在学生放学后社交媒体应用更受欢迎。在市场开放时股市交易软件更常用。\n",
    "* 预测明天的股价要比过去的股价更困难，尽管两者都只是估计一个数字。\n",
    "  毕竟，先见之明比事后诸葛亮难得多。\n",
    "  在统计学中，前者（对超出已知观测范围进行预测）称为*外推法*（extrapolation），\n",
    "  而后者（在现有观测值之间进行估计）称为*内插法*（interpolation）。\n",
    "* 在本质上，音乐、语音、文本和视频都是连续的。\n",
    "  如果它们的序列被我们重排，那么就会失去原有的意义。\n",
    "  比如，一个文本标题“狗咬人”远没有“人咬狗”那么令人惊讶，尽管组成两句话的字完全相同。\n",
    "* 地震具有很强的相关性，即大地震发生后，很可能会有几次小余震，\n",
    "  这些余震的强度比非大地震后的余震要大得多。\n",
    "  事实上，地震是时空相关的，即余震通常发生在很短的时间跨度和很近的距离内。\n",
    "* 人类之间的互动也是连续的，这可以从微博上的争吵和辩论中看出。\n",
    "\n",
    "## 统计工具\n",
    "\n",
    "处理序列数据需要统计工具和新的深度神经网络架构。\n",
    "为了简单起见，我们以 :numref:`fig_ftse100`所示的股票价格（富时100指数）为例。\n",
    "\n",
    "![近30年的富时100指数](../img/ftse100.png)\n",
    ":width:`400px`\n",
    ":label:`fig_ftse100`\n",
    "\n",
    "其中，用$x_t$表示价格，即在*时间步*（time step）\n",
    "$t \\in \\mathbb{Z}^+$时，观察到的价格$x_t$。\n",
    "请注意，$t$对于本文中的序列通常是离散的，并在整数或其子集上变化。\n",
    "假设一个交易员想在$t$日的股市中表现良好，于是通过以下途径预测$x_t$：\n",
    "\n",
    "$$x_t \\sim P(x_t \\mid x_{t-1}, \\ldots, x_1).$$\n",
    "\n",
    "### 自回归模型\n",
    "\n",
    "为了实现这个预测，交易员可以使用回归模型，\n",
    "例如在 :numref:`sec_linear_concise`中训练的模型。\n",
    "仅有一个主要问题：输入数据的数量，\n",
    "输入$x_{t-1}, \\ldots, x_1$本身因$t$而异。\n",
    "也就是说，输入数据的数量这个数字将会随着我们遇到的数据量的增加而增加，\n",
    "因此需要一个近似方法来使这个计算变得容易处理。\n",
    "本章后面的大部分内容将围绕着如何有效估计\n",
    "$P(x_t \\mid x_{t-1}, \\ldots, x_1)$展开。\n",
    "简单地说，它归结为以下两种策略。\n",
    "\n",
    "第一种策略，假设在现实情况下相当长的序列\n",
    "$x_{t-1}, \\ldots, x_1$可能是不必要的，\n",
    "因此我们只需要满足某个长度为$\\tau$的时间跨度，\n",
    "即使用观测序列$x_{t-1}, \\ldots, x_{t-\\tau}$。\n",
    "当下获得的最直接的好处就是参数的数量总是不变的，\n",
    "至少在$t > \\tau$时如此，这就使我们能够训练一个上面提及的深度网络。\n",
    "这种模型被称为*自回归模型*（autoregressive models），\n",
    "因为它们是对自己执行回归。\n",
    "\n",
    "第二种策略，如 :numref:`fig_sequence-model`所示，\n",
    "是保留一些对过去观测的总结$h_t$，\n",
    "并且同时更新预测$\\hat{x}_t$和总结$h_t$。\n",
    "这就产生了基于$\\hat{x}_t = P(x_t \\mid h_{t})$估计$x_t$，\n",
    "以及公式$h_t = g(h_{t-1}, x_{t-1})$更新的模型。\n",
    "由于$h_t$从未被观测到，这类模型也被称为\n",
    "*隐变量自回归模型*（latent autoregressive models）。\n",
    "\n",
    "![隐变量自回归模型](../img/sequence-model.svg)\n",
    ":label:`fig_sequence-model`\n",
    "\n",
    "这两种情况都有一个显而易见的问题：如何生成训练数据？\n",
    "一个经典方法是使用历史观测来预测下一个未来观测。\n",
    "显然，我们并不指望时间会停滞不前。\n",
    "然而，一个常见的假设是虽然特定值$x_t$可能会改变，\n",
    "但是序列本身的动力学不会改变。\n",
    "这样的假设是合理的，因为新的动力学一定受新的数据影响，\n",
    "而我们不可能用目前所掌握的数据来预测新的动力学。\n",
    "统计学家称不变的动力学为*静止的*（stationary）。\n",
    "因此，整个序列的估计值都将通过以下的方式获得：\n",
    "\n",
    "$$P(x_1, \\ldots, x_T) = \\prod_{t=1}^T P(x_t \\mid x_{t-1}, \\ldots, x_1).$$\n",
    "\n",
    "注意，如果我们处理的是离散的对象（如单词），\n",
    "而不是连续的数字，则上述的考虑仍然有效。\n",
    "唯一的差别是，对于离散的对象，\n",
    "我们需要使用分类器而不是回归模型来估计$P(x_t \\mid  x_{t-1}, \\ldots, x_1)$。\n",
    "\n",
    "### 马尔可夫模型\n",
    "\n",
    "回想一下，在自回归模型的近似法中，\n",
    "我们使用$x_{t-1}, \\ldots, x_{t-\\tau}$\n",
    "而不是$x_{t-1}, \\ldots, x_1$来估计$x_t$。\n",
    "只要这种是近似精确的，我们就说序列满足*马尔可夫条件*（Markov condition）。\n",
    "特别是，如果$\\tau = 1$，得到一个\n",
    "*一阶马尔可夫模型*（first-order Markov model），\n",
    "$P(x)$由下式给出：\n",
    "\n",
    "$$P(x_1, \\ldots, x_T) = \\prod_{t=1}^T P(x_t \\mid x_{t-1}) \\text{ 当 } P(x_1 \\mid x_0) = P(x_1).$$\n",
    "\n",
    "当假设$x_t$仅是离散值时，这样的模型特别棒，\n",
    "因为在这种情况下，使用动态规划可以沿着马尔可夫链精确地计算结果。\n",
    "例如，我们可以高效地计算$P(x_{t+1} \\mid x_{t-1})$：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(x_{t+1} \\mid x_{t-1})\n",
    "&= \\frac{\\sum_{x_t} P(x_{t+1}, x_t, x_{t-1})}{P(x_{t-1})}\\\\\n",
    "&= \\frac{\\sum_{x_t} P(x_{t+1} \\mid x_t, x_{t-1}) P(x_t, x_{t-1})}{P(x_{t-1})}\\\\\n",
    "&= \\sum_{x_t} P(x_{t+1} \\mid x_t) P(x_t \\mid x_{t-1})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "利用这一事实，我们只需要考虑过去观察中的一个非常短的历史：\n",
    "$P(x_{t+1} \\mid x_t, x_{t-1}) = P(x_{t+1} \\mid x_t)$。\n",
    "隐马尔可夫模型中的动态规划超出了本节的范围\n",
    "（我们将在 :numref:`sec_bi_rnn`再次遇到），\n",
    "而动态规划这些计算工具已经在控制算法和强化学习算法广泛使用。\n",
    "\n",
    "### 因果关系\n",
    "\n",
    "原则上，将$P(x_1, \\ldots, x_T)$倒序展开也没什么问题。\n",
    "毕竟，基于条件概率公式，我们总是可以写出：\n",
    "\n",
    "$$P(x_1, \\ldots, x_T) = \\prod_{t=T}^1 P(x_t \\mid x_{t+1}, \\ldots, x_T).$$\n",
    "\n",
    "事实上，如果基于一个马尔可夫模型，\n",
    "我们还可以得到一个反向的条件概率分布。\n",
    "然而，在许多情况下，数据存在一个自然的方向，即在时间上是前进的。\n",
    "很明显，未来的事件不能影响过去。\n",
    "因此，如果我们改变$x_t$，可能会影响未来发生的事情$x_{t+1}$，但不能反过来。\n",
    "也就是说，如果我们改变$x_t$，基于过去事件得到的分布不会改变。\n",
    "因此，解释$P(x_{t+1} \\mid x_t)$应该比解释$P(x_t \\mid x_{t+1})$更容易。\n",
    "例如，在某些情况下，对于某些可加性噪声$\\epsilon$，\n",
    "显然我们可以找到$x_{t+1} = f(x_t) + \\epsilon$，\n",
    "而反之则不行 :cite:`Hoyer.Janzing.Mooij.ea.2009`。\n",
    "而这个向前推进的方向恰好也是我们通常感兴趣的方向。\n",
    "彼得斯等人 :cite:`Peters.Janzing.Scholkopf.2017`\n",
    "对该主题的更多内容做了详尽的解释，而我们的上述讨论只是其中的冰山一角。\n",
    "\n",
    "## 训练\n",
    "\n",
    "在了解了上述统计工具后，让我们在实践中尝试一下！\n",
    "首先，我们生成一些数据：(**使用正弦函数和一些可加性噪声来生成序列数据，\n",
    "时间步为$1, 2, \\ldots, 1000$。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, np, npx, gluon, init\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaab9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "%matplotlib inline\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a2fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "from paddle import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch, paddle\n",
    "T = 1000  # 总共产生1000个点\n",
    "time = d2l.arange(1, T + 1, dtype=d2l.float32)\n",
    "x = d2l.sin(0.01 * time) + d2l.normal(0, 0.2, (T,))\n",
    "d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6caba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "T = 1000  # 总共产生1000个点\n",
    "time = d2l.arange(1, T + 1, dtype=d2l.float32)\n",
    "x = d2l.sin(0.01 * time) + d2l.normal([T], 0, 0.2)\n",
    "d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28033e",
   "metadata": {},
   "source": [
    "接下来，我们将这个序列转换为模型的*特征－标签*（feature-label）对。\n",
    "基于嵌入维度$\\tau$，我们[**将数据映射为数据对$y_t = x_t$\n",
    "和$\\mathbf{x}_t = [x_{t-\\tau}, \\ldots, x_{t-1}]$。**]\n",
    "这比我们提供的数据样本少了$\\tau$个，\n",
    "因为我们没有足够的历史记录来描述前$\\tau$个数据样本。\n",
    "一个简单的解决办法是：如果拥有足够长的序列就丢弃这几项；\n",
    "另一个方法是用零填充序列。\n",
    "在这里，我们仅使用前600个“特征－标签”对进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ef65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch, paddle\n",
    "tau = 4\n",
    "features = d2l.zeros((T - tau, tau))\n",
    "for i in range(tau):\n",
    "    features[:, i] = x[i: T - tau + i]\n",
    "labels = d2l.reshape(x[tau:], (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9827bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tau = 4\n",
    "features = tf.Variable(d2l.zeros((T - tau, tau)))\n",
    "for i in range(tau):\n",
    "    features[:, i].assign(x[i: T - tau + i])\n",
    "labels = d2l.reshape(x[tau:], (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a204171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "batch_size, n_train = 16, 600\n",
    "# 只有前n_train个样本用于训练\n",
    "train_iter = d2l.load_array((features[:n_train], labels[:n_train]),\n",
    "                            batch_size, is_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f20ed0",
   "metadata": {},
   "source": [
    "在这里，我们[**使用一个相当简单的架构训练模型：\n",
    "一个拥有两个全连接层的多层感知机**]，ReLU激活函数和平方损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个简单的多层感知机\n",
    "def get_net():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(10, activation='relu'),\n",
    "            nn.Dense(1))\n",
    "    net.initialize(init.Xavier())\n",
    "    return net\n",
    "\n",
    "# 平方损失\n",
    "loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "# 初始化网络权重的函数\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "# 一个简单的多层感知机\n",
    "def get_net():\n",
    "    net = nn.Sequential(nn.Linear(4, 10),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(10, 1))\n",
    "    net.apply(init_weights)\n",
    "    return net\n",
    "\n",
    "# 平方损失。注意：MSELoss计算平方误差时不带系数1/2\n",
    "loss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06a618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "# 一个简单的多层感知机\n",
    "def get_net():\n",
    "    net = tf.keras.Sequential([tf.keras.layers.Dense(10, activation='relu'),\n",
    "                              tf.keras.layers.Dense(1)])\n",
    "    return net\n",
    "\n",
    "# 最小均方损失。注意：MeanSquaredError计算平方误差时不带系数1/2\n",
    "loss = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6646e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.initializer.XavierUniform(m.weight)\n",
    "\n",
    "#一个简单的多层感知机\n",
    "def get_net():\n",
    "    net = nn.Sequential(nn.Linear(4, 10),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(10, 1))\n",
    "    net.apply(init_weights)\n",
    "    return net\n",
    "\n",
    "#平方损失。注意:MSELoss计算平方误差时不带系数1/2\n",
    "loss = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6734df",
   "metadata": {},
   "source": [
    "现在，准备[**训练模型**]了。实现下面的训练代码的方式与前面几节（如 :numref:`sec_linear_concise`）中的循环训练基本相同。因此，我们不会深入探讨太多细节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927408cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, loss, epochs, lr):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': lr})\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in train_iter:\n",
    "            with autograd.record():\n",
    "                l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "        print(f'epoch {epoch + 1}, '\n",
    "              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')\n",
    "\n",
    "net = get_net()\n",
    "train(net, train_iter, loss, 5, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38de8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def train(net, train_iter, loss, epochs, lr):\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            l = loss(net(X), y)\n",
    "            l.sum().backward()\n",
    "            trainer.step()\n",
    "        print(f'epoch {epoch + 1}, '\n",
    "              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')\n",
    "\n",
    "net = get_net()\n",
    "train(net, train_iter, loss, 5, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344a309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def train(net, train_iter, loss, epochs, lr):\n",
    "    trainer = tf.keras.optimizers.Adam()\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in train_iter:\n",
    "            with tf.GradientTape() as g:\n",
    "                out = net(X)\n",
    "                l = loss(y, out)\n",
    "                params = net.trainable_variables\n",
    "                grads = g.gradient(l, params)\n",
    "            trainer.apply_gradients(zip(grads, params))\n",
    "        print(f'epoch {epoch + 1}, '\n",
    "              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')\n",
    "\n",
    "net = get_net()\n",
    "train(net, train_iter, loss, 5, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ddc506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def train(net, train_iter, loss, epochs, lr):\n",
    "    trainer = paddle.optimizer.Adam(learning_rate=lr, parameters=net.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        for i,(X, y) in enumerate (train_iter()):\n",
    "            trainer.clear_grad()\n",
    "            l = loss(net(X), y)\n",
    "            l.sum().backward()\n",
    "            trainer.step()\n",
    "        print(f'epoch {epoch + 1}, '\n",
    "              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')\n",
    "\n",
    "net = get_net()\n",
    "train(net, train_iter, loss, 5, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62527b64",
   "metadata": {},
   "source": [
    "## 预测\n",
    "\n",
    "由于训练损失很小，因此我们期望模型能有很好的工作效果。\n",
    "让我们看看这在实践中意味着什么。\n",
    "首先是检查[**模型预测下一个时间步**]的能力，\n",
    "也就是*单步预测*（one-step-ahead prediction）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ea6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "onestep_preds = net(features)\n",
    "d2l.plot([time, time[tau:]], \n",
    "         [d2l.numpy(x), d2l.numpy(onestep_preds)], 'time',\n",
    "         'x', legend=['data', '1-step preds'], xlim=[1, 1000], \n",
    "         figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88552965",
   "metadata": {},
   "source": [
    "正如我们所料，单步预测效果不错。\n",
    "即使这些预测的时间步超过了$600+4$（`n_train + tau`），\n",
    "其结果看起来仍然是可信的。\n",
    "然而有一个小问题：如果数据观察序列的时间步只到$604$，\n",
    "我们需要一步一步地向前迈进：\n",
    "$$\n",
    "\\hat{x}_{605} = f(x_{601}, x_{602}, x_{603}, x_{604}), \\\\\n",
    "\\hat{x}_{606} = f(x_{602}, x_{603}, x_{604}, \\hat{x}_{605}), \\\\\n",
    "\\hat{x}_{607} = f(x_{603}, x_{604}, \\hat{x}_{605}, \\hat{x}_{606}),\\\\\n",
    "\\hat{x}_{608} = f(x_{604}, \\hat{x}_{605}, \\hat{x}_{606}, \\hat{x}_{607}),\\\\\n",
    "\\hat{x}_{609} = f(\\hat{x}_{605}, \\hat{x}_{606}, \\hat{x}_{607}, \\hat{x}_{608}),\\\\\n",
    "\\ldots\n",
    "$$\n",
    "\n",
    "通常，对于直到$x_t$的观测序列，其在时间步$t+k$处的预测输出$\\hat{x}_{t+k}$\n",
    "称为$k$*步预测*（$k$-step-ahead-prediction）。\n",
    "由于我们的观察已经到了$x_{604}$，它的$k$步预测是$\\hat{x}_{604+k}$。\n",
    "换句话说，我们必须使用我们自己的预测（而不是原始数据）来[**进行多步预测**]。\n",
    "让我们看看效果如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "multistep_preds = d2l.zeros(T)\n",
    "multistep_preds[: n_train + tau] = x[: n_train + tau]\n",
    "for i in range(n_train + tau, T):\n",
    "    multistep_preds[i] = net(\n",
    "        d2l.reshape(multistep_preds[i - tau: i], (1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc077e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "multistep_preds = tf.Variable(d2l.zeros(T))\n",
    "multistep_preds[:n_train + tau].assign(x[:n_train + tau])\n",
    "for i in range(n_train + tau, T):\n",
    "    multistep_preds[i].assign(d2l.reshape(net(\n",
    "        d2l.reshape(multistep_preds[i - tau: i], (1, -1))), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fbd8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "multistep_preds = d2l.zeros([T])\n",
    "multistep_preds[: n_train + tau] = x[: n_train + tau]\n",
    "for i in range(n_train + tau, T):\n",
    "    multistep_preds[i] = net(\n",
    "        d2l.reshape(multistep_preds[i - tau: i], (1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d475aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "d2l.plot([time, time[tau:], time[n_train + tau:]],\n",
    "         [d2l.numpy(x), d2l.numpy(onestep_preds),\n",
    "          d2l.numpy(multistep_preds[n_train + tau:])], 'time',\n",
    "         'x', legend=['data', '1-step preds', 'multistep preds'],\n",
    "         xlim=[1, 1000], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22489370",
   "metadata": {},
   "source": [
    "如上面的例子所示，绿线的预测显然并不理想。\n",
    "经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。\n",
    "为什么这个算法效果这么差呢？事实是由于错误的累积：\n",
    "假设在步骤$1$之后，我们积累了一些错误$\\epsilon_1 = \\bar\\epsilon$。\n",
    "于是，步骤$2$的输入被扰动了$\\epsilon_1$，\n",
    "结果积累的误差是依照次序的$\\epsilon_2 = \\bar\\epsilon + c \\epsilon_1$，\n",
    "其中$c$为某个常数，后面的预测误差依此类推。\n",
    "因此误差可能会相当快地偏离真实的观测结果。\n",
    "例如，未来$24$小时的天气预报往往相当准确，\n",
    "但超过这一点，精度就会迅速下降。\n",
    "我们将在本章及后续章节中讨论如何改进这一点。\n",
    "\n",
    "基于$k = 1, 4, 16, 64$，通过对整个序列预测的计算，\n",
    "让我们[**更仔细地看一下$k$步预测**]的困难。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc05ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "max_steps = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "features = d2l.zeros((T - tau - max_steps + 1, tau + max_steps))\n",
    "# 列i（i<tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）\n",
    "for i in range(tau):\n",
    "    features[:, i] = x[i: i + T - tau - max_steps + 1]\n",
    "\n",
    "# 列i（i>=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）\n",
    "for i in range(tau, tau + max_steps):\n",
    "    features[:, i] = d2l.reshape(net(features[:, i - tau: i]), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2284797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "features = tf.Variable(d2l.zeros((T - tau - max_steps + 1, tau + max_steps)))\n",
    "# 列i（i<tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）\n",
    "for i in range(tau):\n",
    "    features[:, i].assign(x[i: i + T - tau - max_steps + 1].numpy())\n",
    "\n",
    "# 列i（i>=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）\n",
    "for i in range(tau, tau + max_steps):\n",
    "    features[:, i].assign(d2l.reshape(net((features[:, i - tau: i])), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc30633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "features = d2l.zeros((T - tau - max_steps + 1, tau + max_steps))\n",
    "# 列i（i<tau）是来自x的观测，其时间步从（i+1）到（i+T-tau-max_steps+1）\n",
    "for i in range(tau):\n",
    "    features[:, i] = x[i: i + T - tau - max_steps + 1]\n",
    "\n",
    "# 列i（i>=tau）是来自（i-tau+1）步的预测，其时间步从（i+1）到（i+T-tau-max_steps+1）\n",
    "for i in range(tau, tau + max_steps):\n",
    "    features[:, i] = d2l.reshape(net(features[:, i - tau: i]), [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "steps = (1, 4, 16, 64)\n",
    "d2l.plot([time[tau + i - 1: T - max_steps + i] for i in steps],\n",
    "         [d2l.numpy(features[:, tau + i - 1]) for i in steps], 'time', 'x',\n",
    "         legend=[f'{i}-step preds' for i in steps], xlim=[5, 1000],\n",
    "         figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b997f",
   "metadata": {},
   "source": [
    "以上例子清楚地说明了当我们试图预测更远的未来时，预测的质量是如何变化的。\n",
    "虽然“$4$步预测”看起来仍然不错，但超过这个跨度的任何预测几乎都是无用的。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 内插法（在现有观测值之间进行估计）和外推法（对超出已知观测范围进行预测）在实践的难度上差别很大。因此，对于所拥有的序列数据，在训练时始终要尊重其时间顺序，即最好不要基于未来的数据进行训练。\n",
    "* 序列模型的估计需要专门的统计工具，两种较流行的选择是自回归模型和隐变量自回归模型。\n",
    "* 对于时间是向前推进的因果模型，正向估计通常比反向估计更容易。\n",
    "* 对于直到时间步$t$的观测序列，其在时间步$t+k$的预测输出是“$k$步预测”。随着我们对预测时间$k$值的增加，会造成误差的快速累积和预测质量的极速下降。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 改进本节实验中的模型。\n",
    "    1. 是否包含了过去$4$个以上的观测结果？真实值需要是多少个？\n",
    "    1. 如果没有噪音，需要多少个过去的观测结果？提示：把$\\sin$和$\\cos$写成微分方程。\n",
    "    1. 可以在保持特征总数不变的情况下合并旧的观察结果吗？这能提高正确度吗？为什么？\n",
    "    1. 改变神经网络架构并评估其性能。\n",
    "1. 一位投资者想要找到一种好的证券来购买。他查看过去的回报，以决定哪一种可能是表现良好的。这一策略可能会出什么问题呢？\n",
    "1. 时间是向前推进的因果模型在多大程度上适用于文本呢？\n",
    "1. 举例说明什么时候可能需要隐变量自回归模型来捕捉数据的动力学模型。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2090)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2091)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/2092)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11795)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b5c43",
   "metadata": {},
   "source": [
    "# 文本预处理\n",
    ":label:`sec_text_preprocessing`\n",
    "\n",
    "对于序列数据处理问题，我们在 :numref:`sec_sequence`中\n",
    "评估了所需的统计工具和预测时面临的挑战。\n",
    "这样的数据存在许多种形式，文本是最常见例子之一。\n",
    "例如，一篇文章可以被简单地看作一串单词序列，甚至是一串字符序列。\n",
    "本节中，我们将解析文本的常见预处理步骤。\n",
    "这些步骤通常包括：\n",
    "\n",
    "1. 将文本作为字符串加载到内存中。\n",
    "1. 将字符串拆分为词元（如单词和字符）。\n",
    "1. 建立一个词表，将拆分的词元映射到数字索引。\n",
    "1. 将文本转换为数字索引序列，方便模型操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from d2l import mxnet as d2l\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ac4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import collections\n",
    "from d2l import torch as d2l\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87694ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import collections\n",
    "from d2l import tensorflow as d2l\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14fac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import collections\n",
    "from d2l import paddle as d2l\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e36062",
   "metadata": {},
   "source": [
    "## 读取数据集\n",
    "\n",
    "首先，我们从H.G.Well的[时光机器](https://www.gutenberg.org/ebooks/35)中加载文本。\n",
    "这是一个相当小的语料库，只有30000多个单词，但足够我们小试牛刀，\n",
    "而现实中的文档集合可能会包含数十亿个单词。\n",
    "下面的函数(**将数据集读取到由多条文本行组成的列表中**)，其中每条文本行都是一个字符串。\n",
    "为简单起见，我们在这里忽略了标点符号和字母大写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076af17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
    "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine():  #@save\n",
    "    \"\"\"将时间机器数据集加载到文本行的列表中\"\"\"\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "lines = read_time_machine()\n",
    "print(f'# 文本总行数: {len(lines)}')\n",
    "print(lines[0])\n",
    "print(lines[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc67ed",
   "metadata": {},
   "source": [
    "## 词元化\n",
    "\n",
    "下面的`tokenize`函数将文本行列表（`lines`）作为输入，\n",
    "列表中的每个元素是一个文本序列（如一条文本行）。\n",
    "[**每个文本序列又被拆分成一个词元列表**]，*词元*（token）是文本的基本单位。\n",
    "最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d347e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def tokenize(lines, token='word'):  #@save\n",
    "    \"\"\"将文本行拆分为单词或字符词元\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误：未知词元类型：' + token)\n",
    "\n",
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b289ec7",
   "metadata": {},
   "source": [
    "## 词表\n",
    "\n",
    "词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。\n",
    "现在，让我们[**构建一个字典，通常也叫做*词表*（vocabulary），\n",
    "用来将字符串类型的词元映射到从$0$开始的数字索引中**]。\n",
    "我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，\n",
    "得到的统计结果称之为*语料*（corpus）。\n",
    "然后根据每个唯一词元的出现频率，为其分配一个数字索引。\n",
    "很少出现的词元通常被移除，这可以降低复杂性。\n",
    "另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“&lt;unk&gt;”。\n",
    "我们可以选择增加一个列表，用于保存那些被保留的词元，\n",
    "例如：填充词元（“&lt;pad&gt;”）；\n",
    "序列开始词元（“&lt;bos&gt;”）；\n",
    "序列结束词元（“&lt;eos&gt;”）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5120fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "class Vocab:  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = [] \n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "        \n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5abab5b",
   "metadata": {},
   "source": [
    "我们首先使用时光机器数据集作为语料库来[**构建词表**]，然后打印前几个高频词元及其索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a637cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04974e7",
   "metadata": {},
   "source": [
    "现在，我们可以(**将每一条文本行转换成一个数字索引列表**)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58779732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "for i in [0, 10]:\n",
    "    print('文本:', tokens[i])\n",
    "    print('索引:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6d580",
   "metadata": {},
   "source": [
    "## 整合所有功能\n",
    "\n",
    "在使用上述函数时，我们[**将所有功能打包到`load_corpus_time_machine`函数中**]，\n",
    "该函数返回`corpus`（词元索引列表）和`vocab`（时光机器语料库的词表）。\n",
    "我们在这里所做的改变是：\n",
    "\n",
    "1. 为了简化后面章节中的训练，我们使用字符（而不是单词）实现文本词元化；\n",
    "1. 时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的`corpus`仅处理为单个列表，而不是使用多词元列表构成的一个列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62988cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b24f2c",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 文本是序列数据的一种最常见的形式之一。\n",
    "* 为了对文本进行预处理，我们通常将文本拆分为词元，构建词表将词元字符串映射为数字索引，并将文本数据转换为词元索引以供模型操作。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 词元化是一个关键的预处理步骤，它因语言而异。尝试找到另外三种常用的词元化文本的方法。\n",
    "1. 在本节的实验中，将文本词元为单词和更改`Vocab`实例的`min_freq`参数。这对词表大小有何影响？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2093)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2094)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/2095)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11796)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d066f55",
   "metadata": {},
   "source": [
    "# 语言模型和数据集\n",
    ":label:`sec_language_model`\n",
    "\n",
    "在 :numref:`sec_text_preprocessing`中，\n",
    "我们了解了如何将文本数据映射为词元，\n",
    "以及将这些词元可以视为一系列离散的观测，例如单词或字符。\n",
    "假设长度为$T$的文本序列中的词元依次为$x_1, x_2, \\ldots, x_T$。\n",
    "于是，$x_t$（$1 \\leq t \\leq T$）\n",
    "可以被认为是文本序列在时间步$t$处的观测或标签。\n",
    "在给定这样的文本序列时，*语言模型*（language model）的目标是估计序列的联合概率\n",
    "\n",
    "$$P(x_1, x_2, \\ldots, x_T).$$\n",
    "\n",
    "例如，只需要一次抽取一个词元$x_t \\sim P(x_t \\mid x_{t-1}, \\ldots, x_1)$，\n",
    "一个理想的语言模型就能够基于模型本身生成自然文本。\n",
    "与猴子使用打字机完全不同的是，从这样的模型中提取的文本\n",
    "都将作为自然语言（例如，英语文本）来传递。\n",
    "只需要基于前面的对话片断中的文本，\n",
    "就足以生成一个有意义的对话。\n",
    "显然，我们离设计出这样的系统还很遥远，\n",
    "因为它需要“理解”文本，而不仅仅是生成语法合理的内容。\n",
    "\n",
    "尽管如此，语言模型依然是非常有用的。\n",
    "例如，短语“to recognize speech”和“to wreck a nice beach”读音上听起来非常相似。\n",
    "这种相似性会导致语音识别中的歧义，但是这很容易通过语言模型来解决，\n",
    "因为第二句的语义很奇怪。\n",
    "同样，在文档摘要生成算法中，\n",
    "“狗咬人”比“人咬狗”出现的频率要高得多，\n",
    "或者“我想吃奶奶”是一个相当匪夷所思的语句，\n",
    "而“我想吃，奶奶”则要正常得多。\n",
    "\n",
    "## 学习语言模型\n",
    "\n",
    "显而易见，我们面对的问题是如何对一个文档，\n",
    "甚至是一个词元序列进行建模。\n",
    "假设在单词级别对文本数据进行词元化，\n",
    "我们可以依靠在 :numref:`sec_sequence`中对序列模型的分析。\n",
    "让我们从基本概率规则开始：\n",
    "\n",
    "$$P(x_1, x_2, \\ldots, x_T) = \\prod_{t=1}^T P(x_t  \\mid  x_1, \\ldots, x_{t-1}).$$\n",
    "\n",
    "例如，包含了四个单词的一个文本序列的概率是：\n",
    "\n",
    "$$P(\\text{deep}, \\text{learning}, \\text{is}, \\text{fun}) =  P(\\text{deep}) P(\\text{learning}  \\mid  \\text{deep}) P(\\text{is}  \\mid  \\text{deep}, \\text{learning}) P(\\text{fun}  \\mid  \\text{deep}, \\text{learning}, \\text{is}).$$\n",
    "\n",
    "为了训练语言模型，我们需要计算单词的概率，\n",
    "以及给定前面几个单词后出现某个单词的条件概率。\n",
    "这些概率本质上就是语言模型的参数。\n",
    "\n",
    "这里，我们假设训练数据集是一个大型的文本语料库。\n",
    "比如，维基百科的所有条目、\n",
    "[古登堡计划](https://en.wikipedia.org/wiki/Project_Gutenberg)，\n",
    "或者所有发布在网络上的文本。\n",
    "训练数据集中词的概率可以根据给定词的相对词频来计算。\n",
    "例如，可以将估计值$\\hat{P}(\\text{deep})$\n",
    "计算为任何以单词“deep”开头的句子的概率。\n",
    "一种（稍稍不太精确的）方法是统计单词“deep”在数据集中的出现次数，\n",
    "然后将其除以整个语料库中的单词总数。\n",
    "这种方法效果不错，特别是对于频繁出现的单词。\n",
    "接下来，我们可以尝试估计\n",
    "\n",
    "$$\\hat{P}(\\text{learning} \\mid \\text{deep}) = \\frac{n(\\text{deep, learning})}{n(\\text{deep})},$$\n",
    "\n",
    "其中$n(x)$和$n(x, x')$分别是单个单词和连续单词对的出现次数。\n",
    "不幸的是，由于连续单词对“deep learning”的出现频率要低得多，\n",
    "所以估计这类单词正确的概率要困难得多。\n",
    "特别是对于一些不常见的单词组合，要想找到足够的出现次数来获得准确的估计可能都不容易。\n",
    "而对于三个或者更多的单词组合，情况会变得更糟。\n",
    "许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。\n",
    "除非我们提供某种解决方案，来将这些单词组合指定为非零计数，\n",
    "否则将无法在语言模型中使用它们。\n",
    "如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。\n",
    "\n",
    "一种常见的策略是执行某种形式的*拉普拉斯平滑*（Laplace smoothing），\n",
    "具体方法是在所有计数中添加一个小常量。\n",
    "用$n$表示训练集中的单词总数，用$m$表示唯一单词的数量。\n",
    "此解决方案有助于处理单元素问题，例如通过：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat{P}(x) & = \\frac{n(x) + \\epsilon_1/m}{n + \\epsilon_1}, \\\\\n",
    "    \\hat{P}(x' \\mid x) & = \\frac{n(x, x') + \\epsilon_2 \\hat{P}(x')}{n(x) + \\epsilon_2}, \\\\\n",
    "    \\hat{P}(x'' \\mid x,x') & = \\frac{n(x, x',x'') + \\epsilon_3 \\hat{P}(x'')}{n(x, x') + \\epsilon_3}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中，$\\epsilon_1,\\epsilon_2$和$\\epsilon_3$是超参数。\n",
    "以$\\epsilon_1$为例：当$\\epsilon_1 = 0$时，不应用平滑；\n",
    "当$\\epsilon_1$接近正无穷大时，$\\hat{P}(x)$接近均匀概率分布$1/m$。\n",
    "上面的公式是 :cite:`Wood.Gasthaus.Archambeau.ea.2011`\n",
    "的一个相当原始的变形。\n",
    "\n",
    "然而，这样的模型很容易变得无效，原因如下：\n",
    "首先，我们需要存储所有的计数；\n",
    "其次，这完全忽略了单词的意思。\n",
    "例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中，\n",
    "但是想根据上下文调整这类模型其实是相当困难的。\n",
    "最后，长单词序列大部分是没出现过的，\n",
    "因此一个模型如果只是简单地统计先前“看到”的单词序列频率，\n",
    "那么模型面对这种问题肯定是表现不佳的。\n",
    "\n",
    "## 马尔可夫模型与$n$元语法\n",
    "\n",
    "在讨论包含深度学习的解决方案之前，我们需要了解更多的概念和术语。\n",
    "回想一下我们在 :numref:`sec_sequence`中对马尔可夫模型的讨论，\n",
    "并且将其应用于语言建模。\n",
    "如果$P(x_{t+1} \\mid x_t, \\ldots, x_1) = P(x_{t+1} \\mid x_t)$，\n",
    "则序列上的分布满足一阶马尔可夫性质。\n",
    "阶数越高，对应的依赖关系就越长。\n",
    "这种性质推导出了许多可以应用于序列建模的近似公式：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2) P(x_3) P(x_4),\\\\\n",
    "P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_2) P(x_4  \\mid  x_3),\\\\\n",
    "P(x_1, x_2, x_3, x_4) &=  P(x_1) P(x_2  \\mid  x_1) P(x_3  \\mid  x_1, x_2) P(x_4  \\mid  x_2, x_3).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "通常，涉及一个、两个和三个变量的概率公式分别被称为\n",
    "*一元语法*（unigram）、*二元语法*（bigram）和*三元语法*（trigram）模型。\n",
    "下面，我们将学习如何去设计更好的模型。\n",
    "\n",
    "## 自然语言统计\n",
    "\n",
    "我们看看在真实数据上如果进行自然语言统计。\n",
    "根据 :numref:`sec_text_preprocessing`中介绍的时光机器数据集构建词表，\n",
    "并打印前$10$个最常用的（频率最高的）单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "import random\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6125260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd74e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d843f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8b8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "tokens = d2l.tokenize(d2l.read_time_machine())\n",
    "# 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起\n",
    "corpus = [token for line in tokens for token in line]\n",
    "vocab = d2l.Vocab(corpus)\n",
    "vocab.token_freqs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ea965",
   "metadata": {},
   "source": [
    "正如我们所看到的，(**最流行的词**)看起来很无聊，\n",
    "这些词通常(**被称为*停用词***)（stop words），因此可以被过滤掉。\n",
    "尽管如此，它们本身仍然是有意义的，我们仍然会在模型中使用它们。\n",
    "此外，还有个明显的问题是词频衰减的速度相当地快。\n",
    "例如，最常用单词的词频对比，第$10$个还不到第$1$个的$1/5$。\n",
    "为了更好地理解，我们可以[**画出的词频图**]："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5103b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "freqs = [freq for token, freq in vocab.token_freqs]\n",
    "d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)',\n",
    "         xscale='log', yscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71163e",
   "metadata": {},
   "source": [
    "通过此图我们可以发现：词频以一种明确的方式迅速衰减。\n",
    "将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。\n",
    "这意味着单词的频率满足*齐普夫定律*（Zipf's law），\n",
    "即第$i$个最常用单词的频率$n_i$为：\n",
    "\n",
    "$$n_i \\propto \\frac{1}{i^\\alpha},$$\n",
    ":eqlabel:`eq_zipf_law`\n",
    "\n",
    "等价于\n",
    "\n",
    "$$\\log n_i = -\\alpha \\log i + c,$$\n",
    "\n",
    "其中$\\alpha$是刻画分布的指数，$c$是常数。\n",
    "这告诉我们想要通过计数统计和平滑来建模单词是不可行的，\n",
    "因为这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。\n",
    "那么[**其他的词元组合，比如二元语法、三元语法等等，又会如何呢？**]\n",
    "我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549df824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]\n",
    "bigram_vocab = d2l.Vocab(bigram_tokens)\n",
    "bigram_vocab.token_freqs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4c6c1",
   "metadata": {},
   "source": [
    "这里值得注意：在十个最频繁的词对中，有九个是由两个停用词组成的，\n",
    "只有一个与“the time”有关。\n",
    "我们再进一步看看三元语法的频率是否表现出相同的行为方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8446c155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "trigram_tokens = [triple for triple in zip(\n",
    "    corpus[:-2], corpus[1:-1], corpus[2:])]\n",
    "trigram_vocab = d2l.Vocab(trigram_tokens)\n",
    "trigram_vocab.token_freqs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf33ec",
   "metadata": {},
   "source": [
    "最后，我们[**直观地对比三种模型中的词元频率**]：一元语法、二元语法和三元语法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75263224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]\n",
    "trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]\n",
    "d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',\n",
    "         ylabel='frequency: n(x)', xscale='log', yscale='log',\n",
    "         legend=['unigram', 'bigram', 'trigram'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c72c5",
   "metadata": {},
   "source": [
    "这张图非常令人振奋！原因有很多：\n",
    "\n",
    "1. 除了一元语法词，单词序列似乎也遵循齐普夫定律，\n",
    "尽管公式 :eqref:`eq_zipf_law`中的指数$\\alpha$更小\n",
    "（指数的大小受序列长度的影响）；\n",
    "2. 词表中$n$元组的数量并没有那么大，这说明语言中存在相当多的结构，\n",
    "这些结构给了我们应用模型的希望；\n",
    "3. 很多$n$元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。\n",
    "作为代替，我们将使用基于深度学习的模型。\n",
    "\n",
    "## 读取长序列数据\n",
    "\n",
    "由于序列数据本质上是连续的，因此我们在处理数据时需要解决这个问题。\n",
    "在 :numref:`sec_sequence`中我们以一种相当特别的方式做到了这一点：\n",
    "当序列变得太长而不能被模型一次性全部处理时，\n",
    "我们可能希望拆分这样的序列方便模型读取。\n",
    "\n",
    "在介绍该模型之前，我们看一下总体策略。\n",
    "假设我们将使用神经网络来训练语言模型，\n",
    "模型中的网络一次处理具有预定义长度\n",
    "（例如$n$个时间步）的一个小批量序列。\n",
    "现在的问题是如何[**随机生成一个小批量数据的特征和标签以供读取。**]\n",
    "\n",
    "首先，由于文本序列可以是任意长的，\n",
    "例如整本《时光机器》（*The Time Machine*），\n",
    "于是任意长的序列可以被我们划分为具有相同时间步数的子序列。\n",
    "当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。\n",
    "假设网络一次只处理具有$n$个时间步的子序列。\n",
    " :numref:`fig_timemachine_5gram`画出了\n",
    "从原始文本序列获得子序列的所有不同的方式，\n",
    "其中$n=5$，并且每个时间步的词元对应于一个字符。\n",
    "请注意，因为我们可以选择任意偏移量来指示初始位置，所以我们有相当大的自由度。\n",
    "\n",
    "![分割文本时，不同的偏移量会导致不同的子序列](../img/timemachine-5gram.svg)\n",
    ":label:`fig_timemachine_5gram`\n",
    "\n",
    "因此，我们应该从 :numref:`fig_timemachine_5gram`中选择哪一个呢？\n",
    "事实上，他们都一样的好。\n",
    "然而，如果我们只选择一个偏移量，\n",
    "那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。\n",
    "因此，我们可以从随机偏移量开始划分序列，\n",
    "以同时获得*覆盖性*（coverage）和*随机性*（randomness）。\n",
    "下面，我们将描述如何实现*随机采样*（random sampling）和\n",
    "*顺序分区*（sequential partitioning）策略。\n",
    "\n",
    "### 随机采样\n",
    "\n",
    "(**在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。**)\n",
    "在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。\n",
    "对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元，\n",
    "因此标签是移位了一个词元的原始序列。\n",
    "\n",
    "下面的代码每次可以从数据中随机生成一个小批量。\n",
    "在这里，参数`batch_size`指定了每个小批量中子序列样本的数目，\n",
    "参数`num_steps`是每个子序列中预定义的时间步数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield d2l.tensor(X), d2l.tensor(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f3d848",
   "metadata": {},
   "source": [
    "下面我们[**生成一个从$0$到$34$的序列**]。\n",
    "假设批量大小为$2$，时间步数为$5$，这意味着可以生成\n",
    "$\\lfloor (35 - 1) / 5 \\rfloor= 6$个“特征－标签”子序列对。\n",
    "如果设置小批量大小为$2$，我们只能得到$3$个小批量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "my_seq = list(range(35))\n",
    "for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13092b",
   "metadata": {},
   "source": [
    "### 顺序分区\n",
    "\n",
    "在迭代过程中，除了对原始序列可以随机抽样外，\n",
    "我们还可以[**保证两个相邻的小批量中的子序列在原始序列上也是相邻的**]。\n",
    "这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始划分序列\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = d2l.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = d2l.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc59710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始划分序列\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = d2l.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = d2l.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs = d2l.reshape(Xs, (batch_size, -1))\n",
    "    Ys = d2l.reshape(Ys, (batch_size, -1))\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_batches * num_steps, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始划分序列\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = d2l.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = d2l.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape((batch_size, -1)), Ys.reshape((batch_size, -1))\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94cd62e",
   "metadata": {},
   "source": [
    "基于相同的设置，通过顺序分区[**读取每个小批量的子序列的特征`X`和标签`Y`**]。\n",
    "通过将它们打印出来可以发现：\n",
    "迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f35b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n",
    "    print('X: ', X, '\\nY:', Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f73df1",
   "metadata": {},
   "source": [
    "现在，我们[**将上面的两个采样函数包装到一个类中**]，\n",
    "以便稍后可以将其用作数据迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeba147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "class SeqDataLoader:  #@save\n",
    "    \"\"\"加载序列数据的迭代器\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = d2l.seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = d2l.seq_data_iter_sequential\n",
    "        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1316df48",
   "metadata": {},
   "source": [
    "[**最后，我们定义了一个函数`load_data_time_machine`，\n",
    "它同时返回数据迭代器和词表**]，\n",
    "因此可以与其他带有`load_data`前缀的函数\n",
    "（如 :numref:`sec_fashion_mnist`中定义的\n",
    "`d2l.load_data_fashion_mnist`）类似地使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def load_data_time_machine(batch_size, num_steps,  #@save\n",
    "                           use_random_iter=False, max_tokens=10000):\n",
    "    \"\"\"返回时光机器数据集的迭代器和词表\"\"\"\n",
    "    data_iter = SeqDataLoader(\n",
    "        batch_size, num_steps, use_random_iter, max_tokens)\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a1b75",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 语言模型是自然语言处理的关键。\n",
    "* $n$元语法通过截断相关性，为处理长序列提供了一种实用的模型。\n",
    "* 长序列存在一个问题：它们很少出现或者从不出现。\n",
    "* 齐普夫定律支配着单词的分布，这个分布不仅适用于一元语法，还适用于其他$n$元语法。\n",
    "* 通过拉普拉斯平滑法可以有效地处理结构丰富而频率不足的低频词词组。\n",
    "* 读取长序列的主要方式是随机采样和顺序分区。在迭代过程中，后者可以保证来自两个相邻的小批量中的子序列在原始序列上也是相邻的。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 假设训练数据集中有$100,000$个单词。一个四元语法需要存储多少个词频和相邻多词频率？\n",
    "1. 我们如何对一系列对话建模？\n",
    "1. 一元语法、二元语法和三元语法的齐普夫定律的指数是不一样的，能设法估计么？\n",
    "1. 想一想读取长序列数据的其他方法？\n",
    "1. 考虑一下我们用于读取长序列的随机偏移量。\n",
    "    1. 为什么随机偏移量是个好主意？\n",
    "    1. 它真的会在文档的序列上实现完美的均匀分布吗？\n",
    "    1. 要怎么做才能使分布更均匀？\n",
    "1. 如果我们希望一个序列样本是一个完整的句子，那么这在小批量抽样中会带来怎样的问题？如何解决？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2096)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2097)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/2098)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11797)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a8d79",
   "metadata": {},
   "source": [
    "# 循环神经网络\n",
    ":label:`sec_rnn`\n",
    "\n",
    "在 :numref:`sec_language_model`中，\n",
    "我们介绍了$n$元语法模型，\n",
    "其中单词$x_t$在时间步$t$的条件概率仅取决于前面$n-1$个单词。\n",
    "对于时间步$t-(n-1)$之前的单词，\n",
    "如果我们想将其可能产生的影响合并到$x_t$上，\n",
    "需要增加$n$，然而模型参数的数量也会随之呈指数增长，\n",
    "因为词表$\\mathcal{V}$需要存储$|\\mathcal{V}|^n$个数字，\n",
    "因此与其将$P(x_t \\mid x_{t-1}, \\ldots, x_{t-n+1})$模型化，\n",
    "不如使用隐变量模型：\n",
    "\n",
    "$$P(x_t \\mid x_{t-1}, \\ldots, x_1) \\approx P(x_t \\mid h_{t-1}),$$\n",
    "\n",
    "其中$h_{t-1}$是*隐状态*（hidden state），\n",
    "也称为*隐藏变量*（hidden variable），\n",
    "它存储了到时间步$t-1$的序列信息。\n",
    "通常，我们可以基于当前输入$x_{t}$和先前隐状态$h_{t-1}$\n",
    "来计算时间步$t$处的任何时间的隐状态：\n",
    "\n",
    "$$h_t = f(x_{t}, h_{t-1}).$$\n",
    ":eqlabel:`eq_ht_xt`\n",
    "\n",
    "对于 :eqref:`eq_ht_xt`中的函数$f$，隐变量模型不是近似值。\n",
    "毕竟$h_t$是可以仅仅存储到目前为止观察到的所有数据，\n",
    "然而这样的操作可能会使计算和存储的代价都变得昂贵。\n",
    "\n",
    "回想一下，我们在 :numref:`chap_perceptrons`中\n",
    "讨论过的具有隐藏单元的隐藏层。\n",
    "值得注意的是，隐藏层和隐状态指的是两个截然不同的概念。\n",
    "如上所述，隐藏层是在从输入到输出的路径上（以观测角度来理解）的隐藏的层，\n",
    "而隐状态则是在给定步骤所做的任何事情（以技术角度来定义）的*输入*，\n",
    "并且这些状态只能通过先前时间步的数据来计算。\n",
    "\n",
    "*循环神经网络*（recurrent neural networks，RNNs）\n",
    "是具有隐状态的神经网络。\n",
    "在介绍循环神经网络模型之前，\n",
    "我们首先回顾 :numref:`sec_mlp`中介绍的多层感知机模型。\n",
    "\n",
    "## 无隐状态的神经网络\n",
    "\n",
    "让我们来看一看只有单隐藏层的多层感知机。\n",
    "设隐藏层的激活函数为$\\phi$，\n",
    "给定一个小批量样本$\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，\n",
    "其中批量大小为$n$，输入维度为$d$，\n",
    "则隐藏层的输出$\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$通过下式计算：\n",
    "\n",
    "$$\\mathbf{H} = \\phi(\\mathbf{X} \\mathbf{W}_{xh} + \\mathbf{b}_h).$$\n",
    ":eqlabel:`rnn_h_without_state`\n",
    "\n",
    "在 :eqref:`rnn_h_without_state`中，\n",
    "我们拥有的隐藏层权重参数为$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}$，\n",
    "偏置参数为$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$，\n",
    "以及隐藏单元的数目为$h$。\n",
    "因此求和时可以应用广播机制（见 :numref:`subsec_broadcasting`）。\n",
    "接下来，将隐藏变量$\\mathbf{H}$用作输出层的输入。\n",
    "输出层由下式给出：\n",
    "\n",
    "$$\\mathbf{O} = \\mathbf{H} \\mathbf{W}_{hq} + \\mathbf{b}_q,$$\n",
    "\n",
    "其中，$\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$是输出变量，\n",
    "$\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$是权重参数，\n",
    "$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$是输出层的偏置参数。\n",
    "如果是分类问题，我们可以用$\\text{softmax}(\\mathbf{O})$\n",
    "来计算输出类别的概率分布。\n",
    "\n",
    "这完全类似于之前在 :numref:`sec_sequence`中解决的回归问题，\n",
    "因此我们省略了细节。\n",
    "无须多言，只要可以随机选择“特征-标签”对，\n",
    "并且通过自动微分和随机梯度下降能够学习网络参数就可以了。\n",
    "\n",
    "## 有隐状态的循环神经网络\n",
    ":label:`subsec_rnn_w_hidden_states`\n",
    "\n",
    "有了隐状态后，情况就完全不同了。\n",
    "假设我们在时间步$t$有小批量输入$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$。\n",
    "换言之，对于$n$个序列样本的小批量，\n",
    "$\\mathbf{X}_t$的每一行对应于来自该序列的时间步$t$处的一个样本。\n",
    "接下来，用$\\mathbf{H}_t  \\in \\mathbb{R}^{n \\times h}$\n",
    "表示时间步$t$的隐藏变量。\n",
    "与多层感知机不同的是，\n",
    "我们在这里保存了前一个时间步的隐藏变量$\\mathbf{H}_{t-1}$，\n",
    "并引入了一个新的权重参数$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$，\n",
    "来描述如何在当前时间步中使用前一个时间步的隐藏变量。\n",
    "具体地说，当前时间步隐藏变量由当前时间步的输入\n",
    "与前一个时间步的隐藏变量一起计算得出：\n",
    "\n",
    "$$\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}  + \\mathbf{b}_h).$$\n",
    ":eqlabel:`rnn_h_with_state`\n",
    "\n",
    "与 :eqref:`rnn_h_without_state`相比，\n",
    " :eqref:`rnn_h_with_state`多添加了一项\n",
    "$\\mathbf{H}_{t-1} \\mathbf{W}_{hh}$，\n",
    "从而实例化了 :eqref:`eq_ht_xt`。\n",
    "从相邻时间步的隐藏变量$\\mathbf{H}_t$和\n",
    "$\\mathbf{H}_{t-1}$之间的关系可知，\n",
    "这些变量捕获并保留了序列直到其当前时间步的历史信息，\n",
    "就如当前时间步下神经网络的状态或记忆，\n",
    "因此这样的隐藏变量被称为*隐状态*（hidden state）。\n",
    "由于在当前时间步中，\n",
    "隐状态使用的定义与前一个时间步中使用的定义相同，\n",
    "因此 :eqref:`rnn_h_with_state`的计算是*循环的*（recurrent）。\n",
    "于是基于循环计算的隐状态神经网络被命名为\n",
    "*循环神经网络*（recurrent neural network）。\n",
    "在循环神经网络中执行 :eqref:`rnn_h_with_state`计算的层\n",
    "称为*循环层*（recurrent layer）。\n",
    "\n",
    "有许多不同的方法可以构建循环神经网络，\n",
    "由 :eqref:`rnn_h_with_state`定义的隐状态的循环神经网络是非常常见的一种。\n",
    "对于时间步$t$，输出层的输出类似于多层感知机中的计算：\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{hq} + \\mathbf{b}_q.$$\n",
    "\n",
    "循环神经网络的参数包括隐藏层的权重\n",
    "$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$和偏置$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$，\n",
    "以及输出层的权重$\\mathbf{W}_{hq} \\in \\mathbb{R}^{h \\times q}$\n",
    "和偏置$\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}$。\n",
    "值得一提的是，即使在不同的时间步，循环神经网络也总是使用这些模型参数。\n",
    "因此，循环神经网络的参数开销不会随着时间步的增加而增加。\n",
    "\n",
    " :numref:`fig_rnn`展示了循环神经网络在三个相邻时间步的计算逻辑。\n",
    "在任意时间步$t$，隐状态的计算可以被视为：\n",
    "\n",
    "1. 拼接当前时间步$t$的输入$\\mathbf{X}_t$和前一时间步$t-1$的隐状态$\\mathbf{H}_{t-1}$；\n",
    "1. 将拼接的结果送入带有激活函数$\\phi$的全连接层。\n",
    "   全连接层的输出是当前时间步$t$的隐状态$\\mathbf{H}_t$。\n",
    "   \n",
    "在本例中，模型参数是$\\mathbf{W}_{xh}$和$\\mathbf{W}_{hh}$的拼接，\n",
    "以及$\\mathbf{b}_h$的偏置，所有这些参数都来自 :eqref:`rnn_h_with_state`。\n",
    "当前时间步$t$的隐状态$\\mathbf{H}_t$\n",
    "将参与计算下一时间步$t+1$的隐状态$\\mathbf{H}_{t+1}$。\n",
    "而且$\\mathbf{H}_t$还将送入全连接输出层，\n",
    "用于计算当前时间步$t$的输出$\\mathbf{O}_t$。\n",
    "\n",
    "![具有隐状态的循环神经网络](../img/rnn.svg)\n",
    ":label:`fig_rnn`\n",
    "\n",
    "我们刚才提到，隐状态中\n",
    "$\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{H}_{t-1} \\mathbf{W}_{hh}$的计算，\n",
    "相当于$\\mathbf{X}_t$和$\\mathbf{H}_{t-1}$的拼接\n",
    "与$\\mathbf{W}_{xh}$和$\\mathbf{W}_{hh}$的拼接的矩阵乘法。\n",
    "虽然这个性质可以通过数学证明，\n",
    "但在下面我们使用一个简单的代码来说明一下。\n",
    "首先，我们定义矩阵`X`、`W_xh`、`H`和`W_hh`，\n",
    "它们的形状分别为$(3，1)$、$(1，4)$、$(3，4)$和$(4，4)$。\n",
    "分别将`X`乘以`W_xh`，将`H`乘以`W_hh`，\n",
    "然后将这两个乘法相加，我们得到一个形状为$(3，4)$的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd540617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4285ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2b03c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch, paddle\n",
    "X, W_xh = d2l.normal(0, 1, (3, 1)), d2l.normal(0, 1, (1, 4))\n",
    "H, W_hh = d2l.normal(0, 1, (3, 4)), d2l.normal(0, 1, (4, 4))\n",
    "d2l.matmul(X, W_xh) + d2l.matmul(H, W_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693555bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "X, W_xh = d2l.normal((3, 1), 0, 1), d2l.normal((1, 4), 0, 1)\n",
    "H, W_hh = d2l.normal((3, 4), 0, 1), d2l.normal((4, 4), 0, 1)\n",
    "d2l.matmul(X, W_xh) + d2l.matmul(H, W_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813d52e",
   "metadata": {},
   "source": [
    "现在，我们沿列（轴1）拼接矩阵`X`和`H`，\n",
    "沿行（轴0）拼接矩阵`W_xh`和`W_hh`。\n",
    "这两个拼接分别产生形状$(3, 5)$和形状$(5, 4)$的矩阵。\n",
    "再将这两个拼接的矩阵相乘，\n",
    "我们得到与上面相同形状$(3, 4)$的输出矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "d2l.matmul(d2l.concat((X, H), 1), d2l.concat((W_xh, W_hh), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da17e33",
   "metadata": {},
   "source": [
    "## 基于循环神经网络的字符级语言模型\n",
    "\n",
    "回想一下 :numref:`sec_language_model`中的语言模型，\n",
    "我们的目标是根据过去的和当前的词元预测下一个词元，\n",
    "因此我们将原始序列移位一个词元作为标签。\n",
    "Bengio等人首先提出使用神经网络进行语言建模\n",
    " :cite:`Bengio.Ducharme.Vincent.ea.2003`。\n",
    "接下来，我们看一下如何使用循环神经网络来构建语言模型。\n",
    "设小批量大小为1，批量中的文本序列为“machine”。\n",
    "为了简化后续部分的训练，我们考虑使用\n",
    "*字符级语言模型*（character-level language model），\n",
    "将文本词元化为字符而不是单词。\n",
    " :numref:`fig_rnn_train`演示了\n",
    "如何通过基于字符级语言建模的循环神经网络，\n",
    "使用当前的和先前的字符预测下一个字符。\n",
    "\n",
    "![基于循环神经网络的字符级语言模型：输入序列和标签序列分别为“machin”和“achine”](../img/rnn-train.svg)\n",
    ":label:`fig_rnn_train`\n",
    "\n",
    "在训练过程中，我们对每个时间步的输出层的输出进行softmax操作，\n",
    "然后利用交叉熵损失计算模型输出和标签之间的误差。\n",
    "由于隐藏层中隐状态的循环计算，\n",
    " :numref:`fig_rnn_train`中的第$3$个时间步的输出$\\mathbf{O}_3$\n",
    "由文本序列“m”“a”和“c”确定。\n",
    "由于训练数据中这个文本序列的下一个字符是“h”，\n",
    "因此第$3$个时间步的损失将取决于下一个字符的概率分布，\n",
    "而下一个字符是基于特征序列“m”“a”“c”和这个时间步的标签“h”生成的。\n",
    "\n",
    "在实践中，我们使用的批量大小为$n>1$，\n",
    "每个词元都由一个$d$维向量表示。\n",
    "因此，在时间步$t$输入$\\mathbf X_t$将是一个$n\\times d$矩阵，\n",
    "这与我们在 :numref:`subsec_rnn_w_hidden_states`中的讨论相同。\n",
    "\n",
    "## 困惑度（Perplexity）\n",
    ":label:`subsec_perplexity`\n",
    "\n",
    "最后，让我们讨论如何度量语言模型的质量，\n",
    "这将在后续部分中用于评估基于循环神经网络的模型。\n",
    "一个好的语言模型能够用高度准确的词元来预测我们接下来会看到什么。\n",
    "考虑一下由不同的语言模型给出的对“It is raining ...”（“...下雨了”）的续写：\n",
    "\n",
    "1. \"It is raining outside\"（外面下雨了）；\n",
    "1. \"It is raining banana tree\"（香蕉树下雨了）；\n",
    "1. \"It is raining piouw;kcj pwepoiut\"（piouw;kcj pwepoiut下雨了）。\n",
    "\n",
    "就质量而言，例$1$显然是最合乎情理、在逻辑上最连贯的。\n",
    "虽然这个模型可能没有很准确地反映出后续词的语义，\n",
    "比如，“It is raining in San Francisco”（旧金山下雨了）\n",
    "和“It is raining in winter”（冬天下雨了）\n",
    "可能才是更完美的合理扩展，\n",
    "但该模型已经能够捕捉到跟在后面的是哪类单词。\n",
    "例$2$则要糟糕得多，因为其产生了一个无意义的续写。\n",
    "尽管如此，至少该模型已经学会了如何拼写单词，\n",
    "以及单词之间的某种程度的相关性。\n",
    "最后，例$3$表明了训练不足的模型是无法正确地拟合数据的。\n",
    "\n",
    "我们可以通过计算序列的似然概率来度量模型的质量。\n",
    "然而这是一个难以理解、难以比较的数字。\n",
    "毕竟，较短的序列比较长的序列更有可能出现，\n",
    "因此评估模型产生托尔斯泰的巨著《战争与和平》的可能性\n",
    "不可避免地会比产生圣埃克苏佩里的中篇小说《小王子》可能性要小得多。\n",
    "而缺少的可能性值相当于平均数。\n",
    "\n",
    "在这里，信息论可以派上用场了。\n",
    "我们在引入softmax回归\n",
    "（ :numref:`subsec_info_theory_basics`）时定义了熵、惊异和交叉熵，\n",
    "并在[信息论的在线附录](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html)\n",
    "中讨论了更多的信息论知识。\n",
    "如果想要压缩文本，我们可以根据当前词元集预测的下一个词元。\n",
    "一个更好的语言模型应该能让我们更准确地预测下一个词元。\n",
    "因此，它应该允许我们在压缩序列时花费更少的比特。\n",
    "所以我们可以通过一个序列中所有的$n$个词元的交叉熵损失的平均值来衡量：\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1),$$\n",
    ":eqlabel:`eq_avg_ce_for_lm`\n",
    "\n",
    "其中$P$由语言模型给出，\n",
    "$x_t$是在时间步$t$从该序列中观察到的实际词元。\n",
    "这使得不同长度的文档的性能具有了可比性。\n",
    "由于历史原因，自然语言处理的科学家更喜欢使用一个叫做*困惑度*（perplexity）的量。\n",
    "简而言之，它是 :eqref:`eq_avg_ce_for_lm`的指数：\n",
    "\n",
    "$$\\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right).$$\n",
    "\n",
    "困惑度的最好的理解是“下一个词元的实际选择数的调和平均数”。\n",
    "我们看看一些案例。\n",
    "\n",
    "* 在最好的情况下，模型总是完美地估计标签词元的概率为1。\n",
    "  在这种情况下，模型的困惑度为1。\n",
    "* 在最坏的情况下，模型总是预测标签词元的概率为0。\n",
    "  在这种情况下，困惑度是正无穷大。\n",
    "* 在基线上，该模型的预测是词表的所有可用词元上的均匀分布。\n",
    "  在这种情况下，困惑度等于词表中唯一词元的数量。\n",
    "  事实上，如果我们在没有任何压缩的情况下存储序列，\n",
    "  这将是我们能做的最好的编码方式。\n",
    "  因此，这种方式提供了一个重要的上限，\n",
    "  而任何实际模型都必须超越这个上限。\n",
    "\n",
    "在接下来的小节中，我们将基于循环神经网络实现字符级语言模型，\n",
    "并使用困惑度来评估这样的模型。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 对隐状态使用循环计算的神经网络称为循环神经网络（RNN）。\n",
    "* 循环神经网络的隐状态可以捕获直到当前时间步序列的历史信息。\n",
    "* 循环神经网络模型的参数数量不会随着时间步的增加而增加。\n",
    "* 我们可以使用循环神经网络创建字符级语言模型。\n",
    "* 我们可以使用困惑度来评价语言模型的质量。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果我们使用循环神经网络来预测文本序列中的下一个字符，那么任意输出所需的维度是多少？\n",
    "1. 为什么循环神经网络可以基于文本序列中所有先前的词元，在某个时间步表示当前词元的条件概率？\n",
    "1. 如果基于一个长序列进行反向传播，梯度会发生什么状况？\n",
    "1. 与本节中描述的语言模型相关的问题有哪些？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2099)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2100)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/2101)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11798)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a53dad",
   "metadata": {},
   "source": [
    "# 循环神经网络的从零开始实现\n",
    ":label:`sec_rnn_scratch`\n",
    "\n",
    "本节将根据 :numref:`sec_rnn`中的描述，\n",
    "从头开始基于循环神经网络实现字符级语言模型。\n",
    "这样的模型将在H.G.Wells的时光机器数据集上训练。\n",
    "和前面 :numref:`sec_language_model`中介绍过的一样，\n",
    "我们先读取数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "import math\n",
    "from mxnet import autograd, gluon, np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1998ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b244005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "%matplotlib inline\n",
    "from d2l import tensorflow as d2l\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "import paddle\n",
    "from paddle import nn\n",
    "from paddle.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "train_random_iter, vocab_random_iter = d2l.load_data_time_machine(\n",
    "    batch_size, num_steps, use_random_iter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62855a",
   "metadata": {},
   "source": [
    "## [**独热编码**]\n",
    "\n",
    "回想一下，在`train_iter`中，每个词元都表示为一个数字索引，\n",
    "将这些索引直接输入神经网络可能会使学习变得困难。\n",
    "我们通常将每个词元表示为更具表现力的特征向量。\n",
    "最简单的表示称为*独热编码*（one-hot encoding），\n",
    "它在 :numref:`subsec_classification-problem`中介绍过。\n",
    "\n",
    "简言之，将每个索引映射为相互不同的单位向量：\n",
    "假设词表中不同词元的数目为$N$（即`len(vocab)`），\n",
    "词元索引的范围为$0$到$N-1$。\n",
    "如果词元的索引是整数$i$，\n",
    "那么我们将创建一个长度为$N$的全$0$向量，\n",
    "并将第$i$处的元素设置为$1$。\n",
    "此向量是原始词元的一个独热向量。\n",
    "索引为$0$和$2$的独热向量如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.one_hot(np.array([0, 2]), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335c6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "F.one_hot(torch.tensor([0, 2]), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be20335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "tf.one_hot(tf.constant([0, 2]), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53521c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "F.one_hot(paddle.to_tensor([0, 2]), len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f730bd3",
   "metadata": {},
   "source": [
    "我们每次采样的(**小批量数据形状是二维张量：\n",
    "（批量大小，时间步数）。**)\n",
    "`one_hot`函数将这样一个小批量数据转换成三维张量，\n",
    "张量的最后一个维度等于词表大小（`len(vocab)`）。\n",
    "我们经常转换输入的维度，以便获得形状为\n",
    "（时间步数，批量大小，词表大小）的输出。\n",
    "这将使我们能够更方便地通过最外层的维度，\n",
    "一步一步地更新小批量数据的隐状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddfaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d2l.reshape(d2l.arange(10), (2, 5))\n",
    "npx.one_hot(X.T, 28).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "X = d2l.reshape(d2l.arange(10), (2, 5))\n",
    "F.one_hot(X.T, 28).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "X = d2l.reshape(d2l.arange(10), (2, 5))\n",
    "tf.one_hot(tf.transpose(X), 28).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "X = paddle.arange(10).reshape((2, 5))\n",
    "F.one_hot(X.T, 28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98af65a",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "接下来，我们[**初始化循环神经网络模型的模型参数**]。\n",
    "隐藏单元数`num_hiddens`是一个可调的超参数。\n",
    "当训练语言模型时，输入和输出来自相同的词表。\n",
    "因此，它们具有相同的维度，即词表的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877630bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return np.random.normal(scale=0.01, size=shape, ctx=device)\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = d2l.zeros(num_hiddens, ctx=device)\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = d2l.zeros(num_outputs, ctx=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec36995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = d2l.zeros(num_hiddens, device=device)\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = d2l.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3523496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def get_params(vocab_size, num_hiddens):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "    \n",
    "    def normal(shape):\n",
    "        return d2l.normal(shape=shape,stddev=0.01,mean=0,dtype=tf.float32)\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = tf.Variable(normal((num_inputs, num_hiddens)), dtype=tf.float32)\n",
    "    W_hh = tf.Variable(normal((num_hiddens, num_hiddens)), dtype=tf.float32)\n",
    "    b_h = tf.Variable(d2l.zeros(num_hiddens), dtype=tf.float32)\n",
    "    # 输出层参数\n",
    "    W_hq = tf.Variable(normal((num_hiddens, num_outputs)), dtype=tf.float32)\n",
    "    b_q = tf.Variable(d2l.zeros(num_outputs), dtype=tf.float32)\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d232ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def get_params(vocab_size, num_hiddens):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return paddle.randn(shape=shape)* 0.01\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = normal([num_inputs, num_hiddens])\n",
    "    W_hh = normal([num_hiddens, num_hiddens])\n",
    "    b_h = d2l.zeros(shape=[num_hiddens])\n",
    "    # 输出层参数\n",
    "    W_hq = normal([num_hiddens, num_outputs])\n",
    "    b_q = d2l.zeros(shape=[num_outputs])\n",
    "    # 附加梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.stop_gradient=False\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4adfe1e",
   "metadata": {},
   "source": [
    "## 循环神经网络模型\n",
    "\n",
    "为了定义循环神经网络模型，\n",
    "我们首先需要[**一个`init_rnn_state`函数在初始化时返回隐状态**]。\n",
    "这个函数的返回是一个张量，张量全用0填充，\n",
    "形状为（批量大小，隐藏单元数）。\n",
    "在后面的章节中我们将会遇到隐状态包含多个变量的情况，\n",
    "而使用元组可以更容易地处理些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b832e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (d2l.zeros((batch_size, num_hiddens), ctx=device), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc605a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (d2l.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efb1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def init_rnn_state(batch_size, num_hiddens):\n",
    "    return (d2l.zeros((batch_size, num_hiddens)), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98eea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def init_rnn_state(batch_size, num_hiddens):\n",
    "    return (paddle.zeros(shape=[batch_size, num_hiddens]), )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0289ab",
   "metadata": {},
   "source": [
    "[**下面的`rnn`函数定义了如何在一个时间步内计算隐状态和输出。**]\n",
    "循环神经网络模型通过`inputs`最外层的维度实现循环，\n",
    "以便逐时间步更新小批量数据的隐状态`H`。\n",
    "此外，这里使用$\\tanh$函数作为激活函数。\n",
    "如 :numref:`sec_mlp`所述，\n",
    "当元素在实数上满足均匀分布时，$\\tanh$函数的平均值为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4898fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # inputs的形状：(时间步数量，批量大小，词表大小)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    # X的形状：(批量大小，词表大小)\n",
    "    for X in inputs:\n",
    "        H = np.tanh(np.dot(X, W_xh) + np.dot(H, W_hh) + b_h)\n",
    "        Y = np.dot(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return np.concatenate(outputs, axis=0), (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs的形状：(时间步数量，批量大小，词表大小)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    # X的形状：(批量大小，词表大小)\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "        Y = torch.mm(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4be797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs的形状：(时间步数量，批量大小，词表大小)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    # X的形状：(批量大小，词表大小)\n",
    "    for X in inputs:\n",
    "        X = tf.reshape(X,[-1,W_xh.shape[0]])\n",
    "        H = tf.tanh(tf.matmul(X, W_xh) + tf.matmul(H, W_hh) + b_h)\n",
    "        Y = tf.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return d2l.concat(outputs, axis=0), (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b1269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def rnn(inputs, state, params):\n",
    "    # inputs的形状：(时间步数量，批量大小，词表大小)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    # X的形状：(批量大小，词表大小)\n",
    "    for X in inputs:\n",
    "        H = paddle.tanh(paddle.mm(X, W_xh) + paddle.mm(H, W_hh) + b_h)\n",
    "        Y = paddle.mm(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return paddle.concat(x=outputs, axis=0), (H,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e4e63f",
   "metadata": {},
   "source": [
    "定义了所有需要的函数之后，接下来我们[**创建一个类来包装这些函数**]，\n",
    "并存储从零开始实现的循环神经网络模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3803b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch:  #@save\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device, get_params,\n",
    "                 init_state, forward_fn):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = npx.one_hot(X.T, self.vocab_size)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, ctx):\n",
    "        return self.init_state(batch_size, self.num_hiddens, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f4d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class RNNModelScratch: #@save\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device,\n",
    "                 get_params, init_state, forward_fn):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d1e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "class RNNModelScratch: #@save\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens,\n",
    "                 init_state, forward_fn, get_params):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "        self.trainable_variables = get_params(vocab_size, num_hiddens)\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = tf.one_hot(tf.transpose(X), self.vocab_size)\n",
    "        X = tf.cast(X, tf.float32)\n",
    "        return self.forward_fn(X, state, self.trainable_variables)\n",
    "\n",
    "    def begin_state(self, batch_size, *args, **kwargs):\n",
    "        return self.init_state(batch_size, self.num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78df55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class RNNModelScratch: #@save\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens,\n",
    "                 get_params, init_state, forward_fn):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size):\n",
    "        return self.init_state(batch_size, self.num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82ce322",
   "metadata": {},
   "source": [
    "让我们[**检查输出是否具有正确的形状**]。\n",
    "例如，隐状态的维数是否保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a486b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "num_hiddens = 512\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "state = net.begin_state(X.shape[0], d2l.try_gpu())\n",
    "Y, new_state = net(X.as_in_context(d2l.try_gpu()), state)\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e273c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "num_hiddens = 512\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "state = net.begin_state(X.shape[0], d2l.try_gpu())\n",
    "Y, new_state = net(X.to(d2l.try_gpu()), state)\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ca850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "# 定义tensorflow训练策略\n",
    "device_name = d2l.try_gpu()._device_name\n",
    "strategy = tf.distribute.OneDeviceStrategy(device_name)\n",
    "\n",
    "num_hiddens = 512\n",
    "with strategy.scope():\n",
    "    net = RNNModelScratch(len(vocab), num_hiddens, init_rnn_state, rnn,\n",
    "                          get_params)\n",
    "state = net.begin_state(X.shape[0])\n",
    "Y, new_state = net(X, state)\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb47d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "num_hiddens = 512\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "state = net.begin_state(X.shape[0])\n",
    "Y, new_state = net(X, state)\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c7b890",
   "metadata": {},
   "source": [
    "我们可以看到输出形状是（时间步数$\\times$批量大小，词表大小），\n",
    "而隐状态形状保持不变，即（批量大小，隐藏单元数）。\n",
    "\n",
    "## 预测\n",
    "\n",
    "让我们[**首先定义预测函数来生成`prefix`之后的新字符**]，\n",
    "其中的`prefix`是一个用户提供的包含多个字符的字符串。\n",
    "在循环遍历`prefix`中的开始字符时，\n",
    "我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。\n",
    "这被称为*预热*（warm-up）期，\n",
    "因为在此期间模型会自我更新（例如，更新隐状态），\n",
    "但不会进行预测。\n",
    "预热期结束后，隐状态的值通常比刚开始的初始值更适合预测，\n",
    "从而预测字符并输出它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dc26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, ctx=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: d2l.reshape(\n",
    "        d2l.tensor([outputs[-1]], ctx=device), (1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(axis=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f19a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: d2l.reshape(d2l.tensor(\n",
    "        [outputs[-1]], device=device), (1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def predict_ch8(prefix, num_preds, net, vocab):  #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, dtype=tf.float32)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: d2l.reshape(d2l.tensor([outputs[-1]]), \n",
    "                                    (1, 1)).numpy()\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.numpy().argmax(axis=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: d2l.reshape(d2l.tensor(outputs[-1], place=device), (1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(paddle.reshape(paddle.argmax(y,axis=1),shape=[1])))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3c810",
   "metadata": {},
   "source": [
    "现在我们可以测试`predict_ch8`函数。\n",
    "我们将前缀指定为`time traveller `，\n",
    "并基于这个前缀生成10个后续字符。\n",
    "鉴于我们还没有训练网络，它会生成荒谬的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet,pytorch, paddle\n",
    "predict_ch8('time traveller ', 10, net, vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "predict_ch8('time traveller ', 10, net, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdf2160",
   "metadata": {},
   "source": [
    "## [**梯度裁剪**]\n",
    "\n",
    "对于长度为$T$的序列，我们在迭代中计算这$T$个时间步上的梯度，\n",
    "将会在反向传播过程中产生长度为$\\mathcal{O}(T)$的矩阵乘法链。\n",
    "如 :numref:`sec_numerical_stability`所述，\n",
    "当$T$较大时，它可能导致数值不稳定，\n",
    "例如可能导致梯度爆炸或梯度消失。\n",
    "因此，循环神经网络模型往往需要额外的方式来支持稳定训练。\n",
    "\n",
    "一般来说，当解决优化问题时，我们对模型参数采用更新步骤。\n",
    "假定在向量形式的$\\mathbf{x}$中，\n",
    "或者在小批量数据的负梯度$\\mathbf{g}$方向上。\n",
    "例如，使用$\\eta > 0$作为学习率时，在一次迭代中，\n",
    "我们将$\\mathbf{x}$更新为$\\mathbf{x} - \\eta \\mathbf{g}$。\n",
    "如果我们进一步假设目标函数$f$表现良好，\n",
    "即函数$f$在常数$L$下是*利普希茨连续的*（Lipschitz continuous）。\n",
    "也就是说，对于任意$\\mathbf{x}$和$\\mathbf{y}$我们有：\n",
    "\n",
    "$$|f(\\mathbf{x}) - f(\\mathbf{y})| \\leq L \\|\\mathbf{x} - \\mathbf{y}\\|.$$\n",
    "\n",
    "在这种情况下，我们可以安全地假设：\n",
    "如果我们通过$\\eta \\mathbf{g}$更新参数向量，则\n",
    "\n",
    "$$|f(\\mathbf{x}) - f(\\mathbf{x} - \\eta\\mathbf{g})| \\leq L \\eta\\|\\mathbf{g}\\|,$$\n",
    "\n",
    "这意味着我们不会观察到超过$L \\eta \\|\\mathbf{g}\\|$的变化。\n",
    "这既是坏事也是好事。\n",
    "坏的方面，它限制了取得进展的速度；\n",
    "好的方面，它限制了事情变糟的程度，尤其当我们朝着错误的方向前进时。\n",
    "\n",
    "有时梯度可能很大，从而优化算法可能无法收敛。\n",
    "我们可以通过降低$\\eta$的学习率来解决这个问题。\n",
    "但是如果我们很少得到大的梯度呢？\n",
    "在这种情况下，这种做法似乎毫无道理。\n",
    "一个流行的替代方案是通过将梯度$\\mathbf{g}$投影回给定半径\n",
    "（例如$\\theta$）的球来裁剪梯度$\\mathbf{g}$。\n",
    "如下式：\n",
    "\n",
    "(**$$\\mathbf{g} \\leftarrow \\min\\left(1, \\frac{\\theta}{\\|\\mathbf{g}\\|}\\right) \\mathbf{g}.$$**)\n",
    "\n",
    "通过这样做，我们知道梯度范数永远不会超过$\\theta$，\n",
    "并且更新后的梯度完全与$\\mathbf{g}$的原始方向对齐。\n",
    "它还有一个值得拥有的副作用，\n",
    "即限制任何给定的小批量数据（以及其中任何给定的样本）对参数向量的影响，\n",
    "这赋予了模型一定程度的稳定性。\n",
    "梯度裁剪提供了一个快速修复梯度爆炸的方法，\n",
    "虽然它并不能完全解决问题，但它是众多有效的技术之一。\n",
    "\n",
    "下面我们定义一个函数来裁剪模型的梯度，\n",
    "模型是从零开始实现的模型或由高级API构建的模型。\n",
    "我们在此计算了所有模型参数的梯度的范数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, gluon.Block):\n",
    "        params = [p.data() for p in net.collect_params().values()]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = math.sqrt(sum((p.grad ** 2).sum() for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3166af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507cb701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def grad_clipping(grads, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    theta = tf.constant(theta, dtype=tf.float32)\n",
    "    new_grad = []\n",
    "    for grad in grads:\n",
    "        if isinstance(grad, tf.IndexedSlices):\n",
    "            new_grad.append(tf.convert_to_tensor(grad))\n",
    "        else:\n",
    "            new_grad.append(grad)\n",
    "    norm = tf.math.sqrt(sum((tf.reduce_sum(grad ** 2)).numpy()\n",
    "                        for grad in new_grad))\n",
    "    norm = tf.cast(norm, tf.float32)\n",
    "    if tf.greater(norm, theta):\n",
    "        for i, grad in enumerate(new_grad):\n",
    "            new_grad[i] = grad * theta / norm\n",
    "    else:\n",
    "        new_grad = new_grad\n",
    "    return new_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d125cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Layer):\n",
    "        params = [p for p in net.parameters() if not p.stop_gradient]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = paddle.sqrt(sum(paddle.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        with paddle.no_grad():\n",
    "            for param in params:\n",
    "                param.grad.set_value(param.grad * theta / norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b65af1",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "在训练模型之前，让我们[**定义一个函数在一个迭代周期内训练模型**]。\n",
    "它与我们训练 :numref:`sec_softmax_scratch`模型的方式有三个不同之处。\n",
    "\n",
    "1. 序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。\n",
    "1. 我们在更新模型参数之前裁剪梯度。\n",
    "   这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。\n",
    "1. 我们用困惑度来评价模型。如 :numref:`subsec_perplexity`所述，\n",
    "   这样的度量确保了不同长度的序列具有可比性。\n",
    "\n",
    "具体来说，当使用顺序分区时，\n",
    "我们只在每个迭代周期的开始位置初始化隐状态。\n",
    "由于下一个小批量数据中的第$i$个子序列样本\n",
    "与当前第$i$个子序列样本相邻，\n",
    "因此当前小批量数据最后一个样本的隐状态，\n",
    "将用于初始化下一个小批量数据第一个样本的隐状态。\n",
    "这样，存储在隐状态中的序列的历史信息\n",
    "可以在一个迭代周期内流经相邻的子序列。\n",
    "然而，在任何一点隐状态的计算，\n",
    "都依赖于同一迭代周期中前面所有的小批量数据，\n",
    "这使得梯度计算变得复杂。\n",
    "为了降低计算量，在处理任何一个小批量数据之前，\n",
    "我们先分离梯度，使得隐状态的梯度计算总是限制在一个小批量数据的时间步内。\n",
    "\n",
    "当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，\n",
    "因此需要为每个迭代周期重新初始化隐状态。\n",
    "与 :numref:`sec_softmax_scratch`中的\n",
    "`train_epoch_ch3`函数相同，\n",
    "`updater`是更新模型参数的常用函数。\n",
    "它既可以是从头开始实现的`d2l.sgd`函数，\n",
    "也可以是深度学习框架中内置的优化函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练模型一个迭代周期（定义见第8章）\"\"\"\n",
    "    state, timer = None, d2l.Timer()\n",
    "    metric = d2l.Accumulator(2)  # 训练损失之和,词元数量\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], ctx=device)\n",
    "        else:\n",
    "            for s in state:\n",
    "                s.detach()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.as_in_ctx(device), y.as_in_ctx(device)\n",
    "        with autograd.record():\n",
    "            y_hat, state = net(X, state)\n",
    "            l = loss(y_hat, y).mean()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        updater(batch_size=1)  # 因为已经调用了mean函数\n",
    "        metric.add(l * d2l.size(y), d2l.size(y))\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期（定义见第8章）\"\"\"\n",
    "    state, timer = None, d2l.Timer()\n",
    "    metric = d2l.Accumulator(2)  # 训练损失之和,词元数量\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # state对于nn.GRU是个张量\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了mean函数\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * d2l.size(y), d2l.size(y))\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c9a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "#@save\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, use_random_iter):\n",
    "    \"\"\"训练模型一个迭代周期（定义见第8章）\"\"\"\n",
    "    state, timer = None, d2l.Timer()\n",
    "    metric = d2l.Accumulator(2)  # 训练损失之和,词元数量\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], dtype=tf.float32)\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            y_hat, state = net(X, state)\n",
    "            y = d2l.reshape(tf.transpose(Y), (-1))\n",
    "            l = loss(y, y_hat)\n",
    "        params = net.trainable_variables\n",
    "        grads = g.gradient(l, params)\n",
    "        grads = grad_clipping(grads, 1)\n",
    "        updater.apply_gradients(zip(grads, params))\n",
    "        # Keras默认返回一个批量中的平均损失\n",
    "        metric.add(l * d2l.size(y), d2l.size(y))\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1cbe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期（定义见第8章)\"\"\"\n",
    "    state, timer = None, d2l.Timer()\n",
    "    metric = d2l.Accumulator(2)  # 训练损失之和,词元数量\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0])\n",
    "        else:\n",
    "            if isinstance(net, nn.Layer) and not isinstance(state, tuple):\n",
    "                # state对于nn.GRU是个张量\n",
    "                state.stop_gradient=True\n",
    "            else:\n",
    "                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量\n",
    "                for s in state:\n",
    "                    s.stop_gradient=True\n",
    "        y = paddle.reshape(Y.T,shape=[-1])\n",
    "        X = paddle.to_tensor(X, place=device)\n",
    "        y = paddle.to_tensor(y, place=device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y).mean()\n",
    "        if isinstance(updater, paddle.optimizer.Optimizer):\n",
    "            updater.clear_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了mean函数\n",
    "            updater(batch_size=1)\n",
    "        \n",
    "        metric.add(l * d2l.size(y), d2l.size(y))\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e462cf",
   "metadata": {},
   "source": [
    "[**循环神经网络模型的训练函数既支持从零开始实现，\n",
    "也可以使用高级API来实现。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,  #@save\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    # 初始化\n",
    "    if isinstance(net, gluon.Block):\n",
    "        net.initialize(ctx=device, force_reinit=True,\n",
    "                         init=init.Normal(0.01))\n",
    "        trainer = gluon.Trainer(net.collect_params(),\n",
    "                                'sgd', {'learning_rate': lr})\n",
    "        updater = lambda batch_size: trainer.step(batch_size)\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb47f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    # 初始化\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "#@save\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, strategy,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    with strategy.scope():\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True)\n",
    "        updater = tf.keras.optimizers.SGD(lr)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater,\n",
    "                                     use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    device = d2l.try_gpu()._device_name\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    # 初始化\n",
    "    if isinstance(net, nn.Layer):\n",
    "        updater = paddle.optimizer.SGD(\n",
    "                learning_rate=lr, parameters=net.parameters())\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35decd17",
   "metadata": {},
   "source": [
    "[**现在，我们训练循环神经网络模型。**]\n",
    "因为我们在数据集中只使用了10000个词元，\n",
    "所以模型需要更多的迭代周期来更好地收敛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88187811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet,pytorch, paddle\n",
    "num_epochs, lr = 500, 1\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03649602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "num_epochs, lr = 500, 1\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c35345",
   "metadata": {},
   "source": [
    "[**最后，让我们检查一下使用随机抽样方法的结果。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7aaebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet,pytorch\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),\n",
    "          use_random_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7de670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "with strategy.scope():\n",
    "    net = RNNModelScratch(len(vocab), num_hiddens, init_rnn_state, rnn,\n",
    "                          get_params)\n",
    "train_ch8(net, train_iter, vocab_random_iter, lr, num_epochs, strategy,\n",
    "          use_random_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835cb07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),\n",
    "          use_random_iter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd8fc12",
   "metadata": {},
   "source": [
    "从零开始实现上述循环神经网络模型，\n",
    "虽然有指导意义，但是并不方便。\n",
    "在下一节中，我们将学习如何改进循环神经网络模型。\n",
    "例如，如何使其实现地更容易，且运行速度更快。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 我们可以训练一个基于循环神经网络的字符级语言模型，根据用户提供的文本的前缀生成后续文本。\n",
    "* 一个简单的循环神经网络语言模型包括输入编码、循环神经网络模型和输出生成。\n",
    "* 循环神经网络模型在训练以前需要初始化状态，不过随机抽样和顺序划分使用初始化方法不同。\n",
    "* 当使用顺序划分时，我们需要分离梯度以减少计算量。\n",
    "* 在进行任何预测之前，模型通过预热期进行自我更新（例如，获得比初始值更好的隐状态）。\n",
    "* 梯度裁剪可以防止梯度爆炸，但不能应对梯度消失。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 尝试说明独热编码等价于为每个对象选择不同的嵌入表示。\n",
    "1. 通过调整超参数（如迭代周期数、隐藏单元数、小批量数据的时间步数、学习率等）来改善困惑度。\n",
    "    * 困惑度可以降到多少？\n",
    "    * 用可学习的嵌入表示替换独热编码，是否会带来更好的表现？\n",
    "    * 如果用H.G.Wells的其他书作为数据集时效果如何，\n",
    "      例如[*世界大战*](http://www.gutenberg.org/ebooks/36)？\n",
    "1. 修改预测函数，例如使用采样，而不是选择最有可能的下一个字符。\n",
    "    * 会发生什么？\n",
    "    * 调整模型使之偏向更可能的输出，例如，当$\\alpha > 1$，从$q(x_t \\mid x_{t-1}, \\ldots, x_1) \\propto P(x_t \\mid x_{t-1}, \\ldots, x_1)^\\alpha$中采样。\n",
    "1. 在不裁剪梯度的情况下运行本节中的代码会发生什么？\n",
    "1. 更改顺序划分，使其不会从计算图中分离隐状态。运行时间会有变化吗？困惑度呢？\n",
    "1. 用ReLU替换本节中使用的激活函数，并重复本节中的实验。我们还需要梯度裁剪吗？为什么？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2102)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2103)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/2104)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11799)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7b97c9",
   "metadata": {},
   "source": [
    "# 循环神经网络的简洁实现\n",
    ":label:`sec_rnn-concise`\n",
    "\n",
    "虽然 :numref:`sec_rnn_scratch`\n",
    "对了解循环神经网络的实现方式具有指导意义，但并不方便。\n",
    "本节将展示如何使用深度学习框架的高级API提供的函数更有效地实现相同的语言模型。\n",
    "我们仍然从读取时光机器数据集开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f21210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn, rnn\n",
    "npx.set_np()\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b42a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ada6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3adc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "from paddle import nn\n",
    "from paddle.nn import functional as F\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e7a61",
   "metadata": {},
   "source": [
    "## [**定义模型**]\n",
    "\n",
    "高级API提供了循环神经网络的实现。\n",
    "我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层`rnn_layer`。\n",
    "事实上，我们还没有讨论多层循环神经网络的意义（这将在 :numref:`sec_deep_rnn`中介绍）。\n",
    "现在仅需要将多层理解为一层循环神经网络的输出被用作下一层循环神经网络的输入就足够了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95bc93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = rnn.RNN(num_hiddens)\n",
    "rnn_layer.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa5e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb82c9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "num_hiddens = 256\n",
    "rnn_cell = tf.keras.layers.SimpleRNNCell(num_hiddens,\n",
    "    kernel_initializer='glorot_uniform')\n",
    "rnn_layer = tf.keras.layers.RNN(rnn_cell, time_major=True,\n",
    "    return_sequences=True, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99becde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "num_hiddens = 256\n",
    "rnn_layer = nn.SimpleRNN(len(vocab), num_hiddens, time_major=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788563e",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "初始化隐状态是简单的，只需要调用成员函数`begin_state`即可。\n",
    "函数将返回一个列表（`state`），列表中包含了初始隐状态用于小批量数据中的每个样本，\n",
    "其形状为（隐藏层数，批量大小，隐藏单元数）。\n",
    "对于以后要介绍的一些模型（例如长-短期记忆网络），这样的列表还会包含其他信息。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "我们(**使用张量来初始化隐状态**)，它的形状是（隐藏层数，批量大小，隐藏单元数）。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d41167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = rnn_layer.begin_state(batch_size=batch_size)\n",
    "len(state), state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "state = rnn_cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bd753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "state = paddle.zeros(shape=[1, batch_size, num_hiddens])\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e107ee",
   "metadata": {},
   "source": [
    "[**通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。**]\n",
    "需要强调的是，`rnn_layer`的“输出”（`Y`）不涉及输出层的计算：\n",
    "它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "此外，`rnn_layer`返回的更新后的隐状态（`state_new`）\n",
    "是指小批量数据的最后时间步的隐状态。\n",
    "这个隐状态可以用来初始化顺序分区中一个迭代周期内下一个小批量数据的隐状态。\n",
    "对于多个隐藏层，每一层的隐状态将存储在（`state_new`）变量中。\n",
    "至于稍后要介绍的某些模型（例如，长－短期记忆），此变量还包含其他信息。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25898c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(size=(num_steps, batch_size, len(vocab)))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "Y.shape, len(state_new), state_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "Y.shape, state_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5749a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "X = tf.random.uniform((num_steps, batch_size, len(vocab)))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "Y.shape, len(state_new), state_new[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ac0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "X = paddle.rand(shape=[num_steps, batch_size, len(vocab)])\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "Y.shape, state_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fbb350",
   "metadata": {},
   "source": [
    "与 :numref:`sec_rnn_scratch`类似，\n",
    "[**我们为一个完整的循环神经网络模型定义了一个`RNNModel`类**]。\n",
    "注意，`rnn_layer`只包含隐藏的循环层，我们还需要创建一个单独的输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c2d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class RNNModel(nn.Block):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dense = nn.Dense(vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = npx.one_hot(inputs.T, self.vocab_size)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)\n",
    "        output = self.dense(Y.reshape(-1, Y.shape[-1]))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.begin_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)。\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens), \n",
    "                                device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b450db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "#@save\n",
    "class RNNModel(tf.keras.layers.Layer):\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        X = tf.one_hot(tf.transpose(inputs), self.vocab_size)\n",
    "        # rnn返回两个以上的值\n",
    "        Y, *state = self.rnn(X, state)\n",
    "        output = self.dense(tf.reshape(Y, (-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, *args, **kwargs):\n",
    "        return self.rnn.cell.get_initial_state(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b20242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "class RNNModel(nn.Layer):   \n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        if self.rnn.num_directions==1:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T, self.vocab_size) \n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)。\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  paddle.zeros(shape=[self.num_directions * self.rnn.num_layers,\n",
    "                                                           batch_size, self.num_hiddens])\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (paddle.zeros(\n",
    "                shape=[self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens]),\n",
    "                    paddle.zeros(\n",
    "                        shape=[self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5f2f5",
   "metadata": {},
   "source": [
    "## 训练与预测\n",
    "\n",
    "在训练模型之前，让我们[**基于一个具有随机权重的模型进行预测**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e46934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = d2l.try_gpu()\n",
    "net = RNNModel(rnn_layer, len(vocab))\n",
    "net.initialize(force_reinit=True, ctx=device)\n",
    "d2l.predict_ch8('time traveller', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc2e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "device = d2l.try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "net = net.to(device)\n",
    "d2l.predict_ch8('time traveller', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3413024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "device_name = d2l.try_gpu()._device_name\n",
    "strategy = tf.distribute.OneDeviceStrategy(device_name)\n",
    "with strategy.scope():\n",
    "    net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "\n",
    "d2l.predict_ch8('time traveller', 10, net, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34de3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "device = d2l.try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "d2l.predict_ch8('time traveller', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5956ba8",
   "metadata": {},
   "source": [
    "很明显，这种模型根本不能输出好的结果。\n",
    "接下来，我们使用 :numref:`sec_rnn_scratch`中\n",
    "定义的超参数调用`train_ch8`，并且[**使用高级API训练模型**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826d89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10bcc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "num_epochs, lr = 500, 1\n",
    "d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e191318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "num_epochs, lr = 500, 1\n",
    "d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719682c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "num_epochs, lr = 500, 1.0\n",
    "d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f6e29",
   "metadata": {},
   "source": [
    "与上一节相比，由于深度学习框架的高级API对代码进行了更多的优化，\n",
    "该模型在较短的时间内达到了较低的困惑度。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 深度学习框架的高级API提供了循环神经网络层的实现。\n",
    "* 高级API的循环神经网络层返回一个输出和一个更新后的隐状态，我们还需要计算整个模型的输出层。\n",
    "* 相比从零开始实现的循环神经网络，使用高级API实现可以加速训练。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 尝试使用高级API，能使循环神经网络模型过拟合吗？\n",
    "1. 如果在循环神经网络模型中增加隐藏层的数量会发生什么？能使模型正常工作吗？\n",
    "1. 尝试使用循环神经网络实现 :numref:`sec_sequence`的自回归模型。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2105)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2106)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/5766)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11800)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd4c668",
   "metadata": {},
   "source": [
    "# 通过时间反向传播\n",
    ":label:`sec_bptt`\n",
    "\n",
    "到目前为止，我们已经反复提到像*梯度爆炸*或*梯度消失*，\n",
    "以及需要对循环神经网络*分离梯度*。\n",
    "例如，在 :numref:`sec_rnn_scratch`中，\n",
    "我们在序列上调用了`detach`函数。\n",
    "为了能够快速构建模型并了解其工作原理，\n",
    "上面所说的这些概念都没有得到充分的解释。\n",
    "本节将更深入地探讨序列模型反向传播的细节，\n",
    "以及相关的数学原理。\n",
    "\n",
    "当我们首次实现循环神经网络（ :numref:`sec_rnn_scratch`）时，\n",
    "遇到了梯度爆炸的问题。\n",
    "如果做了练习题，就会发现梯度截断对于确保模型收敛至关重要。\n",
    "为了更好地理解此问题，本节将回顾序列模型梯度的计算方式，\n",
    "它的工作原理没有什么新概念，毕竟我们使用的仍然是链式法则来计算梯度。\n",
    "\n",
    "我们在 :numref:`sec_backprop`中描述了多层感知机中的\n",
    "前向与反向传播及相关的计算图。\n",
    "循环神经网络中的前向传播相对简单。\n",
    "*通过时间反向传播*（backpropagation through time，BPTT）\n",
    " :cite:`Werbos.1990`实际上是循环神经网络中反向传播技术的一个特定应用。\n",
    "它要求我们将循环神经网络的计算图一次展开一个时间步，\n",
    "以获得模型变量和参数之间的依赖关系。\n",
    "然后，基于链式法则，应用反向传播来计算和存储梯度。\n",
    "由于序列可能相当长，因此依赖关系也可能相当长。\n",
    "例如，某个1000个字符的序列，\n",
    "其第一个词元可能会对最后位置的词元产生重大影响。\n",
    "这在计算上是不可行的（它需要的时间和内存都太多了），\n",
    "并且还需要超过1000个矩阵的乘积才能得到非常难以捉摸的梯度。\n",
    "这个过程充满了计算与统计的不确定性。\n",
    "在下文中，我们将阐明会发生什么以及如何在实践中解决它们。\n",
    "\n",
    "## 循环神经网络的梯度分析\n",
    ":label:`subsec_bptt_analysis`\n",
    "\n",
    "我们从一个描述循环神经网络工作原理的简化模型开始，\n",
    "此模型忽略了隐状态的特性及其更新方式的细节。\n",
    "这里的数学表示没有像过去那样明确地区分标量、向量和矩阵，\n",
    "因为这些细节对于分析并不重要，\n",
    "反而只会使本小节中的符号变得混乱。\n",
    "\n",
    "在这个简化模型中，我们将时间步$t$的隐状态表示为$h_t$，\n",
    "输入表示为$x_t$，输出表示为$o_t$。\n",
    "回想一下我们在 :numref:`subsec_rnn_w_hidden_states`中的讨论，\n",
    "输入和隐状态可以拼接后与隐藏层中的一个权重变量相乘。\n",
    "因此，我们分别使用$w_h$和$w_o$来表示隐藏层和输出层的权重。\n",
    "每个时间步的隐状态和输出可以写为：\n",
    "\n",
    "$$\\begin{aligned}h_t &= f(x_t, h_{t-1}, w_h),\\\\o_t &= g(h_t, w_o),\\end{aligned}$$\n",
    ":eqlabel:`eq_bptt_ht_ot`\n",
    "\n",
    "其中$f$和$g$分别是隐藏层和输出层的变换。\n",
    "因此，我们有一个链\n",
    "$\\{\\ldots, (x_{t-1}, h_{t-1}, o_{t-1}), (x_{t}, h_{t}, o_t), \\ldots\\}$，\n",
    "它们通过循环计算彼此依赖。\n",
    "前向传播相当简单，一次一个时间步的遍历三元组$(x_t, h_t, o_t)$，\n",
    "然后通过一个目标函数在所有$T$个时间步内\n",
    "评估输出$o_t$和对应的标签$y_t$之间的差异：\n",
    "\n",
    "$$L(x_1, \\ldots, x_T, y_1, \\ldots, y_T, w_h, w_o) = \\frac{1}{T}\\sum_{t=1}^T l(y_t, o_t).$$\n",
    "\n",
    "对于反向传播，问题则有点棘手，\n",
    "特别是当我们计算目标函数$L$关于参数$w_h$的梯度时。\n",
    "具体来说，按照链式法则：\n",
    "\n",
    "$$\\begin{aligned}\\frac{\\partial L}{\\partial w_h}  & = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial w_h}  \\\\& = \\frac{1}{T}\\sum_{t=1}^T \\frac{\\partial l(y_t, o_t)}{\\partial o_t} \\frac{\\partial g(h_t, w_o)}{\\partial h_t}  \\frac{\\partial h_t}{\\partial w_h}.\\end{aligned}$$\n",
    ":eqlabel:`eq_bptt_partial_L_wh`\n",
    "\n",
    "在 :eqref:`eq_bptt_partial_L_wh`中乘积的第一项和第二项很容易计算，\n",
    "而第三项$\\partial h_t/\\partial w_h$是使事情变得棘手的地方，\n",
    "因为我们需要循环地计算参数$w_h$对$h_t$的影响。\n",
    "根据 :eqref:`eq_bptt_ht_ot`中的递归计算，\n",
    "$h_t$既依赖于$h_{t-1}$又依赖于$w_h$，\n",
    "其中$h_{t-1}$的计算也依赖于$w_h$。\n",
    "因此，使用链式法则产生：\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial w_h}= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h} +\\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_h}.$$\n",
    ":eqlabel:`eq_bptt_partial_ht_wh_recur`\n",
    "\n",
    "为了导出上述梯度，假设我们有三个序列$\\{a_{t}\\},\\{b_{t}\\},\\{c_{t}\\}$，\n",
    "当$t=1,2,\\ldots$时，序列满足$a_{0}=0$且$a_{t}=b_{t}+c_{t}a_{t-1}$。\n",
    "对于$t\\geq 1$，就很容易得出：\n",
    "\n",
    "$$a_{t}=b_{t}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t}c_{j}\\right)b_{i}.$$\n",
    ":eqlabel:`eq_bptt_at`\n",
    "\n",
    "基于下列公式替换$a_t$、$b_t$和$c_t$：\n",
    "\n",
    "$$\\begin{aligned}a_t &= \\frac{\\partial h_t}{\\partial w_h},\\\\\n",
    "b_t &= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h}, \\\\\n",
    "c_t &= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}},\\end{aligned}$$\n",
    "\n",
    "公式 :eqref:`eq_bptt_partial_ht_wh_recur`中的梯度计算\n",
    "满足$a_{t}=b_{t}+c_{t}a_{t-1}$。\n",
    "因此，对于每个 :eqref:`eq_bptt_at`，\n",
    "我们可以使用下面的公式移除 :eqref:`eq_bptt_partial_ht_wh_recur`中的循环计算\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial w_h}=\\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h}+\\sum_{i=1}^{t-1}\\left(\\prod_{j=i+1}^{t} \\frac{\\partial f(x_{j},h_{j-1},w_h)}{\\partial h_{j-1}} \\right) \\frac{\\partial f(x_{i},h_{i-1},w_h)}{\\partial w_h}.$$\n",
    ":eqlabel:`eq_bptt_partial_ht_wh_gen`\n",
    "\n",
    "虽然我们可以使用链式法则递归地计算$\\partial h_t/\\partial w_h$，\n",
    "但当$t$很大时这个链就会变得很长。\n",
    "我们需要想想办法来处理这一问题.\n",
    "\n",
    "### 完全计算 ###\n",
    "\n",
    "显然，我们可以仅仅计算 :eqref:`eq_bptt_partial_ht_wh_gen`中的全部总和，\n",
    "然而，这样的计算非常缓慢，并且可能会发生梯度爆炸，\n",
    "因为初始条件的微小变化就可能会对结果产生巨大的影响。\n",
    "也就是说，我们可以观察到类似于蝴蝶效应的现象，\n",
    "即初始条件的很小变化就会导致结果发生不成比例的变化。\n",
    "这对于我们想要估计的模型而言是非常不可取的。\n",
    "毕竟，我们正在寻找的是能够很好地泛化高稳定性模型的估计器。\n",
    "因此，在实践中，这种方法几乎从未使用过。\n",
    "\n",
    "### 截断时间步 ###\n",
    "\n",
    "或者，我们可以在$\\tau$步后截断\n",
    " :eqref:`eq_bptt_partial_ht_wh_gen`中的求和计算。\n",
    "这是我们到目前为止一直在讨论的内容，\n",
    "例如在 :numref:`sec_rnn_scratch`中分离梯度时。\n",
    "这会带来真实梯度的*近似*，\n",
    "只需将求和终止为$\\partial h_{t-\\tau}/\\partial w_h$。\n",
    "在实践中，这种方式工作得很好。\n",
    "它通常被称为截断的通过时间反向传播 :cite:`Jaeger.2002`。\n",
    "这样做导致该模型主要侧重于短期影响，而不是长期影响。\n",
    "这在现实中是可取的，因为它会将估计值偏向更简单和更稳定的模型。\n",
    "\n",
    "### 随机截断 ###\n",
    "\n",
    "最后，我们可以用一个随机变量替换$\\partial h_t/\\partial w_h$，\n",
    "该随机变量在预期中是正确的，但是会截断序列。\n",
    "这个随机变量是通过使用序列$\\xi_t$来实现的，\n",
    "序列预定义了$0 \\leq \\pi_t \\leq 1$，\n",
    "其中$P(\\xi_t = 0) = 1-\\pi_t$且$P(\\xi_t = \\pi_t^{-1}) = \\pi_t$，\n",
    "因此$E[\\xi_t] = 1$。\n",
    "我们使用它来替换 :eqref:`eq_bptt_partial_ht_wh_recur`中的\n",
    "梯度$\\partial h_t/\\partial w_h$得到：\n",
    "\n",
    "$$z_t= \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial w_h} +\\xi_t \\frac{\\partial f(x_{t},h_{t-1},w_h)}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial w_h}.$$\n",
    "\n",
    "从$\\xi_t$的定义中推导出来$E[z_t] = \\partial h_t/\\partial w_h$。\n",
    "每当$\\xi_t = 0$时，递归计算终止在这个$t$时间步。\n",
    "这导致了不同长度序列的加权和，其中长序列出现的很少，\n",
    "所以将适当地加大权重。\n",
    "这个想法是由塔莱克和奥利维尔 :cite:`Tallec.Ollivier.2017`提出的。\n",
    "\n",
    "### 比较策略\n",
    "\n",
    "![比较RNN中计算梯度的策略，3行自上而下分别为：随机截断、常规截断、完整计算](../img/truncated-bptt.svg)\n",
    ":label:`fig_truncated_bptt`\n",
    "\n",
    " :numref:`fig_truncated_bptt`说明了\n",
    "当基于循环神经网络使用通过时间反向传播\n",
    "分析《时间机器》书中前几个字符的三种策略：\n",
    "\n",
    "* 第一行采用随机截断，方法是将文本划分为不同长度的片断；\n",
    "* 第二行采用常规截断，方法是将文本分解为相同长度的子序列。\n",
    "  这也是我们在循环神经网络实验中一直在做的；\n",
    "* 第三行采用通过时间的完全反向传播，结果是产生了在计算上不可行的表达式。\n",
    "\n",
    "遗憾的是，虽然随机截断在理论上具有吸引力，\n",
    "但很可能是由于多种因素在实践中并不比常规截断更好。\n",
    "首先，在对过去若干个时间步经过反向传播后，\n",
    "观测结果足以捕获实际的依赖关系。\n",
    "其次，增加的方差抵消了时间步数越多梯度越精确的事实。\n",
    "第三，我们真正想要的是只有短范围交互的模型。\n",
    "因此，模型需要的正是截断的通过时间反向传播方法所具备的轻度正则化效果。\n",
    "\n",
    "## 通过时间反向传播的细节\n",
    "\n",
    "在讨论一般性原则之后，我们看一下通过时间反向传播问题的细节。\n",
    "与 :numref:`subsec_bptt_analysis`中的分析不同，\n",
    "下面我们将展示如何计算目标函数相对于所有分解模型参数的梯度。\n",
    "为了保持简单，我们考虑一个没有偏置参数的循环神经网络，\n",
    "其在隐藏层中的激活函数使用恒等映射（$\\phi(x)=x$）。\n",
    "对于时间步$t$，设单个样本的输入及其对应的标签分别为\n",
    "$\\mathbf{x}_t \\in \\mathbb{R}^d$和$y_t$。\n",
    "计算隐状态$\\mathbf{h}_t \\in \\mathbb{R}^h$和\n",
    "输出$\\mathbf{o}_t \\in \\mathbb{R}^q$的方式为：\n",
    "\n",
    "$$\\begin{aligned}\\mathbf{h}_t &= \\mathbf{W}_{hx} \\mathbf{x}_t + \\mathbf{W}_{hh} \\mathbf{h}_{t-1},\\\\\n",
    "\\mathbf{o}_t &= \\mathbf{W}_{qh} \\mathbf{h}_{t},\\end{aligned}$$\n",
    "\n",
    "其中权重参数为$\\mathbf{W}_{hx} \\in \\mathbb{R}^{h \\times d}$、\n",
    "$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$和\n",
    "$\\mathbf{W}_{qh} \\in \\mathbb{R}^{q \\times h}$。\n",
    "用$l(\\mathbf{o}_t, y_t)$表示时间步$t$处\n",
    "（即从序列开始起的超过$T$个时间步）的损失函数，\n",
    "则我们的目标函数的总体损失是：\n",
    "\n",
    "$$L = \\frac{1}{T} \\sum_{t=1}^T l(\\mathbf{o}_t, y_t).$$\n",
    "\n",
    "为了在循环神经网络的计算过程中可视化模型变量和参数之间的依赖关系，\n",
    "我们可以为模型绘制一个计算图，\n",
    "如 :numref:`fig_rnn_bptt`所示。\n",
    "例如，时间步3的隐状态$\\mathbf{h}_3$的计算\n",
    "取决于模型参数$\\mathbf{W}_{hx}$和$\\mathbf{W}_{hh}$，\n",
    "以及最终时间步的隐状态$\\mathbf{h}_2$\n",
    "以及当前时间步的输入$\\mathbf{x}_3$。\n",
    "\n",
    "![上图表示具有三个时间步的循环神经网络模型依赖关系的计算图。未着色的方框表示变量，着色的方框表示参数，圆表示运算符](../img/rnn-bptt.svg)\n",
    ":label:`fig_rnn_bptt`\n",
    "\n",
    "正如刚才所说， :numref:`fig_rnn_bptt`中的模型参数是\n",
    "$\\mathbf{W}_{hx}$、$\\mathbf{W}_{hh}$和$\\mathbf{W}_{qh}$。\n",
    "通常，训练该模型需要对这些参数进行梯度计算：\n",
    "$\\partial L/\\partial \\mathbf{W}_{hx}$、\n",
    "$\\partial L/\\partial \\mathbf{W}_{hh}$和\n",
    "$\\partial L/\\partial \\mathbf{W}_{qh}$。\n",
    "根据 :numref:`fig_rnn_bptt`中的依赖关系，\n",
    "我们可以沿箭头的相反方向遍历计算图，依次计算和存储梯度。\n",
    "为了灵活地表示链式法则中不同形状的矩阵、向量和标量的乘法，\n",
    "我们继续使用如 :numref:`sec_backprop`中\n",
    "所述的$\\text{prod}$运算符。\n",
    "\n",
    "首先，在任意时间步$t$，\n",
    "目标函数关于模型输出的微分计算是相当简单的：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{o}_t} =  \\frac{\\partial l (\\mathbf{o}_t, y_t)}{T \\cdot \\partial \\mathbf{o}_t} \\in \\mathbb{R}^q.$$\n",
    ":eqlabel:`eq_bptt_partial_L_ot`\n",
    "\n",
    "现在，我们可以计算目标函数关于输出层中参数$\\mathbf{W}_{qh}$的梯度：\n",
    "$\\partial L/\\partial \\mathbf{W}_{qh} \\in \\mathbb{R}^{q \\times h}$。\n",
    "基于 :numref:`fig_rnn_bptt`，\n",
    "目标函数$L$通过$\\mathbf{o}_1, \\ldots, \\mathbf{o}_T$\n",
    "依赖于$\\mathbf{W}_{qh}$。\n",
    "依据链式法则，得到\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_{qh}}\n",
    "= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{W}_{qh}}\\right)\n",
    "= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{o}_t} \\mathbf{h}_t^\\top,\n",
    "$$\n",
    "\n",
    "其中$\\partial L/\\partial \\mathbf{o}_t$是\n",
    "由 :eqref:`eq_bptt_partial_L_ot`给出的。\n",
    "\n",
    "接下来，如 :numref:`fig_rnn_bptt`所示，\n",
    "在最后的时间步$T$，目标函数$L$仅通过$\\mathbf{o}_T$\n",
    "依赖于隐状态$\\mathbf{h}_T$。\n",
    "因此，我们通过使用链式法可以很容易地得到梯度\n",
    "$\\partial L/\\partial \\mathbf{h}_T \\in \\mathbb{R}^h$：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{h}_T} = \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_T}, \\frac{\\partial \\mathbf{o}_T}{\\partial \\mathbf{h}_T} \\right) = \\mathbf{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_T}.$$\n",
    ":eqlabel:`eq_bptt_partial_L_hT_final_step`\n",
    "\n",
    "当目标函数$L$通过$\\mathbf{h}_{t+1}$和$\\mathbf{o}_t$\n",
    "依赖$\\mathbf{h}_t$时，\n",
    "对任意时间步$t < T$来说都变得更加棘手。\n",
    "根据链式法则，隐状态的梯度\n",
    "$\\partial L/\\partial \\mathbf{h}_t \\in \\mathbb{R}^h$\n",
    "在任何时间步骤$t < T$时都可以递归地计算为：\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{h}_t} = \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}}, \\frac{\\partial \\mathbf{h}_{t+1}}{\\partial \\mathbf{h}_t} \\right) + \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{o}_t}, \\frac{\\partial \\mathbf{o}_t}{\\partial \\mathbf{h}_t} \\right) = \\mathbf{W}_{hh}^\\top \\frac{\\partial L}{\\partial \\mathbf{h}_{t+1}} + \\mathbf{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_t}.$$\n",
    ":eqlabel:`eq_bptt_partial_L_ht_recur`\n",
    "\n",
    "为了进行分析，对于任何时间步$1 \\leq t \\leq T$展开递归计算得\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{h}_t}= \\sum_{i=t}^T {\\left(\\mathbf{W}_{hh}^\\top\\right)}^{T-i} \\mathbf{W}_{qh}^\\top \\frac{\\partial L}{\\partial \\mathbf{o}_{T+t-i}}.$$\n",
    ":eqlabel:`eq_bptt_partial_L_ht`\n",
    "\n",
    "我们可以从 :eqref:`eq_bptt_partial_L_ht`中看到，\n",
    "这个简单的线性例子已经展现了长序列模型的一些关键问题：\n",
    "它陷入到$\\mathbf{W}_{hh}^\\top$的潜在的非常大的幂。\n",
    "在这个幂中，小于1的特征值将会消失，大于1的特征值将会发散。\n",
    "这在数值上是不稳定的，表现形式为梯度消失或梯度爆炸。\n",
    "解决此问题的一种方法是按照计算方便的需要截断时间步长的尺寸\n",
    "如 :numref:`subsec_bptt_analysis`中所述。\n",
    "实际上，这种截断是通过在给定数量的时间步之后分离梯度来实现的。\n",
    "稍后，我们将学习更复杂的序列模型（如长短期记忆模型）\n",
    "是如何进一步缓解这一问题的。\n",
    "\n",
    "最后， :numref:`fig_rnn_bptt`表明：\n",
    "目标函数$L$通过隐状态$\\mathbf{h}_1, \\ldots, \\mathbf{h}_T$\n",
    "依赖于隐藏层中的模型参数$\\mathbf{W}_{hx}$和$\\mathbf{W}_{hh}$。\n",
    "为了计算有关这些参数的梯度\n",
    "$\\partial L / \\partial \\mathbf{W}_{hx} \\in \\mathbb{R}^{h \\times d}$和$\\partial L / \\partial \\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$，\n",
    "我们应用链式规则得：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_{hx}}\n",
    "&= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_{hx}}\\right)\n",
    "= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{x}_t^\\top,\\\\\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_{hh}}\n",
    "&= \\sum_{t=1}^T \\text{prod}\\left(\\frac{\\partial L}{\\partial \\mathbf{h}_t}, \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{W}_{hh}}\\right)\n",
    "= \\sum_{t=1}^T \\frac{\\partial L}{\\partial \\mathbf{h}_t} \\mathbf{h}_{t-1}^\\top,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中$\\partial L/\\partial \\mathbf{h}_t$\n",
    "是由 :eqref:`eq_bptt_partial_L_hT_final_step`和\n",
    " :eqref:`eq_bptt_partial_L_ht_recur`递归计算得到的，\n",
    "是影响数值稳定性的关键量。\n",
    "\n",
    "正如我们在 :numref:`sec_backprop`中所解释的那样，\n",
    "由于通过时间反向传播是反向传播在循环神经网络中的应用方式，\n",
    "所以训练循环神经网络交替使用前向传播和通过时间反向传播。\n",
    "通过时间反向传播依次计算并存储上述梯度。\n",
    "具体而言，存储的中间值会被重复使用，以避免重复计算，\n",
    "例如存储$\\partial L/\\partial \\mathbf{h}_t$，\n",
    "以便在计算$\\partial L / \\partial \\mathbf{W}_{hx}$和\n",
    "$\\partial L / \\partial \\mathbf{W}_{hh}$时使用。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* “通过时间反向传播”仅仅适用于反向传播在具有隐状态的序列模型。\n",
    "* 截断是计算方便性和数值稳定性的需要。截断包括：规则截断和随机截断。\n",
    "* 矩阵的高次幂可能导致神经网络特征值的发散或消失，将以梯度爆炸或梯度消失的形式表现。\n",
    "* 为了计算的效率，“通过时间反向传播”在计算期间会缓存中间值。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 假设我们拥有一个对称矩阵$\\mathbf{M} \\in \\mathbb{R}^{n \\times n}$，其特征值为$\\lambda_i$，对应的特征向量是$\\mathbf{v}_i$（$i = 1, \\ldots, n$）。通常情况下，假设特征值的序列顺序为$|\\lambda_i| \\geq |\\lambda_{i+1}|$。\n",
    "   1. 证明$\\mathbf{M}^k$拥有特征值$\\lambda_i^k$。\n",
    "   1. 证明对于一个随机向量$\\mathbf{x} \\in \\mathbb{R}^n$，$\\mathbf{M}^k \\mathbf{x}$将有较高概率与$\\mathbf{M}$的特征向量$\\mathbf{v}_1$在一条直线上。形式化这个证明过程。\n",
    "   1. 上述结果对于循环神经网络中的梯度意味着什么？\n",
    "1. 除了梯度截断，还有其他方法来应对循环神经网络中的梯度爆炸吗？\n",
    "\n",
    "[Discussions](https://discuss.d2l.ai/t/2107)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

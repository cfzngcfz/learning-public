{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e344e507",
   "metadata": {},
   "source": [
    "# 计算机视觉\n",
    ":label:`chap_cv`\n",
    "\n",
    "近年来，深度学习一直是提高计算机视觉系统性能的变革力量。\n",
    "无论是医疗诊断、自动驾驶，还是智能滤波器、摄像头监控，许多计算机视觉领域的应用都与我们当前和未来的生活密切相关。\n",
    "可以说，最先进的计算机视觉应用与深度学习几乎是不可分割的。\n",
    "有鉴于此，本章将重点介绍计算机视觉领域，并探讨最近在学术界和行业中具有影响力的方法和应用。\n",
    "\n",
    "在 :numref:`chap_cnn`和 :numref:`chap_modern_cnn`中，我们研究了计算机视觉中常用的各种卷积神经网络，并将它们应用到简单的图像分类任务中。\n",
    "本章开头，我们将介绍两种可以改进模型泛化的方法，即*图像增广*和*微调*，并将它们应用于图像分类。\n",
    "由于深度神经网络可以有效地表示多个层次的图像，因此这种分层表示已成功用于各种计算机视觉任务，例如*目标检测*（object detection）、*语义分割*（semantic segmentation）和*样式迁移*（style transfer）。\n",
    "秉承计算机视觉中利用分层表示的关键思想，我们将从物体检测的主要组件和技术开始，继而展示如何使用*完全卷积网络*对图像进行语义分割，然后我们将解释如何使用样式迁移技术来生成像本书封面一样的图像。\n",
    "最后在结束本章时，我们将本章和前几章的知识应用于两个流行的计算机视觉基准数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc3cb96",
   "metadata": {
    "attributes": {
     "classes": [
      "toc"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    ":maxdepth: 2\n",
    "\n",
    "image-augmentation\n",
    "fine-tuning\n",
    "bounding-box\n",
    "anchor\n",
    "multiscale-object-detection\n",
    "object-detection-dataset\n",
    "ssd\n",
    "rcnn\n",
    "semantic-segmentation-and-dataset\n",
    "transposed-conv\n",
    "fcn\n",
    "neural-style\n",
    "kaggle-cifar10\n",
    "kaggle-dog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d47cdba",
   "metadata": {},
   "source": [
    "# 图像增广\n",
    ":label:`sec_image_augmentation`\n",
    "\n",
    " :numref:`sec_alexnet`提到过大型数据集是成功应用深度神经网络的先决条件。\n",
    "图像增广在对训练图像进行一系列的随机变化之后，生成相似但不同的训练样本，从而扩大了训练集的规模。\n",
    "此外，应用图像增广的原因是，随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力。\n",
    "例如，我们可以以不同的方式裁剪图像，使感兴趣的对象出现在不同的位置，减少模型对于对象出现位置的依赖。\n",
    "我们还可以调整亮度、颜色等因素来降低模型对颜色的敏感度。\n",
    "可以说，图像增广技术对于AlexNet的成功是必不可少的。本节将讨论这项广泛应用于计算机视觉的技术。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda071d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, image, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "import paddle.vision as paddlevision\n",
    "from paddle import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded7330",
   "metadata": {},
   "source": [
    "## 常用的图像增广方法\n",
    "\n",
    "在对常用图像增广方法的探索时，我们将使用下面这个尺寸为$400\\times 500$的图像作为示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37aac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "img = image.imread('../img/cat1.jpg')\n",
    "d2l.plt.imshow(img.asnumpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c98cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "d2l.set_figsize()\n",
    "img = d2l.Image.open('../img/cat1.jpg')\n",
    "d2l.plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f20de3",
   "metadata": {},
   "source": [
    "大多数图像增广方法都具有一定的随机性。为了便于观察图像增广的效果，我们下面定义辅助函数`apply`。\n",
    "此函数在输入图像`img`上多次运行图像增广方法`aug`并显示所有结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0abf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):\n",
    "    Y = [aug(img) for _ in range(num_rows * num_cols)]\n",
    "    d2l.show_images(Y, num_rows, num_cols, scale=scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cedba2c",
   "metadata": {},
   "source": [
    "### 翻转和裁剪\n",
    "\n",
    "[**左右翻转图像**]通常不会改变对象的类别。这是最早且最广泛使用的图像增广方法之一。\n",
    "接下来，我们使用`transforms`模块来创建`RandomFlipLeftRight`实例，这样就各有50%的几率使图像向左或向右翻转。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807ae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(img, gluon.data.vision.transforms.RandomFlipLeftRight())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "apply(img, torchvision.transforms.RandomHorizontalFlip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e72e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "apply(img, paddlevision.transforms.RandomHorizontalFlip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b7648",
   "metadata": {},
   "source": [
    "[**上下翻转图像**]不如左右图像翻转那样常用。但是，至少对于这个示例图像，上下翻转不会妨碍识别。接下来，我们创建一个`RandomFlipTopBottom`实例，使图像各有50%的几率向上或向下翻转。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(img, gluon.data.vision.transforms.RandomFlipTopBottom())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cab223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "apply(img, torchvision.transforms.RandomVerticalFlip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "apply(img,  paddlevision.transforms.RandomVerticalFlip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552998d",
   "metadata": {},
   "source": [
    "在我们使用的示例图像中，猫位于图像的中间，但并非所有图像都是这样。\n",
    "在 :numref:`sec_pooling`中，我们解释了汇聚层可以降低卷积层对目标位置的敏感性。\n",
    "另外，我们可以通过对图像进行随机裁剪，使物体以不同的比例出现在图像的不同位置。\n",
    "这也可以降低模型对目标位置的敏感性。\n",
    "\n",
    "下面的代码将[**随机裁剪**]一个面积为原始面积10%到100%的区域，该区域的宽高比从0.5～2之间随机取值。\n",
    "然后，区域的宽度和高度都被缩放到200像素。\n",
    "在本节中（除非另有说明），$a$和$b$之间的随机数指的是在区间$[a, b]$中通过均匀采样获得的连续值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_aug = gluon.data.vision.transforms.RandomResizedCrop(\n",
    "    (200, 200), scale=(0.1, 1), ratio=(0.5, 2))\n",
    "apply(img, shape_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e594f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "shape_aug = torchvision.transforms.RandomResizedCrop(\n",
    "    (200, 200), scale=(0.1, 1), ratio=(0.5, 2))\n",
    "apply(img, shape_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "shape_aug =  paddlevision.transforms.RandomResizedCrop(\n",
    "    (200, 200), scale=(0.1, 1), ratio=(0.5, 2))\n",
    "apply(img, shape_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda8bd3c",
   "metadata": {},
   "source": [
    "### 改变颜色\n",
    "\n",
    "另一种增广方法是改变颜色。\n",
    "我们可以改变图像颜色的四个方面：亮度、对比度、饱和度和色调。\n",
    "在下面的示例中，我们[**随机更改图像的亮度**]，随机值为原始图像的50%（$1-0.5$）到150%（$1+0.5$）之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e07555",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(img, gluon.data.vision.transforms.RandomBrightness(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "apply(img, torchvision.transforms.ColorJitter(\n",
    "    brightness=0.5, contrast=0, saturation=0, hue=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4425ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "apply(img,  paddlevision.transforms.ColorJitter(\n",
    "    brightness=0.5, contrast=0, saturation=0, hue=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e35a34",
   "metadata": {},
   "source": [
    "同样，我们可以[**随机更改图像的色调**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886aeda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(img, gluon.data.vision.transforms.RandomHue(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "apply(img, torchvision.transforms.ColorJitter(\n",
    "    brightness=0, contrast=0, saturation=0, hue=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8435d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "apply(img,  paddlevision.transforms.ColorJitter(\n",
    "    brightness=0, contrast=0, saturation=0, hue=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a79b34",
   "metadata": {},
   "source": [
    "我们还可以创建一个`RandomColorJitter`实例，并设置如何同时[**随机更改图像的亮度（`brightness`）、对比度（`contrast`）、饱和度（`saturation`）和色调（`hue`）**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_aug = gluon.data.vision.transforms.RandomColorJitter(\n",
    "    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\n",
    "apply(img, color_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2d1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "color_aug = torchvision.transforms.ColorJitter(\n",
    "    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\n",
    "apply(img, color_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "color_aug =  paddlevision.transforms.ColorJitter(\n",
    "    brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5)\n",
    "apply(img, color_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b486fad",
   "metadata": {},
   "source": [
    "### [**结合多种图像增广方法**]\n",
    "\n",
    "在实践中，我们将结合多种图像增广方法。比如，我们可以通过使用一个`Compose`实例来综合上面定义的不同的图像增广方法，并将它们应用到每个图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7198ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "augs = gluon.data.vision.transforms.Compose([\n",
    "    gluon.data.vision.transforms.RandomFlipLeftRight(), color_aug, shape_aug])\n",
    "apply(img, augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58182f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(), color_aug, shape_aug])\n",
    "apply(img, augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "augs =  paddlevision.transforms.Compose([\n",
    "     paddle.vision.transforms.RandomHorizontalFlip(), color_aug, shape_aug])\n",
    "apply(img, augs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053f958",
   "metadata": {},
   "source": [
    "## [**使用图像增广进行训练**]\n",
    "\n",
    "让我们使用图像增广来训练模型。\n",
    "这里，我们使用CIFAR-10数据集，而不是我们之前使用的Fashion-MNIST数据集。\n",
    "这是因为Fashion-MNIST数据集中对象的位置和大小已被规范化，而CIFAR-10数据集中对象的颜色和大小差异更明显。\n",
    "CIFAR-10数据集中的前32个训练图像如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36981134",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.show_images(gluon.data.vision.CIFAR10(\n",
    "    train=True)[0:32][0], 4, 8, scale=0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f19855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "all_images = torchvision.datasets.CIFAR10(train=True, root=\"../data\",\n",
    "                                          download=True)\n",
    "d2l.show_images([all_images[i][0] for i in range(32)], 4, 8, scale=0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3095a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "all_images =  paddlevision.datasets.Cifar10(mode='train' , download=True)\n",
    "print(len(all_images))\n",
    "d2l.show_images([all_images[i][0] for i in range(32)], 4, 8, scale=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614af336",
   "metadata": {},
   "source": [
    "为了在预测过程中得到确切的结果，我们通常对训练样本只进行图像增广，且在预测过程中不使用随机操作的图像增广。\n",
    "在这里，我们[**只使用最简单的随机左右翻转**]。\n",
    "此外，我们使用`ToTensor`实例将一批图像转换为深度学习框架所要求的格式，即形状为（批量大小，通道数，高度，宽度）的32位浮点数，取值范围为0～1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c82a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augs = gluon.data.vision.transforms.Compose([\n",
    "    gluon.data.vision.transforms.RandomFlipLeftRight(),\n",
    "    gluon.data.vision.transforms.ToTensor()])\n",
    "\n",
    "test_augs = gluon.data.vision.transforms.Compose([\n",
    "    gluon.data.vision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c3d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "train_augs = torchvision.transforms.Compose([\n",
    "     torchvision.transforms.RandomHorizontalFlip(),\n",
    "     torchvision.transforms.ToTensor()])\n",
    "\n",
    "test_augs = torchvision.transforms.Compose([\n",
    "     torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "train_augs = paddlevision.transforms.Compose([\n",
    "     paddlevision.transforms.RandomHorizontalFlip(),\n",
    "     paddlevision.transforms.ToTensor()])\n",
    "\n",
    "test_augs = paddlevision.transforms.Compose([\n",
    "     paddlevision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba66bc3",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "接下来，我们定义了一个辅助函数，以便于读取图像和应用图像增广。Gluon数据集提供的`transform_first`函数将图像增广应用于每个训练样本的第一个元素（由图像和标签组成），即应用在图像上。有关`DataLoader`的详细介绍，请参阅 :numref:`sec_fashion_mnist`。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "接下来，我们[**定义一个辅助函数，以便于读取图像和应用图像增广**]。PyTorch数据集提供的`transform`参数应用图像增广来转化图像。有关`DataLoader`的详细介绍，请参阅 :numref:`sec_fashion_mnist`。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10(is_train, augs, batch_size):\n",
    "    return gluon.data.DataLoader(\n",
    "        gluon.data.vision.CIFAR10(train=is_train).transform_first(augs),\n",
    "        batch_size=batch_size, shuffle=is_train,\n",
    "        num_workers=d2l.get_dataloader_workers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def load_cifar10(is_train, augs, batch_size):\n",
    "    dataset = torchvision.datasets.CIFAR10(root=\"../data\", train=is_train,\n",
    "                                           transform=augs, download=True)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                    shuffle=is_train, num_workers=d2l.get_dataloader_workers())\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e20a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def load_cifar10(is_train, augs, batch_size):\n",
    "    dataset = paddlevision.datasets.Cifar10(mode=\"train\", \n",
    "                                            transform=augs, download=True)\n",
    "    dataloader = paddle.io.DataLoader(dataset, batch_size=batch_size, \n",
    "                    num_workers=d2l.get_dataloader_workers(), shuffle=is_train)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c7a42",
   "metadata": {},
   "source": [
    "### 多GPU训练\n",
    "\n",
    "我们在CIFAR-10数据集上训练 :numref:`sec_resnet`中的ResNet-18模型。\n",
    "回想一下 :numref:`sec_multi_gpu_concise`中对多GPU训练的介绍。\n",
    "接下来，我们[**定义一个函数，使用多GPU对模型进行训练和评估**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_batch_ch13(net, features, labels, loss, trainer, devices,\n",
    "                     split_f=d2l.split_batch):\n",
    "    \"\"\"用多GPU进行小批量训练\"\"\"\n",
    "    X_shards, y_shards = split_f(features, labels, devices)\n",
    "    with autograd.record():\n",
    "        pred_shards = [net(X_shard) for X_shard in X_shards]\n",
    "        ls = [loss(pred_shard, y_shard) for pred_shard, y_shard\n",
    "              in zip(pred_shards, y_shards)]\n",
    "    for l in ls:\n",
    "        l.backward()\n",
    "    # True标志允许使用过时的梯度，这很有用（例如，在微调BERT中）\n",
    "    trainer.step(labels.shape[0], ignore_stale_grad=True)\n",
    "    train_loss_sum = sum([float(l.sum()) for l in ls])\n",
    "    train_acc_sum = sum(d2l.accuracy(pred_shard, y_shard)\n",
    "                        for pred_shard, y_shard in zip(pred_shards, y_shards))\n",
    "    return train_loss_sum, train_acc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5875541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def train_batch_ch13(net, X, y, loss, trainer, devices):\n",
    "    \"\"\"用多GPU进行小批量训练\"\"\"\n",
    "    if isinstance(X, list):\n",
    "        # 微调BERT中所需\n",
    "        X = [x.to(devices[0]) for x in X]\n",
    "    else:\n",
    "        X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cfc5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def train_batch_ch13(net, X, y, loss, trainer, devices):\n",
    "    \"\"\"用多GPU进行小批量训练\n",
    "    飞桨不支持在notebook上进行多GPU训练\n",
    "    Defined in :numref:`sec_image_augmentation`\"\"\"\n",
    "    if isinstance(X, list):\n",
    "        # 微调BERT中所需（稍后讨论）\n",
    "        X = [paddle.to_tensor(x, place=devices[0]) for x in X]\n",
    "    else:\n",
    "        X = paddle.to_tensor(X, place=devices[0])\n",
    "    y = paddle.to_tensor(y, place=devices[0])\n",
    "    net.train()\n",
    "    trainer.clear_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = d2l.accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420da171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus(), split_f=d2l.split_batch):\n",
    "    \"\"\"用多GPU进行模型训练\"\"\"\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        # 4个维度：储存训练损失，训练准确度，实例数，特点数\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch_ch13(\n",
    "                net, features, labels, loss, trainer, devices, split_f)\n",
    "            metric.add(l, acc, labels.shape[0], labels.size)\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[3],\n",
    "                              None))\n",
    "        test_acc = d2l.evaluate_accuracy_gpus(net, test_iter, split_f)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
    "          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus()):\n",
    "    \"\"\"用多GPU进行模型训练\"\"\"\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    for epoch in range(num_epochs):\n",
    "        # 4个维度：储存训练损失，训练准确度，实例数，特点数\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch_ch13(\n",
    "                net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[3],\n",
    "                              None))\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
    "          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "               devices=d2l.try_all_gpus()):\n",
    "    \"\"\"用多GPU进行模型训练\n",
    "    Defined in :numref:`sec_image_augmentation`\"\"\"\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    net = paddle.DataParallel(net)\n",
    "    for epoch in range(num_epochs):\n",
    "        # 4个维度：储存训练损失，训练准确度，实例数，特点数\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_batch_ch13(\n",
    "                net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[3],\n",
    "                              None))\n",
    "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
    "          f'{metric[1] / metric[3]:.3f}, test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9608083a",
   "metadata": {},
   "source": [
    "现在，我们可以[**定义`train_with_data_aug`函数，使用图像增广来训练模型**]。该函数获取所有的GPU，并使用Adam作为训练的优化算法，将图像增广应用于训练集，最后调用刚刚定义的用于训练和评估模型的`train_ch13`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba53fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10)\n",
    "net.initialize(init=init.Xavier(), ctx=devices)\n",
    "\n",
    "def train_with_data_aug(train_augs, test_augs, net, lr=0.001):\n",
    "    train_iter = load_cifar10(True, train_augs, batch_size)\n",
    "    test_iter = load_cifar10(False, test_augs, batch_size)\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': lr})\n",
    "    train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121cf794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "batch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10, 3)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) in [nn.Linear, nn.Conv2d]:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "def train_with_data_aug(train_augs, test_augs, net, lr=0.001):\n",
    "    train_iter = load_cifar10(True, train_augs, batch_size)\n",
    "    test_iter = load_cifar10(False, test_augs, batch_size)\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028652e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "batch_size, devices, net = 256, d2l.try_all_gpus(), d2l.resnet18(10, 3)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) in [nn.Linear, nn.Conv2D]:\n",
    "        nn.initializer.XavierUniform(m.weight)\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "def train_with_data_aug(train_augs, test_augs, net, lr=0.001):\n",
    "    train_iter = load_cifar10(True, train_augs, batch_size)\n",
    "    test_iter = load_cifar10(False, test_augs, batch_size)\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    trainer = paddle.optimizer.Adam(learning_rate=lr, parameters=net.parameters())\n",
    "    train_ch13(net, train_iter, test_iter, loss, trainer, 10, devices[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553ec25",
   "metadata": {},
   "source": [
    "让我们使用基于随机左右翻转的图像增广来[**训练模型**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371dde91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "train_with_data_aug(train_augs, test_augs, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f53b6",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 图像增广基于现有的训练数据生成随机图像，来提高模型的泛化能力。\n",
    "* 为了在预测过程中得到确切的结果，我们通常对训练样本只进行图像增广，而在预测过程中不使用带随机操作的图像增广。\n",
    "* 深度学习框架提供了许多不同的图像增广方法，这些方法可以被同时应用。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 在不使用图像增广的情况下训练模型：`train_with_data_aug(no_aug, no_aug)`。比较使用和不使用图像增广的训练结果和测试精度。这个对比实验能支持图像增广可以减轻过拟合的论点吗？为什么？\n",
    "2. 在基于CIFAR-10数据集的模型训练中结合多种不同的图像增广方法。它能提高测试准确性吗？\n",
    "3. 参阅深度学习框架的在线文档。它还提供了哪些其他的图像增广方法？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2828)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2829)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11801)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22be866",
   "metadata": {},
   "source": [
    "# 微调\n",
    ":label:`sec_fine_tuning`\n",
    "\n",
    "前面的一些章节介绍了如何在只有6万张图像的Fashion-MNIST训练数据集上训练模型。\n",
    "我们还描述了学术界当下使用最广泛的大规模图像数据集ImageNet，它有超过1000万的图像和1000类的物体。\n",
    "然而，我们平常接触到的数据集的规模通常在这两者之间。\n",
    "\n",
    "假如我们想识别图片中不同类型的椅子，然后向用户推荐购买链接。\n",
    "一种可能的方法是首先识别100把普通椅子，为每把椅子拍摄1000张不同角度的图像，然后在收集的图像数据集上训练一个分类模型。\n",
    "尽管这个椅子数据集可能大于Fashion-MNIST数据集，但实例数量仍然不到ImageNet中的十分之一。\n",
    "适合ImageNet的复杂模型可能会在这个椅子数据集上过拟合。\n",
    "此外，由于训练样本数量有限，训练模型的准确性可能无法满足实际要求。\n",
    "\n",
    "为了解决上述问题，一个显而易见的解决方案是收集更多的数据。\n",
    "但是，收集和标记数据可能需要大量的时间和金钱。\n",
    "例如，为了收集ImageNet数据集，研究人员花费了数百万美元的研究资金。\n",
    "尽管目前的数据收集成本已大幅降低，但这一成本仍不能忽视。\n",
    "\n",
    "另一种解决方案是应用*迁移学习*（transfer learning）将从*源数据集*学到的知识迁移到*目标数据集*。\n",
    "例如，尽管ImageNet数据集中的大多数图像与椅子无关，但在此数据集上训练的模型可能会提取更通用的图像特征，这有助于识别边缘、纹理、形状和对象组合。\n",
    "这些类似的特征也可能有效地识别椅子。\n",
    "\n",
    "## 步骤\n",
    "\n",
    "本节将介绍迁移学习中的常见技巧:*微调*（fine-tuning）。如 :numref:`fig_finetune`所示，微调包括以下四个步骤。\n",
    "\n",
    "1. 在源数据集（例如ImageNet数据集）上预训练神经网络模型，即*源模型*。\n",
    "1. 创建一个新的神经网络模型，即*目标模型*。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。\n",
    "1. 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。\n",
    "1. 在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调。\n",
    "\n",
    "![微调。](../img/finetune.svg)\n",
    ":label:`fig_finetune`\n",
    "\n",
    "当目标数据集比源数据集小得多时，微调有助于提高模型的泛化能力。\n",
    "\n",
    "## 热狗识别\n",
    "\n",
    "让我们通过具体案例演示微调：热狗识别。\n",
    "我们将在一个小型数据集上微调ResNet模型。该模型已在ImageNet数据集上进行了预训练。\n",
    "这个小型数据集包含数千张包含热狗和不包含热狗的图像，我们将使用微调模型来识别图像中是否包含热狗。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "import os\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ee003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e6225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from paddle import nn\n",
    "import paddle\n",
    "import paddle.vision as paddlevision\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff445005",
   "metadata": {},
   "source": [
    "### 获取数据集\n",
    "\n",
    "我们使用的[**热狗数据集来源于网络**]。\n",
    "该数据集包含1400张热狗的“正类”图像，以及包含尽可能多的其他食物的“负类”图像。\n",
    "含着两个类别的1000张图片用于训练，其余的则用于测试。\n",
    "\n",
    "解压下载的数据集，我们获得了两个文件夹`hotdog/train`和`hotdog/test`。\n",
    "这两个文件夹都有`hotdog`（有热狗）和`not-hotdog`（无热狗）两个子文件夹，\n",
    "子文件夹内都包含相应类的图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip', \n",
    "                         'fba480ffa8aa7e0febbb511d181409f899b9baa5')\n",
    "\n",
    "data_dir = d2l.download_extract('hotdog')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fa421",
   "metadata": {},
   "source": [
    "我们创建两个实例来分别读取训练和测试数据集中的所有图像文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab99209",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = gluon.data.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, 'train'))\n",
    "test_imgs = gluon.data.vision.ImageFolderDataset(\n",
    "    os.path.join(data_dir, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d3b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))\n",
    "test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ca8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "train_imgs = paddlevision.datasets.DatasetFolder(os.path.join(data_dir, 'train'))\n",
    "test_imgs = paddlevision.datasets.DatasetFolder(os.path.join(data_dir, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77910fbe",
   "metadata": {},
   "source": [
    "下面显示了前8个正类样本图片和最后8张负类样本图片。正如所看到的，[**图像的大小和纵横比各有不同**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be826e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "hotdogs = [train_imgs[i][0] for i in range(8)]\n",
    "not_hotdogs = [train_imgs[-i - 1][0] for i in range(8)]\n",
    "d2l.show_images(hotdogs + not_hotdogs, 2, 8, scale=1.4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a918bc75",
   "metadata": {},
   "source": [
    "在训练期间，我们首先从图像中裁切随机大小和随机长宽比的区域，然后将该区域缩放为$224 \\times 224$输入图像。\n",
    "在测试过程中，我们将图像的高度和宽度都缩放到256像素，然后裁剪中央$224 \\times 224$区域作为输入。\n",
    "此外，对于RGB（红、绿和蓝）颜色通道，我们分别*标准化*每个通道。\n",
    "具体而言，该通道的每个值减去该通道的平均值，然后将结果除以该通道的标准差。\n",
    "\n",
    "[~~数据增广~~]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用RGB通道的均值和标准差，以标准化每个通道\n",
    "normalize = gluon.data.vision.transforms.Normalize(\n",
    "    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "train_augs = gluon.data.vision.transforms.Compose([\n",
    "    gluon.data.vision.transforms.RandomResizedCrop(224),\n",
    "    gluon.data.vision.transforms.RandomFlipLeftRight(),\n",
    "    gluon.data.vision.transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "test_augs = gluon.data.vision.transforms.Compose([\n",
    "    gluon.data.vision.transforms.Resize(256),\n",
    "    gluon.data.vision.transforms.CenterCrop(224),\n",
    "    gluon.data.vision.transforms.ToTensor(),\n",
    "    normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "# 使用RGB通道的均值和标准差，以标准化每个通道\n",
    "normalize = torchvision.transforms.Normalize(\n",
    "    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "train_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(224),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "test_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize([256, 256]),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174dee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "# 使用RGB通道的均值和标准差，以标准化每个通道\n",
    "normalize = paddle.vision.transforms.Normalize(\n",
    "    [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "train_augs = paddlevision.transforms.Compose([\n",
    "    paddlevision.transforms.RandomResizedCrop(224),\n",
    "    paddlevision.transforms.RandomHorizontalFlip(),\n",
    "    paddlevision.transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "test_augs = paddlevision.transforms.Compose([\n",
    "    paddlevision.transforms.Resize(256),\n",
    "    paddlevision.transforms.CenterCrop(224),\n",
    "    paddlevision.transforms.ToTensor(),\n",
    "    normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c232b1e",
   "metadata": {},
   "source": [
    "### [**定义和初始化模型**]\n",
    "\n",
    "我们使用在ImageNet数据集上预训练的ResNet-18作为源模型。\n",
    "在这里，我们指定`pretrained=True`以自动下载预训练的模型参数。\n",
    "如果首次使用此模型，则需要连接互联网才能下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3fea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_net = gluon.model_zoo.vision.resnet18_v2(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "pretrained_net = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2ce3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "pretrained_net = paddlevision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8b172",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "预训练的源模型实例包含两个成员变量：`features`和`output`。\n",
    "前者包含除输出层以外的模型的所有层，后者是模型的输出层。\n",
    "此划分的主要目的是促进对除输出层以外所有层的模型参数进行微调。\n",
    "源模型的成员变量`output`如下所示。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "预训练的源模型实例包含许多特征层和一个输出层`fc`。\n",
    "此划分的主要目的是促进对除输出层以外所有层的模型参数进行微调。\n",
    "下面给出了源模型的成员变量`fc`。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "预训练的源模型实例包含许多特征层和一个输出层`fc`。\n",
    "此划分的主要目的是促进对除输出层以外所有层的模型参数进行微调。\n",
    "下面给出了源模型的成员变量`fc`。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_net.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfbfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "pretrained_net.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144edf2a",
   "metadata": {},
   "source": [
    "在ResNet的全局平均汇聚层后，全连接层转换为ImageNet数据集的1000个类输出。\n",
    "之后，我们构建一个新的神经网络作为目标模型。\n",
    "它的定义方式与预训练源模型的定义方式相同，只是最终层中的输出数量被设置为目标数据集中的类数（而不是1000个）。\n",
    "\n",
    "在下面的代码中，目标模型`finetune_net`中成员变量`features`的参数被初始化为源模型相应层的模型参数。\n",
    "由于模型参数是在ImageNet数据集上预训练的，并且足够好，因此通常只需要较小的学习率即可微调这些参数。\n",
    "\n",
    "成员变量`output`的参数是随机初始化的，通常需要更高的学习率才能从头开始训练。\n",
    "假设`Trainer`实例中的学习率为$\\eta$，我们将成员变量`output`中参数的学习率设置为$10\\eta$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35335646",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_net = gluon.model_zoo.vision.resnet18_v2(classes=2)\n",
    "finetune_net.features = pretrained_net.features\n",
    "finetune_net.output.initialize(init.Xavier())\n",
    "# 输出层中的学习率比其他层的学习率大十倍\n",
    "finetune_net.output.collect_params().setattr('lr_mult', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0186a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "finetune_net = torchvision.models.resnet18(pretrained=True)\n",
    "finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)\n",
    "nn.init.xavier_uniform_(finetune_net.fc.weight);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "finetune_net = paddlevision.models.resnet18(pretrained=True)\n",
    "finetune_net.fc = nn.Linear(pretrained_net.fc.state_dict()['weight'].shape[0], 2)\n",
    "nn.initializer.XavierUniform(pretrained_net.fc.state_dict()['weight']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d883925",
   "metadata": {},
   "source": [
    "### [**微调模型**]\n",
    "\n",
    "首先，我们定义了一个训练函数`train_fine_tuning`，该函数使用微调，因此可以多次调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7678992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5):\n",
    "    train_iter = gluon.data.DataLoader(\n",
    "        train_imgs.transform_first(train_augs), batch_size, shuffle=True)\n",
    "    test_iter = gluon.data.DataLoader(\n",
    "        test_imgs.transform_first(test_augs), batch_size)\n",
    "    devices = d2l.try_all_gpus()\n",
    "    net.collect_params().reset_ctx(devices)\n",
    "    net.hybridize()\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {\n",
    "        'learning_rate': learning_rate, 'wd': 0.001})\n",
    "    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "                   devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc086505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "# 如果param_group=True，输出层中的模型参数将使用十倍的学习率\n",
    "def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,\n",
    "                      param_group=True):\n",
    "    train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, 'train'), transform=train_augs),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(\n",
    "        os.path.join(data_dir, 'test'), transform=test_augs),\n",
    "        batch_size=batch_size)\n",
    "    devices = d2l.try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters()\n",
    "             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        trainer = torch.optim.SGD([{'params': params_1x},\n",
    "                                   {'params': net.fc.parameters(),\n",
    "                                    'lr': learning_rate * 10}],\n",
    "                                lr=learning_rate, weight_decay=0.001)\n",
    "    else:\n",
    "        trainer = torch.optim.SGD(net.parameters(), lr=learning_rate,\n",
    "                                  weight_decay=0.001)    \n",
    "    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "                   devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "# 如果param_group=True，输出层中的模型参数将使用十倍的学习率\n",
    "def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5,\n",
    "                      param_group=True):\n",
    "    train_iter = paddle.io.DataLoader(paddle.vision.datasets.DatasetFolder(\n",
    "        os.path.join(data_dir, 'train'), transform=train_augs),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    test_iter = paddle.io.DataLoader(paddle.vision.datasets.DatasetFolder(\n",
    "        os.path.join(data_dir, 'test'), transform=test_augs),\n",
    "        batch_size=batch_size)\n",
    "    devices = d2l.try_all_gpus()\n",
    "    loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    if param_group:\n",
    "        params_1x = [param for name, param in net.named_parameters()\n",
    "             if name not in [\"fc.weight\", \"fc.bias\"]]\n",
    "        trainer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=[{'params': params_1x},\n",
    "                                   {'params': net.fc.parameters(),\n",
    "                                    'learning_rate': learning_rate * 10}],\n",
    "                                    weight_decay=0.001)\n",
    "    else:\n",
    "        trainer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=net.parameters(), \n",
    "                                  weight_decay=0.001)\n",
    "    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee600d",
   "metadata": {},
   "source": [
    "我们[**使用较小的学习率**]，通过*微调*预训练获得的模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fine_tuning(finetune_net, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edacd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "train_fine_tuning(finetune_net, 5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad4c91",
   "metadata": {},
   "source": [
    "[**为了进行比较，**]我们定义了一个相同的模型，但是将其(**所有模型参数初始化为随机值**)。\n",
    "由于整个模型需要从头开始训练，因此我们需要使用更大的学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_net = gluon.model_zoo.vision.resnet18_v2(classes=2)\n",
    "scratch_net.initialize(init=init.Xavier())\n",
    "train_fine_tuning(scratch_net, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "scratch_net = torchvision.models.resnet18()\n",
    "scratch_net.fc = nn.Linear(scratch_net.fc.in_features, 2)\n",
    "train_fine_tuning(scratch_net, 5e-4, param_group=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "scratch_net = paddlevision.models.resnet18()\n",
    "scratch_net.fc = nn.Linear(pretrained_net.fc.state_dict()['weight'].shape[0], 2)\n",
    "train_fine_tuning(scratch_net, 5e-4, param_group=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d45d7",
   "metadata": {},
   "source": [
    "意料之中，微调模型往往表现更好，因为它的初始参数值更有效。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 迁移学习将从源数据集中学到的知识*迁移*到目标数据集，微调是迁移学习的常见技巧。\n",
    "* 除输出层外，目标模型从源模型中复制所有模型设计及其参数，并根据目标数据集对这些参数进行微调。但是，目标模型的输出层需要从头开始训练。\n",
    "* 通常，微调参数使用较小的学习率，而从头开始训练输出层可以使用更大的学习率。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 继续提高`finetune_net`的学习率，模型的准确性如何变化？\n",
    "2. 在比较实验中进一步调整`finetune_net`和`scratch_net`的超参数。它们的准确性还有不同吗？\n",
    "3. 将输出层`finetune_net`之前的参数设置为源模型的参数，在训练期间不要更新它们。模型的准确性如何变化？提示：可以使用以下代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_net.features.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb85bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "for param in finetune_net.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc37e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "for param in finetune_net.parameters():\n",
    "    param.stop_gradient = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892569f7",
   "metadata": {},
   "source": [
    "4. 事实上，`ImageNet`数据集中有一个“热狗”类别。我们可以通过以下代码获取其输出层中的相应权重参数，但是我们怎样才能利用这个权重参数？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cde0466",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = pretrained_net.output.weight\n",
    "hotdog_w = np.split(weight.data(), 1000, axis=0)[713]\n",
    "hotdog_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed620b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "weight = pretrained_net.fc.weight\n",
    "hotdog_w = torch.split(weight.data, 1, dim=0)[934]\n",
    "hotdog_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea16056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "weight = pretrained_net.fc.weight\n",
    "hotdog_w = paddle.split(weight.T, 1000, axis=0)[713]\n",
    "hotdog_w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9358689d",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2893)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2894)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11802)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50778402",
   "metadata": {},
   "source": [
    "# 目标检测和边界框\n",
    ":label:`sec_bbox`\n",
    "\n",
    "前面的章节（例如 :numref:`sec_alexnet`— :numref:`sec_googlenet`）介绍了各种图像分类模型。\n",
    "在图像分类任务中，我们假设图像中只有一个主要物体对象，我们只关注如何识别其类别。\n",
    "然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置。\n",
    "在计算机视觉里，我们将这类任务称为*目标检测*（object detection）或*目标识别*（object recognition）。\n",
    "\n",
    "目标检测在多个领域中被广泛使用。\n",
    "例如，在无人驾驶里，我们需要通过识别拍摄到的视频图像里的车辆、行人、道路和障碍物的位置来规划行进线路。\n",
    "机器人也常通过该任务来检测感兴趣的目标。安防领域则需要检测异常目标，如歹徒或者炸弹。\n",
    "\n",
    "接下来的几节将介绍几种用于目标检测的深度学习方法。\n",
    "我们将首先介绍目标的*位置*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f65481",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import image, npx, np\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9432a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "%matplotlib inline\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da748b54",
   "metadata": {},
   "source": [
    "下面加载本节将使用的示例图像。可以看到图像左边是一只狗，右边是一只猫。\n",
    "它们是这张图像里的两个主要目标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a938630",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "img = image.imread('../img/catdog.jpg').asnumpy()\n",
    "d2l.plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, tensorflow, paddle\n",
    "d2l.set_figsize()\n",
    "img = d2l.plt.imread('../img/catdog.jpg')\n",
    "d2l.plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed216909",
   "metadata": {},
   "source": [
    "## 边界框\n",
    "\n",
    "在目标检测中，我们通常使用*边界框*（bounding box）来描述对象的空间位置。\n",
    "边界框是矩形的，由矩形左上角的以及右下角的$x$和$y$坐标决定。\n",
    "另一种常用的边界框表示方法是边界框中心的$(x, y)$轴坐标以及框的宽度和高度。\n",
    "\n",
    "在这里，我们[**定义在这两种表示法之间进行转换的函数**]：`box_corner_to_center`从两角表示法转换为中心宽度表示法，而`box_center_to_corner`反之亦然。\n",
    "输入参数`boxes`可以是长度为4的张量，也可以是形状为（$n$，4）的二维张量，其中$n$是边界框的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6372ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def box_corner_to_center(boxes):\n",
    "    \"\"\"从（左上，右下）转换到（中间，宽度，高度）\"\"\"\n",
    "    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    cx = (x1 + x2) / 2\n",
    "    cy = (y1 + y2) / 2\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    boxes = d2l.stack((cx, cy, w, h), axis=-1)\n",
    "    return boxes\n",
    "\n",
    "#@save\n",
    "def box_center_to_corner(boxes):\n",
    "    \"\"\"从（中间，宽度，高度）转换到（左上，右下）\"\"\"\n",
    "    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    x1 = cx - 0.5 * w\n",
    "    y1 = cy - 0.5 * h\n",
    "    x2 = cx + 0.5 * w\n",
    "    y2 = cy + 0.5 * h\n",
    "    boxes = d2l.stack((x1, y1, x2, y2), axis=-1)\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1f9dd",
   "metadata": {},
   "source": [
    "我们将根据坐标信息[**定义图像中狗和猫的边界框**]。\n",
    "图像中坐标的原点是图像的左上角，向右的方向为$x$轴的正方向，向下的方向为$y$轴的正方向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc81c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "# bbox是边界框的英文缩写\n",
    "dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33716866",
   "metadata": {},
   "source": [
    "我们可以通过转换两次来验证边界框转换函数的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc41ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "boxes = d2l.tensor((dog_bbox, cat_bbox))\n",
    "box_center_to_corner(box_corner_to_center(boxes)) == boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d42216",
   "metadata": {},
   "source": [
    "我们可以[**将边界框在图中画出**]，以检查其是否准确。\n",
    "画之前，我们定义一个辅助函数`bbox_to_rect`。\n",
    "它将边界框表示成`matplotlib`的边界框格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28559582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def bbox_to_rect(bbox, color):\n",
    "    # 将边界框(左上x,左上y,右下x,右下y)格式转换成matplotlib格式：\n",
    "    # ((左上x,左上y),宽,高)\n",
    "    return d2l.plt.Rectangle(\n",
    "        xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0], height=bbox[3]-bbox[1],\n",
    "        fill=False, edgecolor=color, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df99a0",
   "metadata": {},
   "source": [
    "在图像上添加边界框之后，我们可以看到两个物体的主要轮廓基本上在两个框内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e54f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "fig = d2l.plt.imshow(img)\n",
    "fig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))\n",
    "fig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b206d1",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 目标检测不仅可以识别图像中所有感兴趣的物体，还能识别它们的位置，该位置通常由矩形边界框表示。\n",
    "* 我们可以在两种常用的边界框表示（中间，宽度，高度）和（左上，右下）坐标之间进行转换。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 找到另一张图像，然后尝试标记包含该对象的边界框。比较标注边界框和标注类别哪个需要更长的时间？\n",
    "1. 为什么`box_corner_to_center`和`box_center_to_corner`的输入参数的最内层维度总是4？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2943)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2944)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11803)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc7a44",
   "metadata": {},
   "source": [
    "# 锚框\n",
    ":label:`sec_anchor`\n",
    "\n",
    "目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的*真实边界框*（ground-truth bounding box）。\n",
    "不同的模型使用的区域采样方法可能不同。\n",
    "这里我们介绍其中的一种方法：以每个像素为中心，生成多个缩放比和宽高比（aspect ratio）不同的边界框。\n",
    "这些边界框被称为*锚框*（anchor box）我们将在 :numref:`sec_ssd`中设计一个基于锚框的目标检测模型。\n",
    "\n",
    "首先，让我们修改输出精度，以获得更简洁的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac46140",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, image, np, npx\n",
    "\n",
    "np.set_printoptions(2)  # 精简输出精度\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(2)  # 精简输出精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "import numpy as np\n",
    "\n",
    "paddle.set_printoptions(2)  # 精简输出精度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5520cf",
   "metadata": {},
   "source": [
    "## 生成多个锚框\n",
    "\n",
    "假设输入图像的高度为$h$，宽度为$w$。\n",
    "我们以图像的每个像素为中心生成不同形状的锚框：*缩放比*为$s\\in (0, 1]$，*宽高比*为$r > 0$。\n",
    "那么[**锚框的宽度和高度分别是$hs\\sqrt{r}$和$hs/\\sqrt{r}$。**]\n",
    "请注意，当中心位置给定时，已知宽和高的锚框是确定的。\n",
    "\n",
    "要生成多个不同形状的锚框，让我们设置许多缩放比（scale）取值$s_1,\\ldots, s_n$和许多宽高比（aspect ratio）取值$r_1,\\ldots, r_m$。\n",
    "当使用这些比例和长宽比的所有组合以每个像素为中心时，输入图像将总共有$whnm$个锚框。\n",
    "尽管这些锚框可能会覆盖所有真实边界框，但计算复杂性很容易过高。\n",
    "在实践中，(**我们只考虑**)包含$s_1$或$r_1$的(**组合：**)\n",
    "\n",
    "(**\n",
    "$$(s_1, r_1), (s_1, r_2), \\ldots, (s_1, r_m), (s_2, r_1), (s_3, r_1), \\ldots, (s_n, r_1).$$\n",
    "**)\n",
    "\n",
    "也就是说，以同一像素为中心的锚框的数量是$n+m-1$。\n",
    "对于整个输入图像，将共生成$wh(n+m-1)$个锚框。\n",
    "\n",
    "上述生成锚框的方法在下面的`multibox_prior`函数中实现。\n",
    "我们指定输入图像、尺寸列表和宽高比列表，然后此函数将返回所有的锚框。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b819242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def multibox_prior(data, sizes, ratios):\n",
    "    \"\"\"生成以每个像素为中心具有不同形状的锚框\"\"\"\n",
    "    in_height, in_width = data.shape[-2:]\n",
    "    device, num_sizes, num_ratios = data.ctx, len(sizes), len(ratios)\n",
    "    boxes_per_pixel = (num_sizes + num_ratios - 1)\n",
    "    size_tensor = d2l.tensor(sizes, ctx=device)\n",
    "    ratio_tensor = d2l.tensor(ratios, ctx=device)\n",
    "\n",
    "    # 为了将锚点移动到像素的中心，需要设置偏移量。\n",
    "    # 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5\n",
    "    offset_h, offset_w = 0.5, 0.5\n",
    "    steps_h = 1.0 / in_height  # 在y轴上缩放步长\n",
    "    steps_w = 1.0 / in_width  # 在x轴上缩放步长\n",
    "\n",
    "    # 生成锚框的所有中心点\n",
    "    center_h = (d2l.arange(in_height, ctx=device) + offset_h) * steps_h\n",
    "    center_w = (d2l.arange(in_width, ctx=device) + offset_w) * steps_w\n",
    "    shift_x, shift_y = d2l.meshgrid(center_w, center_h)\n",
    "    shift_x, shift_y = shift_x.reshape(-1), shift_y.reshape(-1)\n",
    "\n",
    "    # 生成“boxes_per_pixel”个高和宽，\n",
    "    # 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax)\n",
    "    w = np.concatenate((size_tensor * np.sqrt(ratio_tensor[0]),\n",
    "                        sizes[0] * np.sqrt(ratio_tensor[1:]))) \\\n",
    "                        * in_height / in_width  # 处理矩形输入\n",
    "    h = np.concatenate((size_tensor / np.sqrt(ratio_tensor[0]),\n",
    "                        sizes[0] / np.sqrt(ratio_tensor[1:])))\n",
    "    # 除以2来获得半高和半宽\n",
    "    anchor_manipulations = np.tile(np.stack((-w, -h, w, h)).T,\n",
    "                                   (in_height * in_width, 1)) / 2\n",
    "\n",
    "    # 每个中心点都将有“boxes_per_pixel”个锚框，\n",
    "    # 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次\n",
    "    out_grid = d2l.stack([shift_x, shift_y, shift_x, shift_y],\n",
    "                         axis=1).repeat(boxes_per_pixel, axis=0)\n",
    "    output = out_grid + anchor_manipulations\n",
    "    return np.expand_dims(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def multibox_prior(data, sizes, ratios):\n",
    "    \"\"\"生成以每个像素为中心具有不同形状的锚框\"\"\"\n",
    "    in_height, in_width = data.shape[-2:]\n",
    "    device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)\n",
    "    boxes_per_pixel = (num_sizes + num_ratios - 1)\n",
    "    size_tensor = d2l.tensor(sizes, device=device)\n",
    "    ratio_tensor = d2l.tensor(ratios, device=device)\n",
    "\n",
    "    # 为了将锚点移动到像素的中心，需要设置偏移量。\n",
    "    # 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5\n",
    "    offset_h, offset_w = 0.5, 0.5\n",
    "    steps_h = 1.0 / in_height  # 在y轴上缩放步长\n",
    "    steps_w = 1.0 / in_width  # 在x轴上缩放步长\n",
    "\n",
    "    # 生成锚框的所有中心点\n",
    "    center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h\n",
    "    center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w\n",
    "    shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')\n",
    "    shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)\n",
    "\n",
    "    # 生成“boxes_per_pixel”个高和宽，\n",
    "    # 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax)\n",
    "    w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),\n",
    "                   sizes[0] * torch.sqrt(ratio_tensor[1:])))\\\n",
    "                   * in_height / in_width  # 处理矩形输入\n",
    "    h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),\n",
    "                   sizes[0] / torch.sqrt(ratio_tensor[1:])))\n",
    "    # 除以2来获得半高和半宽\n",
    "    anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(\n",
    "                                        in_height * in_width, 1) / 2\n",
    "\n",
    "    # 每个中心点都将有“boxes_per_pixel”个锚框，\n",
    "    # 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次\n",
    "    out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],\n",
    "                dim=1).repeat_interleave(boxes_per_pixel, dim=0)\n",
    "    output = out_grid + anchor_manipulations\n",
    "    return output.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def multibox_prior(data, sizes, ratios):\n",
    "    \"\"\"生成以每个像素为中心具有不同形状的锚框\"\"\"\n",
    "    in_height, in_width = data.shape[-2:]\n",
    "    place, num_sizes, num_ratios = data.place, len(sizes), len(ratios)\n",
    "    boxes_per_pixel = (num_sizes + num_ratios - 1)\n",
    "    size_tensor = paddle.to_tensor(sizes, place=place)\n",
    "    ratio_tensor = paddle.to_tensor(ratios, place=place)\n",
    "\n",
    "    # 为了将锚点移动到像素的中心，需要设置偏移量。\n",
    "    # 因为一个像素的的高为1且宽为1，我们选择偏移我们的中心0.5\n",
    "    offset_h, offset_w = 0.5, 0.5\n",
    "    steps_h = 1.0 / in_height  # 在y轴上缩放步长\n",
    "    steps_w = 1.0 / in_width  # 在x轴上缩放步长\n",
    "\n",
    "    # 生成锚框的所有中心点\n",
    "    center_h = (paddle.arange(in_height) + offset_h) * steps_h\n",
    "    center_w = (paddle.arange(in_width) + offset_w) * steps_w\n",
    "    shift_y, shift_x = paddle.meshgrid(center_h, center_w)\n",
    "    shift_y, shift_x = shift_y.reshape([-1]), shift_x.reshape([-1])\n",
    "\n",
    "    # 生成“boxes_per_pixel”个高和宽，\n",
    "    # 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax)\n",
    "    w = paddle.concat((size_tensor * paddle.sqrt(ratio_tensor[0]),\n",
    "                       sizes[0] * paddle.sqrt(ratio_tensor[1:])))\\\n",
    "                       * in_height / in_width  # 处理矩形输入\n",
    "    h = paddle.concat((size_tensor / paddle.sqrt(ratio_tensor[0]),\n",
    "                   sizes[0] / paddle.sqrt(ratio_tensor[1:])))\n",
    "    # 除以2来获得半高和半宽\n",
    "    anchor_manipulations = paddle.tile(paddle.stack((-w, -h, w, h)).T,\n",
    "                                        (in_height * in_width, 1)) / 2\n",
    "\n",
    "    # 每个中心点都将有“boxes_per_pixel”个锚框，\n",
    "    # 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次\n",
    "    out_grid = paddle.stack([shift_x, shift_y, shift_x, shift_y], axis=1)\n",
    "    out_grid = paddle.tile(out_grid, repeat_times=[boxes_per_pixel]).reshape((-1, out_grid.shape[1]))\n",
    "    output = out_grid + anchor_manipulations\n",
    "    return output.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70adad",
   "metadata": {},
   "source": [
    "可以看到[**返回的锚框变量`Y`的形状**]是（批量大小，锚框的数量，4）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c178f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.imread('../img/catdog.jpg').asnumpy()\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "print(h, w)\n",
    "X = np.random.uniform(size=(1, 3, h, w))\n",
    "Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "img = d2l.plt.imread('../img/catdog.jpg')\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "print(h, w)\n",
    "X = torch.rand(size=(1, 3, h, w))\n",
    "Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "img = d2l.plt.imread('../img/catdog.jpg')\n",
    "h, w = img.shape[:2]\n",
    "\n",
    "print(h, w)\n",
    "X = paddle.rand(shape=(1, 3, h, w))\n",
    "Y = multibox_prior(X, sizes=[0.75, 0.5, 0.25], ratios=[1, 2, 0.5])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0add4e0",
   "metadata": {},
   "source": [
    "将锚框变量`Y`的形状更改为(图像高度,图像宽度,以同一像素为中心的锚框的数量,4)后，我们可以获得以指定像素的位置为中心的所有锚框。\n",
    "在接下来的内容中，我们[**访问以（250,250）为中心的第一个锚框**]。\n",
    "它有四个元素：锚框左上角的$(x, y)$轴坐标和右下角的$(x, y)$轴坐标。\n",
    "输出中两个轴的坐标各分别除以了图像的宽度和高度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f90da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "boxes = Y.reshape(h, w, 5, 4)\n",
    "boxes[250, 250, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffacdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "boxes = Y.reshape([h, w, 5, 4])\n",
    "boxes[250, 250, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497b873",
   "metadata": {},
   "source": [
    "为了[**显示以图像中以某个像素为中心的所有锚框**]，定义下面的`show_bboxes`函数来在图像上绘制多个边界框。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def show_bboxes(axes, bboxes, labels=None, colors=None):\n",
    "    \"\"\"显示所有边界框\"\"\"\n",
    "    def _make_list(obj, default_values=None):\n",
    "        if obj is None:\n",
    "            obj = default_values\n",
    "        elif not isinstance(obj, (list, tuple)):\n",
    "            obj = [obj]\n",
    "        return obj\n",
    "        \n",
    "    labels = _make_list(labels)\n",
    "    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'c'])\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        color = colors[i % len(colors)]\n",
    "        rect = d2l.bbox_to_rect(d2l.numpy(bbox), color)\n",
    "        axes.add_patch(rect)\n",
    "        if labels and len(labels) > i:\n",
    "            text_color = 'k' if color == 'w' else 'w'\n",
    "            axes.text(rect.xy[0], rect.xy[1], labels[i],\n",
    "                      va='center', ha='center', fontsize=9, color=text_color,\n",
    "                      bbox=dict(facecolor=color, lw=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b42fb",
   "metadata": {},
   "source": [
    "正如从上面代码中所看到的，变量`boxes`中$x$轴和$y$轴的坐标值已分别除以图像的宽度和高度。\n",
    "绘制锚框时，我们需要恢复它们原始的坐标值。\n",
    "因此，在下面定义了变量`bbox_scale`。\n",
    "现在可以绘制出图像中所有以(250,250)为中心的锚框了。\n",
    "如下所示，缩放比为0.75且宽高比为1的蓝色锚框很好地围绕着图像中的狗。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e6e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "d2l.set_figsize()\n",
    "bbox_scale = d2l.tensor((w, h, w, h))\n",
    "fig = d2l.plt.imshow(img)\n",
    "show_bboxes(fig.axes, boxes[250, 250, :, :] * bbox_scale,\n",
    "            ['s=0.75, r=1', 's=0.5, r=1', 's=0.25, r=1', 's=0.75, r=2',\n",
    "             's=0.75, r=0.5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcce7267",
   "metadata": {},
   "source": [
    "## [**交并比（IoU）**]\n",
    "\n",
    "我们刚刚提到某个锚框“较好地”覆盖了图像中的狗。\n",
    "如果已知目标的真实边界框，那么这里的“好”该如何如何量化呢？\n",
    "直观地说，可以衡量锚框和真实边界框之间的相似性。\n",
    "*杰卡德系数*（Jaccard）可以衡量两组之间的相似性。\n",
    "给定集合$\\mathcal{A}$和$\\mathcal{B}$，他们的杰卡德系数是他们交集的大小除以他们并集的大小：\n",
    "\n",
    "$$J(\\mathcal{A},\\mathcal{B}) = \\frac{\\left|\\mathcal{A} \\cap \\mathcal{B}\\right|}{\\left| \\mathcal{A} \\cup \\mathcal{B}\\right|}.$$\n",
    "\n",
    "事实上，我们可以将任何边界框的像素区域视为一组像素。通\n",
    "过这种方式，我们可以通过其像素集的杰卡德系数来测量两个边界框的相似性。\n",
    "对于两个边界框，它们的杰卡德系数通常称为*交并比*（intersection over union，IoU），即两个边界框相交面积与相并面积之比，如 :numref:`fig_iou`所示。\n",
    "交并比的取值范围在0和1之间：0表示两个边界框无重合像素，1表示两个边界框完全重合。\n",
    "\n",
    "![交并比是两个边界框相交面积与相并面积之比。](../img/iou.svg)\n",
    ":label:`fig_iou`\n",
    "\n",
    "接下来部分将使用交并比来衡量锚框和真实边界框之间、以及不同锚框之间的相似度。\n",
    "给定两个锚框或边界框的列表，以下`box_iou`函数将在这两个列表中计算它们成对的交并比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206524c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def box_iou(boxes1, boxes2):\n",
    "    \"\"\"计算两个锚框或边界框列表中成对的交并比\"\"\"\n",
    "    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n",
    "                              (boxes[:, 3] - boxes[:, 1]))\n",
    "    # boxes1,boxes2,areas1,areas2的形状:\n",
    "    # boxes1：(boxes1的数量,4),\n",
    "    # boxes2：(boxes2的数量,4),\n",
    "    # areas1：(boxes1的数量,),\n",
    "    # areas2：(boxes2的数量,)\n",
    "    areas1 = box_area(boxes1)\n",
    "    areas2 = box_area(boxes2)\n",
    "\n",
    "    # inter_upperlefts,inter_lowerrights,inters的形状:\n",
    "    # (boxes1的数量,boxes2的数量,2)\n",
    "    inter_upperlefts = np.maximum(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    inter_lowerrights = np.minimum(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    inters = (inter_lowerrights - inter_upperlefts).clip(min=0)\n",
    "    # inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量)\n",
    "    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n",
    "    union_areas = areas1[:, None] + areas2 - inter_areas\n",
    "    return inter_areas / union_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab2b431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def box_iou(boxes1, boxes2):\n",
    "    \"\"\"计算两个锚框或边界框列表中成对的交并比\"\"\"\n",
    "    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n",
    "                              (boxes[:, 3] - boxes[:, 1]))\n",
    "    # boxes1,boxes2,areas1,areas2的形状:\n",
    "    # boxes1：(boxes1的数量,4),\n",
    "    # boxes2：(boxes2的数量,4),\n",
    "    # areas1：(boxes1的数量,),\n",
    "    # areas2：(boxes2的数量,)\n",
    "    areas1 = box_area(boxes1)\n",
    "    areas2 = box_area(boxes2)\n",
    "    # inter_upperlefts,inter_lowerrights,inters的形状:\n",
    "    # (boxes1的数量,boxes2的数量,2)\n",
    "    inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)\n",
    "    # inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量)\n",
    "    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n",
    "    union_areas = areas1[:, None] + areas2 - inter_areas\n",
    "    return inter_areas / union_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def box_iou(boxes1, boxes2):\n",
    "    \"\"\"计算两个锚框或边界框列表中成对的交并比\"\"\"\n",
    "    box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) *\n",
    "                              (boxes[:, 3] - boxes[:, 1]))\n",
    "    # boxes1,boxes2,areas1,areas2的形状:\n",
    "    # boxes1：(boxes1的数量,4),\n",
    "    # boxes2：(boxes2的数量,4),\n",
    "    # areas1：(boxes1的数量,),\n",
    "    # areas2：(boxes2的数量,)\n",
    "    areas1 = box_area(boxes1)\n",
    "    areas2 = box_area(boxes2)\n",
    "    # inter_upperlefts,inter_lowerrights,inters的形状:\n",
    "    # (boxes1的数量,boxes2的数量,2)\n",
    "    inter_upperlefts = paddle.maximum(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    inter_lowerrights = paddle.minimum(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    inters = (inter_lowerrights - inter_upperlefts).clip(min=0)\n",
    "    # inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量)\n",
    "    inter_areas = inters[:, :, 0] * inters[:, :, 1]\n",
    "    union_areas = areas1[:, None] + areas2 - inter_areas\n",
    "    return inter_areas / union_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70358917",
   "metadata": {},
   "source": [
    "## 在训练数据中标注锚框\n",
    ":label:`subsec_labeling-anchor-boxes`\n",
    "\n",
    "在训练集中，我们将每个锚框视为一个训练样本。\n",
    "为了训练目标检测模型，我们需要每个锚框的*类别*（class）和*偏移量*（offset）标签，其中前者是与锚框相关的对象的类别，后者是真实边界框相对于锚框的偏移量。\n",
    "在预测时，我们为每个图像生成多个锚框，预测所有锚框的类别和偏移量，根据预测的偏移量调整它们的位置以获得预测的边界框，最后只输出符合特定条件的预测边界框。\n",
    "\n",
    "目标检测训练集带有*真实边界框*的位置及其包围物体类别的标签。\n",
    "要标记任何生成的锚框，我们可以参考分配到的最接近此锚框的真实边界框的位置和类别标签。\n",
    "下文将介绍一个算法，它能够把最接近的真实边界框分配给锚框。\n",
    "\n",
    "### [**将真实边界框分配给锚框**]\n",
    "\n",
    "给定图像，假设锚框是$A_1, A_2, \\ldots, A_{n_a}$，真实边界框是$B_1, B_2, \\ldots, B_{n_b}$，其中$n_a \\geq n_b$。\n",
    "让我们定义一个矩阵$\\mathbf{X} \\in \\mathbb{R}^{n_a \\times n_b}$，其中第$i$行、第$j$列的元素$x_{ij}$是锚框$A_i$和真实边界框$B_j$的IoU。\n",
    "该算法包含以下步骤。\n",
    "\n",
    "1. 在矩阵$\\mathbf{X}$中找到最大的元素，并将它的行索引和列索引分别表示为$i_1$和$j_1$。然后将真实边界框$B_{j_1}$分配给锚框$A_{i_1}$。这很直观，因为$A_{i_1}$和$B_{j_1}$是所有锚框和真实边界框配对中最相近的。在第一个分配完成后，丢弃矩阵中${i_1}^\\mathrm{th}$行和${j_1}^\\mathrm{th}$列中的所有元素。\n",
    "1. 在矩阵$\\mathbf{X}$中找到剩余元素中最大的元素，并将它的行索引和列索引分别表示为$i_2$和$j_2$。我们将真实边界框$B_{j_2}$分配给锚框$A_{i_2}$，并丢弃矩阵中${i_2}^\\mathrm{th}$行和${j_2}^\\mathrm{th}$列中的所有元素。\n",
    "1. 此时，矩阵$\\mathbf{X}$中两行和两列中的元素已被丢弃。我们继续，直到丢弃掉矩阵$\\mathbf{X}$中$n_b$列中的所有元素。此时已经为这$n_b$个锚框各自分配了一个真实边界框。\n",
    "1. 只遍历剩下的$n_a - n_b$个锚框。例如，给定任何锚框$A_i$，在矩阵$\\mathbf{X}$的第$i^\\mathrm{th}$行中找到与$A_i$的IoU最大的真实边界框$B_j$，只有当此IoU大于预定义的阈值时，才将$B_j$分配给$A_i$。\n",
    "\n",
    "下面用一个具体的例子来说明上述算法。\n",
    "如 :numref:`fig_anchor_label`（左）所示，假设矩阵$\\mathbf{X}$中的最大值为$x_{23}$，我们将真实边界框$B_3$分配给锚框$A_2$。\n",
    "然后，我们丢弃矩阵第2行和第3列中的所有元素，在剩余元素（阴影区域）中找到最大的$x_{71}$，然后将真实边界框$B_1$分配给锚框$A_7$。\n",
    "接下来，如 :numref:`fig_anchor_label`（中）所示，丢弃矩阵第7行和第1列中的所有元素，在剩余元素（阴影区域）中找到最大的$x_{54}$，然后将真实边界框$B_4$分配给锚框$A_5$。\n",
    "最后，如 :numref:`fig_anchor_label`（右）所示，丢弃矩阵第5行和第4列中的所有元素，在剩余元素（阴影区域）中找到最大的$x_{92}$，然后将真实边界框$B_2$分配给锚框$A_9$。\n",
    "之后，我们只需要遍历剩余的锚框$A_1, A_3, A_4, A_6, A_8$，然后根据阈值确定是否为它们分配真实边界框。\n",
    "\n",
    "![将真实边界框分配给锚框。](../img/anchor-label.svg)\n",
    ":label:`fig_anchor_label`\n",
    "\n",
    "此算法在下面的`assign_anchor_to_bbox`函数中实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28765a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):\n",
    "    \"\"\"将最接近的真实边界框分配给锚框\"\"\"\n",
    "    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n",
    "    # 位于第i行和第j列的元素x_ij是锚框i和真实边界框j的IoU\n",
    "    jaccard = box_iou(anchors, ground_truth)\n",
    "    # 对于每个锚框，分配的真实边界框的张量\n",
    "    anchors_bbox_map = np.full((num_anchors,), -1, dtype=np.int32, ctx=device)\n",
    "    # 根据阈值，决定是否分配真实边界框\n",
    "    max_ious, indices = np.max(jaccard, axis=1), np.argmax(jaccard, axis=1)\n",
    "    anc_i = np.nonzero(max_ious >= iou_threshold)[0]\n",
    "    box_j = indices[max_ious >= iou_threshold]\n",
    "    anchors_bbox_map[anc_i] = box_j\n",
    "    col_discard = np.full((num_anchors,), -1)\n",
    "    row_discard = np.full((num_gt_boxes,), -1)\n",
    "    for _ in range(num_gt_boxes):\n",
    "        max_idx = np.argmax(jaccard)\n",
    "        box_idx = (max_idx % num_gt_boxes).astype('int32')\n",
    "        anc_idx = (max_idx / num_gt_boxes).astype('int32')\n",
    "        anchors_bbox_map[anc_idx] = box_idx\n",
    "        jaccard[:, box_idx] = col_discard\n",
    "        jaccard[anc_idx, :] = row_discard\n",
    "    return anchors_bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb5e4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5):\n",
    "    \"\"\"将最接近的真实边界框分配给锚框\"\"\"\n",
    "    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n",
    "    # 位于第i行和第j列的元素x_ij是锚框i和真实边界框j的IoU\n",
    "    jaccard = box_iou(anchors, ground_truth)\n",
    "    # 对于每个锚框，分配的真实边界框的张量\n",
    "    anchors_bbox_map = torch.full((num_anchors,), -1, dtype=torch.long,\n",
    "                                  device=device)\n",
    "    # 根据阈值，决定是否分配真实边界框\n",
    "    max_ious, indices = torch.max(jaccard, dim=1)\n",
    "    anc_i = torch.nonzero(max_ious >= iou_threshold).reshape(-1)\n",
    "    box_j = indices[max_ious >= iou_threshold]\n",
    "    anchors_bbox_map[anc_i] = box_j\n",
    "    col_discard = torch.full((num_anchors,), -1)\n",
    "    row_discard = torch.full((num_gt_boxes,), -1)\n",
    "    for _ in range(num_gt_boxes):\n",
    "        max_idx = torch.argmax(jaccard)\n",
    "        box_idx = (max_idx % num_gt_boxes).long()\n",
    "        anc_idx = (max_idx / num_gt_boxes).long()\n",
    "        anchors_bbox_map[anc_idx] = box_idx\n",
    "        jaccard[:, box_idx] = col_discard\n",
    "        jaccard[anc_idx, :] = row_discard\n",
    "    return anchors_bbox_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def assign_anchor_to_bbox(ground_truth, anchors, place, iou_threshold=0.5):\n",
    "    \"\"\"将最接近的真实边界框分配给锚框\"\"\"\n",
    "    num_anchors, num_gt_boxes = anchors.shape[0], ground_truth.shape[0]\n",
    "    # 位于第i行和第j列的元素x_ij是锚框i和真实边界框j的IoU\n",
    "    jaccard = box_iou(anchors, ground_truth)\n",
    "    # 对于每个锚框，分配的真实边界框的张量\n",
    "    anchors_bbox_map = paddle.full((num_anchors,), -1, dtype=paddle.int64)\n",
    "    # 根据阈值，决定是否分配真实边界框\n",
    "    max_ious = paddle.max(jaccard, axis=1)\n",
    "    indices = paddle.argmax(jaccard, axis=1)\n",
    "    anc_i = paddle.nonzero(max_ious >= 0.5).reshape([-1])\n",
    "    box_j = indices[max_ious >= 0.5]\n",
    "    anchors_bbox_map[anc_i] = box_j\n",
    "    col_discard = paddle.full((num_anchors,), -1)\n",
    "    row_discard = paddle.full((num_gt_boxes,), -1)\n",
    "    for _ in range(num_gt_boxes):\n",
    "        max_idx = paddle.argmax(jaccard)\n",
    "        box_idx = paddle.cast((max_idx % num_gt_boxes), dtype='int64')\n",
    "        anc_idx = paddle.cast((max_idx / num_gt_boxes), dtype='int64')\n",
    "        anchors_bbox_map[anc_idx] = box_idx\n",
    "        jaccard[:, box_idx] = col_discard\n",
    "        jaccard[anc_idx, :] = row_discard\n",
    "    return anchors_bbox_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7405c6e1",
   "metadata": {},
   "source": [
    "### 标记类别和偏移量\n",
    "\n",
    "现在我们可以为每个锚框标记类别和偏移量了。\n",
    "假设一个锚框$A$被分配了一个真实边界框$B$。\n",
    "一方面，锚框$A$的类别将被标记为与$B$相同。\n",
    "另一方面，锚框$A$的偏移量将根据$B$和$A$中心坐标的相对位置以及这两个框的相对大小进行标记。\n",
    "鉴于数据集内不同的框的位置和大小不同，我们可以对那些相对位置和大小应用变换，使其获得分布更均匀且易于拟合的偏移量。\n",
    "这里介绍一种常见的变换。\n",
    "[**给定框$A$和$B$，中心坐标分别为$(x_a, y_a)$和$(x_b, y_b)$，宽度分别为$w_a$和$w_b$，高度分别为$h_a$和$h_b$，可以将$A$的偏移量标记为：\n",
    "\n",
    "$$\\left( \\frac{ \\frac{x_b - x_a}{w_a} - \\mu_x }{\\sigma_x},\n",
    "\\frac{ \\frac{y_b - y_a}{h_a} - \\mu_y }{\\sigma_y},\n",
    "\\frac{ \\log \\frac{w_b}{w_a} - \\mu_w }{\\sigma_w},\n",
    "\\frac{ \\log \\frac{h_b}{h_a} - \\mu_h }{\\sigma_h}\\right),$$\n",
    "**]\n",
    "其中常量的默认值为 $\\mu_x = \\mu_y = \\mu_w = \\mu_h = 0, \\sigma_x=\\sigma_y=0.1$ ， $\\sigma_w=\\sigma_h=0.2$。\n",
    "这种转换在下面的 `offset_boxes` 函数中实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a239e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def offset_boxes(anchors, assigned_bb, eps=1e-6):\n",
    "    \"\"\"对锚框偏移量的转换\"\"\"\n",
    "    c_anc = d2l.box_corner_to_center(anchors)\n",
    "    c_assigned_bb = d2l.box_corner_to_center(assigned_bb)\n",
    "    offset_xy = 10 * (c_assigned_bb[:, :2] - c_anc[:, :2]) / c_anc[:, 2:]\n",
    "    offset_wh = 5 * d2l.log(eps + c_assigned_bb[:, 2:] / c_anc[:, 2:])\n",
    "    offset = d2l.concat([offset_xy, offset_wh], axis=1)\n",
    "    return offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e2257",
   "metadata": {},
   "source": [
    "如果一个锚框没有被分配真实边界框，我们只需将锚框的类别标记为*背景*（background）。\n",
    "背景类别的锚框通常被称为*负类*锚框，其余的被称为*正类*锚框。\n",
    "我们使用真实边界框（`labels`参数）实现以下`multibox_target`函数，来[**标记锚框的类别和偏移量**]（`anchors`参数）。\n",
    "此函数将背景类别的索引设置为零，然后将新类别的整数索引递增一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba44ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def multibox_target(anchors, labels):\n",
    "    \"\"\"使用真实边界框标记锚框\"\"\"\n",
    "    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n",
    "    batch_offset, batch_mask, batch_class_labels = [], [], []\n",
    "    device, num_anchors = anchors.ctx, anchors.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        label = labels[i, :, :]\n",
    "        anchors_bbox_map = assign_anchor_to_bbox(\n",
    "            label[:, 1:], anchors, device)\n",
    "        bbox_mask = np.tile((np.expand_dims((anchors_bbox_map >= 0),\n",
    "                                            axis=-1)), (1, 4)).astype('int32')\n",
    "        # 将类标签和分配的边界框坐标初始化为零\n",
    "        class_labels = d2l.zeros(num_anchors, dtype=np.int32, ctx=device)\n",
    "        assigned_bb = d2l.zeros((num_anchors, 4), dtype=np.float32,\n",
    "                                ctx=device)\n",
    "        # 使用真实边界框来标记锚框的类别。\n",
    "        # 如果一个锚框没有被分配，标记其为背景（值为零）\n",
    "        indices_true = np.nonzero(anchors_bbox_map >= 0)[0]\n",
    "        bb_idx = anchors_bbox_map[indices_true]\n",
    "        class_labels[indices_true] = label[bb_idx, 0].astype('int32') + 1\n",
    "        assigned_bb[indices_true] = label[bb_idx, 1:]\n",
    "        # 偏移量转换\n",
    "        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\n",
    "        batch_offset.append(offset.reshape(-1))\n",
    "        batch_mask.append(bbox_mask.reshape(-1))\n",
    "        batch_class_labels.append(class_labels)\n",
    "    bbox_offset = d2l.stack(batch_offset)\n",
    "    bbox_mask = d2l.stack(batch_mask)\n",
    "    class_labels = d2l.stack(batch_class_labels)\n",
    "    return (bbox_offset, bbox_mask, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b30f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def multibox_target(anchors, labels):\n",
    "    \"\"\"使用真实边界框标记锚框\"\"\"\n",
    "    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n",
    "    batch_offset, batch_mask, batch_class_labels = [], [], []\n",
    "    device, num_anchors = anchors.device, anchors.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        label = labels[i, :, :]\n",
    "        anchors_bbox_map = assign_anchor_to_bbox(\n",
    "            label[:, 1:], anchors, device)\n",
    "        bbox_mask = ((anchors_bbox_map >= 0).float().unsqueeze(-1)).repeat(\n",
    "            1, 4)\n",
    "        # 将类标签和分配的边界框坐标初始化为零\n",
    "        class_labels = torch.zeros(num_anchors, dtype=torch.long,\n",
    "                                   device=device)\n",
    "        assigned_bb = torch.zeros((num_anchors, 4), dtype=torch.float32,\n",
    "                                  device=device)\n",
    "        # 使用真实边界框来标记锚框的类别。\n",
    "        # 如果一个锚框没有被分配，标记其为背景（值为零）\n",
    "        indices_true = torch.nonzero(anchors_bbox_map >= 0)\n",
    "        bb_idx = anchors_bbox_map[indices_true]\n",
    "        class_labels[indices_true] = label[bb_idx, 0].long() + 1\n",
    "        assigned_bb[indices_true] = label[bb_idx, 1:]\n",
    "        # 偏移量转换\n",
    "        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\n",
    "        batch_offset.append(offset.reshape(-1))\n",
    "        batch_mask.append(bbox_mask.reshape(-1))\n",
    "        batch_class_labels.append(class_labels)\n",
    "    bbox_offset = torch.stack(batch_offset)\n",
    "    bbox_mask = torch.stack(batch_mask)\n",
    "    class_labels = torch.stack(batch_class_labels)\n",
    "    return (bbox_offset, bbox_mask, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def multibox_target(anchors, labels):\n",
    "    \"\"\"使用真实边界框标记锚框\"\"\"\n",
    "    batch_size, anchors = labels.shape[0], anchors.squeeze(0)\n",
    "    batch_offset, batch_mask, batch_class_labels = [], [], []\n",
    "    place, num_anchors = anchors.place, anchors.shape[0]\n",
    "    for i in range(batch_size):\n",
    "        label = labels[i, :, :]\n",
    "        anchors_bbox_map = assign_anchor_to_bbox(\n",
    "            label[:, 1:], anchors, place)\n",
    "        bbox_mask = paddle.tile(paddle.to_tensor((anchors_bbox_map >= 0), dtype='float32').unsqueeze(-1), (1, 4))\n",
    "        # 将类标签和分配的边界框坐标初始化为零\n",
    "        class_labels = paddle.zeros(paddle.to_tensor(num_anchors), dtype=paddle.int64)\n",
    "        assigned_bb = paddle.zeros(paddle.to_tensor((num_anchors, 4)), dtype=paddle.float32)\n",
    "        # 使用真实边界框来标记锚框的类别。\n",
    "        # 如果一个锚框没有被分配，我们标记其为背景（值为零）\n",
    "        indices_true = paddle.nonzero(anchors_bbox_map >= 0).numpy()\n",
    "        bb_idx = anchors_bbox_map[indices_true].numpy()\n",
    "        class_labels[indices_true] = label.numpy()[bb_idx, 0][:] + 1\n",
    "        assigned_bb[indices_true] = label.numpy()[bb_idx, 1:]\n",
    "        class_labels = paddle.to_tensor(class_labels)\n",
    "        assigned_bb = paddle.to_tensor(assigned_bb)\n",
    "        # 偏移量转换\n",
    "        offset = offset_boxes(anchors, assigned_bb) * bbox_mask\n",
    "        batch_offset.append(offset.reshape([-1]))\n",
    "        batch_mask.append(bbox_mask.reshape([-1]))\n",
    "        batch_class_labels.append(class_labels)\n",
    "    bbox_offset = paddle.stack(batch_offset)\n",
    "    bbox_mask = paddle.stack(batch_mask)\n",
    "    class_labels = paddle.stack(batch_class_labels)\n",
    "    return (bbox_offset, bbox_mask, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde2248a",
   "metadata": {},
   "source": [
    "### 一个例子\n",
    "\n",
    "下面通过一个具体的例子来说明锚框标签。\n",
    "我们已经为加载图像中的狗和猫定义了真实边界框，其中第一个元素是类别（0代表狗，1代表猫），其余四个元素是左上角和右下角的$(x, y)$轴坐标（范围介于0和1之间）。\n",
    "我们还构建了五个锚框，用左上角和右下角的坐标进行标记：$A_0, \\ldots, A_4$（索引从0开始）。\n",
    "然后我们[**在图像中绘制这些真实边界框和锚框**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "ground_truth = d2l.tensor([[0, 0.1, 0.08, 0.52, 0.92],\n",
    "                         [1, 0.55, 0.2, 0.9, 0.88]])\n",
    "anchors = d2l.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],\n",
    "                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],\n",
    "                    [0.57, 0.3, 0.92, 0.9]])\n",
    "\n",
    "fig = d2l.plt.imshow(img)\n",
    "show_bboxes(fig.axes, ground_truth[:, 1:] * bbox_scale, ['dog', 'cat'], 'k')\n",
    "show_bboxes(fig.axes, anchors * bbox_scale, ['0', '1', '2', '3', '4']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45577f86",
   "metadata": {},
   "source": [
    "使用上面定义的`multibox_target`函数，我们可以[**根据狗和猫的真实边界框，标注这些锚框的分类和偏移量**]。\n",
    "在这个例子中，背景、狗和猫的类索引分别为0、1和2。\n",
    "下面我们为锚框和真实边界框样本添加一个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050725e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = multibox_target(np.expand_dims(anchors, axis=0),\n",
    "                         np.expand_dims(ground_truth, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594481e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "labels = multibox_target(anchors.unsqueeze(dim=0),\n",
    "                         ground_truth.unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbec7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "labels = multibox_target(anchors.unsqueeze(axis=0),\n",
    "                         ground_truth.unsqueeze(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00865e",
   "metadata": {},
   "source": [
    "返回的结果中有三个元素，都是张量格式。第三个元素包含标记的输入锚框的类别。\n",
    "\n",
    "让我们根据图像中的锚框和真实边界框的位置来分析下面返回的类别标签。\n",
    "首先，在所有的锚框和真实边界框配对中，锚框$A_4$与猫的真实边界框的IoU是最大的。\n",
    "因此，$A_4$的类别被标记为猫。\n",
    "去除包含$A_4$或猫的真实边界框的配对，在剩下的配对中，锚框$A_1$和狗的真实边界框有最大的IoU。\n",
    "因此，$A_1$的类别被标记为狗。\n",
    "接下来，我们需要遍历剩下的三个未标记的锚框：$A_0$、$A_2$和$A_3$。\n",
    "对于$A_0$，与其拥有最大IoU的真实边界框的类别是狗，但IoU低于预定义的阈值（0.5），因此该类别被标记为背景；\n",
    "对于$A_2$，与其拥有最大IoU的真实边界框的类别是猫，IoU超过阈值，所以类别被标记为猫；\n",
    "对于$A_3$，与其拥有最大IoU的真实边界框的类别是猫，但值低于阈值，因此该类别被标记为背景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "labels[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5ffbf",
   "metadata": {},
   "source": [
    "返回的第二个元素是掩码（mask）变量，形状为（批量大小，锚框数的四倍）。\n",
    "掩码变量中的元素与每个锚框的4个偏移量一一对应。\n",
    "由于我们不关心对背景的检测，负类的偏移量不应影响目标函数。\n",
    "通过元素乘法，掩码变量中的零将在计算目标函数之前过滤掉负类偏移量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053e9ef",
   "metadata": {},
   "source": [
    "返回的第一个元素包含了为每个锚框标记的四个偏移值。\n",
    "请注意，负类锚框的偏移量被标记为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9af307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433bed0",
   "metadata": {},
   "source": [
    "## 使用非极大值抑制预测边界框\n",
    ":label:`subsec_predicting-bounding-boxes-nms`\n",
    "\n",
    "在预测时，我们先为图像生成多个锚框，再为这些锚框一一预测类别和偏移量。\n",
    "一个*预测好的边界框*则根据其中某个带有预测偏移量的锚框而生成。\n",
    "下面我们实现了`offset_inverse`函数，该函数将锚框和偏移量预测作为输入，并[**应用逆偏移变换来返回预测的边界框坐标**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89786d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def offset_inverse(anchors, offset_preds):\n",
    "    \"\"\"根据带有预测偏移量的锚框来预测边界框\"\"\"\n",
    "    anc = d2l.box_corner_to_center(anchors)\n",
    "    pred_bbox_xy = (offset_preds[:, :2] * anc[:, 2:] / 10) + anc[:, :2]\n",
    "    pred_bbox_wh = d2l.exp(offset_preds[:, 2:] / 5) * anc[:, 2:]\n",
    "    pred_bbox = d2l.concat((pred_bbox_xy, pred_bbox_wh), axis=1)\n",
    "    predicted_bbox = d2l.box_center_to_corner(pred_bbox)\n",
    "    return predicted_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dedeb3",
   "metadata": {},
   "source": [
    "当有许多锚框时，可能会输出许多相似的具有明显重叠的预测边界框，都围绕着同一目标。\n",
    "为了简化输出，我们可以使用*非极大值抑制*（non-maximum suppression，NMS）合并属于同一目标的类似的预测边界框。\n",
    "\n",
    "以下是非极大值抑制的工作原理。\n",
    "对于一个预测边界框$B$，目标检测模型会计算每个类别的预测概率。\n",
    "假设最大的预测概率为$p$，则该概率所对应的类别$B$即为预测的类别。\n",
    "具体来说，我们将$p$称为预测边界框$B$的*置信度*（confidence）。\n",
    "在同一张图像中，所有预测的非背景边界框都按置信度降序排序，以生成列表$L$。然后我们通过以下步骤操作排序列表$L$。\n",
    "\n",
    "1. 从$L$中选取置信度最高的预测边界框$B_1$作为基准，然后将所有与$B_1$的IoU超过预定阈值$\\epsilon$的非基准预测边界框从$L$中移除。这时，$L$保留了置信度最高的预测边界框，去除了与其太过相似的其他预测边界框。简而言之，那些具有*非极大值*置信度的边界框被*抑制*了。\n",
    "1. 从$L$中选取置信度第二高的预测边界框$B_2$作为又一个基准，然后将所有与$B_2$的IoU大于$\\epsilon$的非基准预测边界框从$L$中移除。\n",
    "1. 重复上述过程，直到$L$中的所有预测边界框都曾被用作基准。此时，$L$中任意一对预测边界框的IoU都小于阈值$\\epsilon$；因此，没有一对边界框过于相似。\n",
    "1. 输出列表$L$中的所有预测边界框。\n",
    "\n",
    "[**以下`nms`函数按降序对置信度进行排序并返回其索引**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b03d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def nms(boxes, scores, iou_threshold):\n",
    "    \"\"\"对预测边界框的置信度进行排序\"\"\"\n",
    "    B = scores.argsort()[::-1]\n",
    "    keep = []  # 保留预测边界框的指标\n",
    "    while B.size > 0:\n",
    "        i = B[0]\n",
    "        keep.append(i)\n",
    "        if B.size == 1: break\n",
    "        iou = box_iou(boxes[i, :].reshape(-1, 4),\n",
    "                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)\n",
    "        inds = np.nonzero(iou <= iou_threshold)[0]\n",
    "        B = B[inds + 1]\n",
    "    return np.array(keep, dtype=np.int32, ctx=boxes.ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb7b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def nms(boxes, scores, iou_threshold):\n",
    "    \"\"\"对预测边界框的置信度进行排序\"\"\"\n",
    "    B = torch.argsort(scores, dim=-1, descending=True)\n",
    "    keep = []  # 保留预测边界框的指标\n",
    "    while B.numel() > 0:\n",
    "        i = B[0]\n",
    "        keep.append(i)\n",
    "        if B.numel() == 1: break\n",
    "        iou = box_iou(boxes[i, :].reshape(-1, 4),\n",
    "                      boxes[B[1:], :].reshape(-1, 4)).reshape(-1)\n",
    "        inds = torch.nonzero(iou <= iou_threshold).reshape(-1)\n",
    "        B = B[inds + 1]\n",
    "    return d2l.tensor(keep, device=boxes.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def nms(boxes, scores, iou_threshold):\n",
    "    \"\"\"对预测边界框的置信度进行排序\"\"\"\n",
    "    B = paddle.argsort(scores, axis=-1, descending=True)\n",
    "    keep = []  # 保留预测边界框的指标\n",
    "    while B.numel().item() > 0:\n",
    "        i = B[0]\n",
    "        keep.append(i.item())\n",
    "        if B.numel().item() == 1: break\n",
    "        iou = box_iou(boxes[i.numpy(), :].reshape([-1, 4]),\n",
    "                      paddle.to_tensor(boxes.numpy()[B[1:].numpy(), :]).reshape([-1, 4])).reshape([-1])\n",
    "        inds = paddle.nonzero(iou <= iou_threshold).numpy().reshape([-1])\n",
    "        B = paddle.to_tensor(B.numpy()[inds + 1])\n",
    "    return paddle.to_tensor(keep, place=boxes.place, dtype='int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ffcec",
   "metadata": {},
   "source": [
    "我们定义以下`multibox_detection`函数来[**将非极大值抑制应用于预测边界框**]。\n",
    "这里的实现有点复杂，请不要担心。我们将在实现之后，马上用一个具体的例子来展示它是如何工作的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec89932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,\n",
    "                       pos_threshold=0.009999999):\n",
    "    \"\"\"使用非极大值抑制来预测边界框\"\"\"\n",
    "    device, batch_size = cls_probs.ctx, cls_probs.shape[0]\n",
    "    anchors = np.squeeze(anchors, axis=0)\n",
    "    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]\n",
    "    out = []\n",
    "    for i in range(batch_size):\n",
    "        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)\n",
    "        conf, class_id = np.max(cls_prob[1:], 0), np.argmax(cls_prob[1:], 0)\n",
    "        predicted_bb = offset_inverse(anchors, offset_pred)\n",
    "        keep = nms(predicted_bb, conf, nms_threshold)\n",
    "\n",
    "        # 找到所有的non_keep索引，并将类设置为背景\n",
    "        all_idx = np.arange(num_anchors, dtype=np.int32, ctx=device)\n",
    "        combined = d2l.concat((keep, all_idx))\n",
    "        unique, counts = np.unique(combined, return_counts=True)\n",
    "        non_keep = unique[counts == 1]\n",
    "        all_id_sorted = d2l.concat((keep, non_keep))\n",
    "        class_id[non_keep] = -1\n",
    "        class_id = class_id[all_id_sorted].astype('float32')\n",
    "        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]\n",
    "        # pos_threshold是一个用于非背景预测的阈值\n",
    "        below_min_idx = (conf < pos_threshold)\n",
    "        class_id[below_min_idx] = -1\n",
    "        conf[below_min_idx] = 1 - conf[below_min_idx]\n",
    "        pred_info = d2l.concat((np.expand_dims(class_id, axis=1),\n",
    "                                np.expand_dims(conf, axis=1),\n",
    "                                predicted_bb), axis=1)\n",
    "        out.append(pred_info)\n",
    "    return d2l.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ec257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,\n",
    "                       pos_threshold=0.009999999):\n",
    "    \"\"\"使用非极大值抑制来预测边界框\"\"\"\n",
    "    device, batch_size = cls_probs.device, cls_probs.shape[0]\n",
    "    anchors = anchors.squeeze(0)\n",
    "    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]\n",
    "    out = []\n",
    "    for i in range(batch_size):\n",
    "        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape(-1, 4)\n",
    "        conf, class_id = torch.max(cls_prob[1:], 0)\n",
    "        predicted_bb = offset_inverse(anchors, offset_pred)\n",
    "        keep = nms(predicted_bb, conf, nms_threshold)\n",
    "\n",
    "        # 找到所有的non_keep索引，并将类设置为背景\n",
    "        all_idx = torch.arange(num_anchors, dtype=torch.long, device=device)\n",
    "        combined = torch.cat((keep, all_idx))\n",
    "        uniques, counts = combined.unique(return_counts=True)\n",
    "        non_keep = uniques[counts == 1]\n",
    "        all_id_sorted = torch.cat((keep, non_keep))\n",
    "        class_id[non_keep] = -1\n",
    "        class_id = class_id[all_id_sorted]\n",
    "        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]\n",
    "        # pos_threshold是一个用于非背景预测的阈值\n",
    "        below_min_idx = (conf < pos_threshold)\n",
    "        class_id[below_min_idx] = -1\n",
    "        conf[below_min_idx] = 1 - conf[below_min_idx]\n",
    "        pred_info = torch.cat((class_id.unsqueeze(1),\n",
    "                               conf.unsqueeze(1),\n",
    "                               predicted_bb), dim=1)\n",
    "        out.append(pred_info)\n",
    "    return d2l.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5,\n",
    "                       pos_threshold=0.009999999):\n",
    "    \"\"\"使用非极大值抑制来预测边界框\"\"\"\n",
    "    batch_size = cls_probs.shape[0]\n",
    "    anchors = anchors.squeeze(0)\n",
    "    num_classes, num_anchors = cls_probs.shape[1], cls_probs.shape[2]\n",
    "    out = []\n",
    "    for i in range(batch_size):\n",
    "        cls_prob, offset_pred = cls_probs[i], offset_preds[i].reshape([-1, 4])\n",
    "        conf = paddle.max(cls_prob[1:], 0)\n",
    "        class_id = paddle.argmax(cls_prob[1:], 0)\n",
    "        predicted_bb = offset_inverse(anchors, offset_pred)\n",
    "        keep = nms(predicted_bb, conf, nms_threshold)\n",
    "\n",
    "        # 找到所有的non_keep索引，并将类设置为背景\n",
    "        all_idx = paddle.arange(num_anchors, dtype='int64')\n",
    "        combined = paddle.concat((keep, all_idx))\n",
    "        uniques, counts = combined.unique(return_counts=True)\n",
    "        non_keep = uniques[counts == 1]\n",
    "        all_id_sorted = paddle.concat([keep, non_keep])\n",
    "        class_id[non_keep.numpy()] = -1\n",
    "        class_id = class_id[all_id_sorted]\n",
    "        conf, predicted_bb = conf[all_id_sorted], predicted_bb[all_id_sorted]\n",
    "        # pos_threshold是一个用于非背景预测的阈值\n",
    "        below_min_idx = (conf < pos_threshold)\n",
    "        class_id[below_min_idx.numpy()] = -1\n",
    "        conf[below_min_idx.numpy()] = 1 - conf[below_min_idx.numpy()]\n",
    "        pred_info = paddle.concat((paddle.to_tensor(class_id, dtype='float32').unsqueeze(1),\n",
    "                               paddle.to_tensor(conf, dtype='float32').unsqueeze(1),\n",
    "                               predicted_bb), axis=1)\n",
    "        out.append(pred_info)\n",
    "    return paddle.stack(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e41322",
   "metadata": {},
   "source": [
    "现在让我们[**将上述算法应用到一个带有四个锚框的具体示例中**]。\n",
    "为简单起见，我们假设预测的偏移量都是零，这意味着预测的边界框即是锚框。\n",
    "对于背景、狗和猫其中的每个类，我们还定义了它的预测概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d00874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "anchors = d2l.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],\n",
    "                      [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])\n",
    "offset_preds = d2l.tensor([0] * d2l.size(anchors))\n",
    "cls_probs = d2l.tensor([[0] * 4,  # 背景的预测概率\n",
    "                      [0.9, 0.8, 0.7, 0.1],  # 狗的预测概率\n",
    "                      [0.1, 0.2, 0.3, 0.9]])  # 猫的预测概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c593ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "anchors = d2l.tensor([[0.1, 0.08, 0.52, 0.92], [0.08, 0.2, 0.56, 0.95],\n",
    "                      [0.15, 0.3, 0.62, 0.91], [0.55, 0.2, 0.9, 0.88]])\n",
    "offset_preds = d2l.tensor([0] * anchors.numel().item())\n",
    "cls_probs = d2l.tensor([[0] * 4,  # 背景的预测概率\n",
    "                      [0.9, 0.8, 0.7, 0.1],  # 狗的预测概率\n",
    "                      [0.1, 0.2, 0.3, 0.9]])  # 猫的预测概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452bafc",
   "metadata": {},
   "source": [
    "我们可以[**在图像上绘制这些预测边界框和置信度**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ead619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "fig = d2l.plt.imshow(img)\n",
    "show_bboxes(fig.axes, anchors * bbox_scale,\n",
    "            ['dog=0.9', 'dog=0.8', 'dog=0.7', 'cat=0.9'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6584d6",
   "metadata": {},
   "source": [
    "现在我们可以调用`multibox_detection`函数来执行非极大值抑制，其中阈值设置为0.5。\n",
    "请注意，我们在示例的张量输入中添加了维度。\n",
    "\n",
    "我们可以看到[**返回结果的形状是（批量大小，锚框的数量，6）**]。\n",
    "最内层维度中的六个元素提供了同一预测边界框的输出信息。\n",
    "第一个元素是预测的类索引，从0开始（0代表狗，1代表猫），值-1表示背景或在非极大值抑制中被移除了。\n",
    "第二个元素是预测的边界框的置信度。\n",
    "其余四个元素分别是预测边界框左上角和右下角的$(x, y)$轴坐标（范围介于0和1之间）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d1756",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = multibox_detection(np.expand_dims(cls_probs, axis=0),\n",
    "                            np.expand_dims(offset_preds, axis=0),\n",
    "                            np.expand_dims(anchors, axis=0),\n",
    "                            nms_threshold=0.5)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de81917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "output = multibox_detection(cls_probs.unsqueeze(dim=0),\n",
    "                            offset_preds.unsqueeze(dim=0),\n",
    "                            anchors.unsqueeze(dim=0),\n",
    "                            nms_threshold=0.5)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "output = multibox_detection(cls_probs.unsqueeze(axis=0),\n",
    "                            offset_preds.unsqueeze(axis=0),\n",
    "                            anchors.unsqueeze(axis=0),\n",
    "                            nms_threshold=0.5)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d7c87",
   "metadata": {},
   "source": [
    "删除-1类别（背景）的预测边界框后，我们可以[**输出由非极大值抑制保存的最终预测边界框**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad73773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "fig = d2l.plt.imshow(img)\n",
    "for i in d2l.numpy(output[0]):\n",
    "    if i[0] == -1:\n",
    "        continue\n",
    "    label = ('dog=', 'cat=')[int(i[0])] + str(i[1])\n",
    "    show_bboxes(fig.axes, [d2l.tensor(i[2:]) * bbox_scale], label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be4d1a",
   "metadata": {},
   "source": [
    "实践中，在执行非极大值抑制前，我们甚至可以将置信度较低的预测边界框移除，从而减少此算法中的计算量。\n",
    "我们也可以对非极大值抑制的输出结果进行后处理。例如，只保留置信度更高的结果作为最终输出。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 我们以图像的每个像素为中心生成不同形状的锚框。\n",
    "* 交并比（IoU）也被称为杰卡德系数，用于衡量两个边界框的相似性。它是相交面积与相并面积的比率。\n",
    "* 在训练集中，我们需要给每个锚框两种类型的标签。一个是与锚框中目标检测的类别，另一个是锚框真实相对于边界框的偏移量。\n",
    "* 预测期间可以使用非极大值抑制（NMS）来移除类似的预测边界框，从而简化输出。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 在`multibox_prior`函数中更改`sizes`和`ratios`的值。生成的锚框有什么变化？\n",
    "1. 构建并可视化两个IoU为0.5的边界框。它们是怎样重叠的？\n",
    "1. 在 :numref:`subsec_labeling-anchor-boxes`和 :numref:`subsec_predicting-bounding-boxes-nms`中修改变量`anchors`，结果如何变化？\n",
    "1. 非极大值抑制是一种贪心算法，它通过*移除*来抑制预测的边界框。是否存在一种可能，被移除的一些框实际上是有用的？如何修改这个算法来柔和地抑制？可以参考Soft-NMS :cite:`Bodla.Singh.Chellappa.ea.2017`。\n",
    "1. 如果非手动，非最大限度的抑制可以被学习吗？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2945)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2946)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11804)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e0db4",
   "metadata": {},
   "source": [
    "# 多尺度目标检测\n",
    ":label:`sec_multiscale-object-detection`\n",
    "\n",
    "在 :numref:`sec_anchor`中，我们以输入图像的每个像素为中心，生成了多个锚框。\n",
    "基本而言，这些锚框代表了图像不同区域的样本。\n",
    "然而，如果为每个像素都生成的锚框，我们最终可能会得到太多需要计算的锚框。\n",
    "想象一个$561 \\times 728$的输入图像，如果以每个像素为中心生成五个形状不同的锚框，就需要在图像上标记和预测超过200万个锚框（$561 \\times 728 \\times 5$）。\n",
    "\n",
    "## 多尺度锚框\n",
    ":label:`subsec_multiscale-anchor-boxes`\n",
    "\n",
    "减少图像上的锚框数量并不困难。\n",
    "比如，我们可以在输入图像中均匀采样一小部分像素，并以它们为中心生成锚框。\n",
    "此外，在不同尺度下，我们可以生成不同数量和不同大小的锚框。\n",
    "直观地说，比起较大的目标，较小的目标在图像上出现的可能性更多样。\n",
    "例如，$1 \\times 1$、$1 \\times 2$和$2 \\times 2$的目标可以分别以4、2和1种可能的方式出现在$2 \\times 2$图像上。\n",
    "因此，当使用较小的锚框检测较小的物体时，我们可以采样更多的区域，而对于较大的物体，我们可以采样较少的区域。\n",
    "\n",
    "为了演示如何在多个尺度下生成锚框，让我们先读取一张图像。\n",
    "它的高度和宽度分别为561和728像素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ef644",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import image, np, npx\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "img = image.imread('../img/catdog.jpg')\n",
    "h, w = img.shape[:2]\n",
    "h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f5cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "\n",
    "img = d2l.plt.imread('../img/catdog.jpg')\n",
    "h, w = img.shape[:2]\n",
    "h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "\n",
    "img = d2l.plt.imread('../img/catdog.jpg')\n",
    "h, w = img.shape[:2]\n",
    "h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330f9f5",
   "metadata": {},
   "source": [
    "回想一下，在 :numref:`sec_conv_layer`中，我们将卷积图层的二维数组输出称为特征图。\n",
    "通过定义特征图的形状，我们可以确定任何图像上均匀采样锚框的中心。\n",
    "\n",
    "`display_anchors`函数定义如下。\n",
    "我们[**在特征图（`fmap`）上生成锚框（`anchors`），每个单位（像素）作为锚框的中心**]。\n",
    "由于锚框中的$(x, y)$轴坐标值（`anchors`）已经被除以特征图（`fmap`）的宽度和高度，因此这些值介于0和1之间，表示特征图中锚框的相对位置。\n",
    "\n",
    "由于锚框（`anchors`）的中心分布于特征图（`fmap`）上的所有单位，因此这些中心必须根据其相对空间位置在任何输入图像上*均匀*分布。\n",
    "更具体地说，给定特征图的宽度和高度`fmap_w`和`fmap_h`，以下函数将*均匀地*对任何输入图像中`fmap_h`行和`fmap_w`列中的像素进行采样。\n",
    "以这些均匀采样的像素为中心，将会生成大小为`s`（假设列表`s`的长度为1）且宽高比（`ratios`）不同的锚框。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfec40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_anchors(fmap_w, fmap_h, s):\n",
    "    d2l.set_figsize()\n",
    "    # 前两个维度上的值不影响输出\n",
    "    fmap = np.zeros((1, 10, fmap_h, fmap_w))\n",
    "    anchors = npx.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])\n",
    "    bbox_scale = np.array((w, h, w, h))\n",
    "    d2l.show_bboxes(d2l.plt.imshow(img.asnumpy()).axes,\n",
    "                    anchors[0] * bbox_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def display_anchors(fmap_w, fmap_h, s):\n",
    "    d2l.set_figsize()\n",
    "    # 前两个维度上的值不影响输出\n",
    "    fmap = d2l.zeros((1, 10, fmap_h, fmap_w))\n",
    "    anchors = d2l.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])\n",
    "    bbox_scale = d2l.tensor((w, h, w, h))\n",
    "    d2l.show_bboxes(d2l.plt.imshow(img).axes,\n",
    "                    anchors[0] * bbox_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def display_anchors(fmap_w, fmap_h, s):\n",
    "    d2l.set_figsize()\n",
    "    # 前两个维度上的值不影响输出\n",
    "    fmap = paddle.zeros(shape=[1, 10, fmap_h, fmap_w])\n",
    "    anchors = d2l.multibox_prior(fmap, sizes=s, ratios=[1, 2, 0.5])\n",
    "    bbox_scale = paddle.to_tensor((w, h, w, h))\n",
    "    d2l.show_bboxes(d2l.plt.imshow(img).axes,\n",
    "                    anchors[0] * bbox_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5be98",
   "metadata": {},
   "source": [
    "首先，让我们考虑[**探测小目标**]。\n",
    "为了在显示时更容易分辨，在这里具有不同中心的锚框不会重叠：\n",
    "锚框的尺度设置为0.15，特征图的高度和宽度设置为4。\n",
    "我们可以看到，图像上4行和4列的锚框的中心是均匀分布的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ad771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "display_anchors(fmap_w=4, fmap_h=4, s=[0.15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0601850",
   "metadata": {},
   "source": [
    "然后，我们[**将特征图的高度和宽度减小一半，然后使用较大的锚框来检测较大的目标**]。\n",
    "当尺度设置为0.4时，一些锚框将彼此重叠。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a26d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "display_anchors(fmap_w=2, fmap_h=2, s=[0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5bd20",
   "metadata": {},
   "source": [
    "最后，我们进一步[**将特征图的高度和宽度减小一半，然后将锚框的尺度增加到0.8**]。\n",
    "此时，锚框的中心即是图像的中心。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "display_anchors(fmap_w=1, fmap_h=1, s=[0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020dd4c7",
   "metadata": {},
   "source": [
    "## 多尺度检测\n",
    "\n",
    "既然我们已经生成了多尺度的锚框，我们就将使用它们来检测不同尺度下各种大小的目标。\n",
    "下面，我们介绍一种基于CNN的多尺度目标检测方法，将在 :numref:`sec_ssd`中实现。\n",
    "\n",
    "在某种规模上，假设我们有$c$张形状为$h \\times w$的特征图。\n",
    "使用 :numref:`subsec_multiscale-anchor-boxes`中的方法，我们生成了$hw$组锚框，其中每组都有$a$个中心相同的锚框。\n",
    "例如，在 :numref:`subsec_multiscale-anchor-boxes`实验的第一个尺度上，给定10个（通道数量）$4 \\times 4$的特征图，我们生成了16组锚框，每组包含3个中心相同的锚框。\n",
    "接下来，每个锚框都根据真实值边界框来标记了类和偏移量。\n",
    "在当前尺度下，目标检测模型需要预测输入图像上$hw$组锚框类别和偏移量，其中不同组锚框具有不同的中心。\n",
    "\n",
    "\n",
    "假设此处的$c$张特征图是CNN基于输入图像的正向传播算法获得的中间输出。\n",
    "既然每张特征图上都有$hw$个不同的空间位置，那么相同空间位置可以看作含有$c$个单元。\n",
    "根据 :numref:`sec_conv_layer`中对感受野的定义，特征图在相同空间位置的$c$个单元在输入图像上的感受野相同：\n",
    "它们表征了同一感受野内的输入图像信息。\n",
    "因此，我们可以将特征图在同一空间位置的$c$个单元变换为使用此空间位置生成的$a$个锚框类别和偏移量。\n",
    "本质上，我们用输入图像在某个感受野区域内的信息，来预测输入图像上与该区域位置相近的锚框类别和偏移量。\n",
    "\n",
    "当不同层的特征图在输入图像上分别拥有不同大小的感受野时，它们可以用于检测不同大小的目标。\n",
    "例如，我们可以设计一个神经网络，其中靠近输出层的特征图单元具有更宽的感受野，这样它们就可以从输入图像中检测到较大的目标。\n",
    "\n",
    "简言之，我们可以利用深层神经网络在多个层次上对图像进行分层表示，从而实现多尺度目标检测。\n",
    "在 :numref:`sec_ssd`，我们将通过一个具体的例子来说明它是如何工作的。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 在多个尺度下，我们可以生成不同尺寸的锚框来检测不同尺寸的目标。\n",
    "* 通过定义特征图的形状，我们可以决定任何图像上均匀采样的锚框的中心。\n",
    "* 我们使用输入图像在某个感受野区域内的信息，来预测输入图像上与该区域位置相近的锚框类别和偏移量。\n",
    "* 我们可以通过深入学习，在多个层次上的图像分层表示进行多尺度目标检测。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 根据我们在 :numref:`sec_alexnet`中的讨论，深度神经网络学习图像特征级别抽象层次，随网络深度的增加而升级。在多尺度目标检测中，不同尺度的特征映射是否对应于不同的抽象层次？为什么？\n",
    "1. 在 :numref:`subsec_multiscale-anchor-boxes`中的实验里的第一个尺度（`fmap_w=4, fmap_h=4`）下，生成可能重叠的均匀分布的锚框。\n",
    "1. 给定形状为$1 \\times c \\times h \\times w$的特征图变量，其中$c$、$h$和$w$分别是特征图的通道数、高度和宽度。怎样才能将这个变量转换为锚框类别和偏移量？输出的形状是什么？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2947)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2948)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11805)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc15e10",
   "metadata": {},
   "source": [
    "# 目标检测数据集\n",
    ":label:`sec_object-detection-dataset`\n",
    "\n",
    "目标检测领域没有像MNIST和Fashion-MNIST那样的小数据集。\n",
    "为了快速测试目标检测模型，[**我们收集并标记了一个小型数据集**]。\n",
    "首先，我们拍摄了一组香蕉的照片，并生成了1000张不同角度和大小的香蕉图像。\n",
    "然后，我们在一些背景图片的随机位置上放一张香蕉的图像。\n",
    "最后，我们在图片上为这些香蕉标记了边界框。\n",
    "\n",
    "## [**下载数据集**]\n",
    "\n",
    "包含所有图像和CSV标签文件的香蕉检测数据集可以直接从互联网下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, image, np, npx\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd8cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import paddle\n",
    "import paddle.vision as paddlevision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552225c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "d2l.DATA_HUB['banana-detection'] = (\n",
    "    d2l.DATA_URL + 'banana-detection.zip',\n",
    "    '5de26c8fce5ccdea9f91267273464dc968d20d72')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9f726",
   "metadata": {},
   "source": [
    "## 读取数据集\n",
    "\n",
    "通过`read_data_bananas`函数，我们[**读取香蕉检测数据集**]。\n",
    "该数据集包括一个的CSV文件，内含目标类别标签和位于左上角和右下角的真实边界框坐标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3483640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def read_data_bananas(is_train=True):\n",
    "    \"\"\"读取香蕉检测数据集中的图像和标签\"\"\"\n",
    "    data_dir = d2l.download_extract('banana-detection')\n",
    "    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train\n",
    "                             else 'bananas_val', 'label.csv')\n",
    "    csv_data = pd.read_csv(csv_fname)\n",
    "    csv_data = csv_data.set_index('img_name')\n",
    "    images, targets = [], []\n",
    "    for img_name, target in csv_data.iterrows():\n",
    "        images.append(image.imread(\n",
    "            os.path.join(data_dir, 'bananas_train' if is_train else\n",
    "                         'bananas_val', 'images', f'{img_name}')))\n",
    "        # 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y），\n",
    "        # 其中所有图像都具有相同的香蕉类（索引为0）\n",
    "        targets.append(list(target))\n",
    "    return images, np.expand_dims(np.array(targets), 1) / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee588922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def read_data_bananas(is_train=True):\n",
    "    \"\"\"读取香蕉检测数据集中的图像和标签\"\"\"\n",
    "    data_dir = d2l.download_extract('banana-detection')\n",
    "    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train\n",
    "                             else 'bananas_val', 'label.csv')\n",
    "    csv_data = pd.read_csv(csv_fname)\n",
    "    csv_data = csv_data.set_index('img_name')\n",
    "    images, targets = [], []\n",
    "    for img_name, target in csv_data.iterrows():\n",
    "        images.append(torchvision.io.read_image(\n",
    "            os.path.join(data_dir, 'bananas_train' if is_train else\n",
    "                         'bananas_val', 'images', f'{img_name}')))\n",
    "        # 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y），\n",
    "        # 其中所有图像都具有相同的香蕉类（索引为0）\n",
    "        targets.append(list(target))\n",
    "    return images, torch.tensor(targets).unsqueeze(1) / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602bd5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def read_data_bananas(is_train=True):\n",
    "    \"\"\"读取香蕉检测数据集中的图像和标签\"\"\"\n",
    "    data_dir = d2l.download_extract('banana-detection')\n",
    "    csv_fname = os.path.join(data_dir, 'bananas_train' if is_train\n",
    "                             else 'bananas_val', 'label.csv')\n",
    "    csv_data = pd.read_csv(csv_fname)\n",
    "    csv_data = csv_data.set_index('img_name')\n",
    "    images, targets = [], []\n",
    "    for img_name, target in csv_data.iterrows():\n",
    "        paddle.vision.set_image_backend('cv2')\n",
    "        images.append(paddlevision.image_load(os.path.join(data_dir, 'bananas_train' if is_train else\n",
    "        'bananas_val', 'images', f'{img_name}'))[..., ::-1])\n",
    "        # 这里的target包含（类别，左上角x，左上角y，右下角x，右下角y）\n",
    "        # 其中所有图像都具有相同的香蕉类（索引为0）\n",
    "        targets.append(list(target))\n",
    "    return images, paddle.to_tensor(targets).unsqueeze(1) / 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec329382",
   "metadata": {},
   "source": [
    "通过使用`read_data_bananas`函数读取图像和标签，以下`BananasDataset`类别将允许我们[**创建一个自定义`Dataset`实例**]来加载香蕉检测数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class BananasDataset(gluon.data.Dataset):\n",
    "    \"\"\"一个用于加载香蕉检测数据集的自定义数据集\"\"\"\n",
    "    def __init__(self, is_train):\n",
    "        self.features, self.labels = read_data_bananas(is_train)\n",
    "        print('read ' + str(len(self.features)) + (f' training examples' if\n",
    "              is_train else f' validation examples'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.features[idx].astype('float32').transpose(2, 0, 1),\n",
    "                self.labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68159810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class BananasDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"一个用于加载香蕉检测数据集的自定义数据集\"\"\"\n",
    "    def __init__(self, is_train):\n",
    "        self.features, self.labels = read_data_bananas(is_train)\n",
    "        print('read ' + str(len(self.features)) + (f' training examples' if\n",
    "              is_train else f' validation examples'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.features[idx].float(), self.labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e502259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "class BananasDataset(paddle.io.Dataset):\n",
    "    \"\"\"一个用于加载香蕉检测数据集的自定义数据集\"\"\"\n",
    "    def __init__(self, is_train):\n",
    "        self.features, self.labels = read_data_bananas(is_train)\n",
    "        print('read ' + str(len(self.features)) + (f' training examples' if\n",
    "              is_train else f' validation examples'))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (paddle.to_tensor(self.features[idx], dtype='float32').transpose([2, 0, 1]), self.labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42d21a2",
   "metadata": {},
   "source": [
    "最后，我们定义`load_data_bananas`函数，来[**为训练集和测试集返回两个数据加载器实例**]。对于测试集，无须按随机顺序读取它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_bananas(batch_size):\n",
    "    \"\"\"加载香蕉检测数据集\"\"\"\n",
    "    train_iter = gluon.data.DataLoader(BananasDataset(is_train=True),\n",
    "                                       batch_size, shuffle=True)\n",
    "    val_iter = gluon.data.DataLoader(BananasDataset(is_train=False),\n",
    "                                     batch_size)\n",
    "    return train_iter, val_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def load_data_bananas(batch_size):\n",
    "    \"\"\"加载香蕉检测数据集\"\"\"\n",
    "    train_iter = torch.utils.data.DataLoader(BananasDataset(is_train=True),\n",
    "                                             batch_size, shuffle=True)\n",
    "    val_iter = torch.utils.data.DataLoader(BananasDataset(is_train=False),\n",
    "                                           batch_size)\n",
    "    return train_iter, val_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e95b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def load_data_bananas(batch_size):\n",
    "    \"\"\"加载香蕉检测数据集\"\"\"\n",
    "    train_iter = paddle.io.DataLoader(BananasDataset(is_train=True),\n",
    "                                      batch_size=batch_size, return_list=True, shuffle=True)\n",
    "    val_iter = paddle.io.DataLoader(BananasDataset(is_train=False),\n",
    "                                    batch_size=batch_size, return_list=True)\n",
    "    return train_iter, val_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcb6b7a",
   "metadata": {},
   "source": [
    "让我们[**读取一个小批量，并打印其中的图像和标签的形状**]。\n",
    "图像的小批量的形状为（批量大小、通道数、高度、宽度），看起来很眼熟：它与我们之前图像分类任务中的相同。\n",
    "标签的小批量的形状为（批量大小，$m$，5），其中$m$是数据集的任何图像中边界框可能出现的最大数量。\n",
    "\n",
    "小批量计算虽然高效，但它要求每张图像含有相同数量的边界框，以便放在同一个批量中。\n",
    "通常来说，图像可能拥有不同数量个边界框；因此，在达到$m$之前，边界框少于$m$的图像将被非法边界框填充。\n",
    "这样，每个边界框的标签将被长度为5的数组表示。\n",
    "数组中的第一个元素是边界框中对象的类别，其中-1表示用于填充的非法边界框。\n",
    "数组的其余四个元素是边界框左上角和右下角的（$x$，$y$）坐标值（值域在0～1之间）。\n",
    "对于香蕉数据集而言，由于每张图像上只有一个边界框，因此$m=1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccedf663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "batch_size, edge_size = 32, 256\n",
    "train_iter, _ = load_data_bananas(batch_size)\n",
    "batch = next(iter(train_iter))\n",
    "batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f00b8a",
   "metadata": {},
   "source": [
    "## [**演示**]\n",
    "\n",
    "让我们展示10幅带有真实边界框的图像。\n",
    "我们可以看到在所有这些图像中香蕉的旋转角度、大小和位置都有所不同。\n",
    "当然，这只是一个简单的人工数据集，实践中真实世界的数据集通常要复杂得多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = (batch[0][0:10].transpose(0, 2, 3, 1)) / 255\n",
    "axes = d2l.show_images(imgs, 2, 5, scale=2)\n",
    "for ax, label in zip(axes, batch[1][0:10]):\n",
    "    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83cebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "imgs = (batch[0][0:10].permute(0, 2, 3, 1)) / 255\n",
    "axes = d2l.show_images(imgs, 2, 5, scale=2)\n",
    "for ax, label in zip(axes, batch[1][0:10]):\n",
    "    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a92d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "imgs = (batch[0][0:10].transpose([0, 2, 3, 1])) / 255\n",
    "axes = d2l.show_images(imgs, 2, 5, scale=2)\n",
    "for ax, label in zip(axes, batch[1][0:10]):\n",
    "    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], colors=['w'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad15b14",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们收集的香蕉检测数据集可用于演示目标检测模型。\n",
    "* 用于目标检测的数据加载与图像分类的数据加载类似。但是，在目标检测中，标签还包含真实边界框的信息，它不出现在图像分类中。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 在香蕉检测数据集中演示其他带有真实边界框的图像。它们在边界框和目标方面有什么不同？\n",
    "1. 假设我们想要将数据增强（例如随机裁剪）应用于目标检测。它与图像分类中的有什么不同？提示：如果裁剪的图像只包含物体的一小部分会怎样？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/3203)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/3202)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11806)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4edd9e",
   "metadata": {},
   "source": [
    "# 单发多框检测（SSD）\n",
    ":label:`sec_ssd`\n",
    "\n",
    "在 :numref:`sec_bbox`— :numref:`sec_object-detection-dataset`中，我们分别介绍了边界框、锚框、多尺度目标检测和用于目标检测的数据集。\n",
    "现在我们已经准备好使用这样的背景知识来设计一个目标检测模型：单发多框检测（SSD） :cite:`Liu.Anguelov.Erhan.ea.2016`。\n",
    "该模型简单、快速且被广泛使用。尽管这只是其中一种目标检测模型，但本节中的一些设计原则和实现细节也适用于其他模型。\n",
    "\n",
    "## 模型\n",
    "\n",
    " :numref:`fig_ssd`描述了单发多框检测模型的设计。\n",
    "此模型主要由基础网络组成，其后是几个多尺度特征块。\n",
    "基本网络用于从输入图像中提取特征，因此它可以使用深度卷积神经网络。\n",
    "单发多框检测论文中选用了在分类层之前截断的VGG :cite:`Liu.Anguelov.Erhan.ea.2016`，现在也常用ResNet替代。\n",
    "我们可以设计基础网络，使它输出的高和宽较大。\n",
    "这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。\n",
    "接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔。\n",
    "\n",
    "回想一下在 :numref:`sec_multiscale-object-detection`中，通过深度神经网络分层表示图像的多尺度目标检测的设计。\n",
    "由于接近 :numref:`fig_ssd`顶部的多尺度特征图较小，但具有较大的感受野，它们适合检测较少但较大的物体。\n",
    "简而言之，通过多尺度特征块，单发多框检测生成不同大小的锚框，并通过预测边界框的类别和偏移量来检测大小不同的目标，因此这是一个多尺度目标检测模型。\n",
    "\n",
    "![单发多框检测模型主要由一个基础网络块和若干多尺度特征块串联而成。](../img/ssd.svg)\n",
    ":label:`fig_ssd`\n",
    "\n",
    "在下面，我们将介绍 :numref:`fig_ssd`中不同块的实施细节。\n",
    "首先，我们将讨论如何实施类别和边界框预测。\n",
    "\n",
    "### [**类别预测层**]\n",
    "\n",
    "设目标类别的数量为$q$。这样一来，锚框有$q+1$个类别，其中0类是背景。\n",
    "在某个尺度下，设特征图的高和宽分别为$h$和$w$。\n",
    "如果以其中每个单元为中心生成$a$个锚框，那么我们需要对$hwa$个锚框进行分类。\n",
    "如果使用全连接层作为输出，很容易导致模型参数过多。\n",
    "回忆 :numref:`sec_nin`一节介绍的使用卷积层的通道来输出类别预测的方法，\n",
    "单发多框检测采用同样的方法来降低模型复杂度。\n",
    "\n",
    "具体来说，类别预测层使用一个保持输入高和宽的卷积层。\n",
    "这样一来，输出和输入在特征图宽和高上的空间坐标一一对应。\n",
    "考虑输出和输入同一空间坐标（$x$、$y$）：输出特征图上（$x$、$y$）坐标的通道里包含了以输入特征图（$x$、$y$）坐标为中心生成的所有锚框的类别预测。\n",
    "因此输出通道数为$a(q+1)$，其中索引为$i(q+1) + j$（$0 \\leq j \\leq q$）的通道代表了索引为$i$的锚框有关类别索引为$j$的预测。\n",
    "\n",
    "在下面，我们定义了这样一个类别预测层，通过参数`num_anchors`和`num_classes`分别指定了$a$和$q$。\n",
    "该图层使用填充为1的$3\\times3$的卷积层。此卷积层的输入和输出的宽度和高度保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca3aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, image, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "def cls_predictor(num_anchors, num_classes):\n",
    "    return nn.Conv2D(num_anchors * (num_classes + 1), kernel_size=3,\n",
    "                     padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6825aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def cls_predictor(num_inputs, num_anchors, num_classes):\n",
    "    return nn.Conv2d(num_inputs, num_anchors * (num_classes + 1),\n",
    "                     kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4904689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "from paddle import nn\n",
    "from paddle.nn import functional as F\n",
    "import paddle.vision as paddlevision\n",
    "\n",
    "def cls_predictor(num_inputs, num_anchors, num_classes):\n",
    "    return nn.Conv2D(num_inputs, num_anchors * (num_classes + 1),\n",
    "                     kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5bc14",
   "metadata": {},
   "source": [
    "### (**边界框预测层**)\n",
    "\n",
    "边界框预测层的设计与类别预测层的设计类似。\n",
    "唯一不同的是，这里需要为每个锚框预测4个偏移量，而不是$q+1$个类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_predictor(num_anchors):\n",
    "    return nn.Conv2D(num_anchors * 4, kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def bbox_predictor(num_inputs, num_anchors):\n",
    "    return nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d07ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def bbox_predictor(num_inputs, num_anchors):\n",
    "    return nn.Conv2D(num_inputs, num_anchors * 4, kernel_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d9ae4e",
   "metadata": {},
   "source": [
    "### [**连结多尺度的预测**]\n",
    "\n",
    "正如我们所提到的，单发多框检测使用多尺度特征图来生成锚框并预测其类别和偏移量。\n",
    "在不同的尺度下，特征图的形状或以同一单元为中心的锚框的数量可能会有所不同。\n",
    "因此，不同尺度下预测输出的形状可能会有所不同。\n",
    "\n",
    "在以下示例中，我们为同一个小批量构建两个不同比例（`Y1`和`Y2`）的特征图，其中`Y2`的高度和宽度是`Y1`的一半。\n",
    "以类别预测为例，假设`Y1`和`Y2`的每个单元分别生成了$5$个和$3$个锚框。\n",
    "进一步假设目标类别的数量为$10$，对于特征图`Y1`和`Y2`，类别预测输出中的通道数分别为$5\\times(10+1)=55$和$3\\times(10+1)=33$，其中任一输出的形状是（批量大小，通道数，高度，宽度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615030b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, block):\n",
    "    block.initialize()\n",
    "    return block(x)\n",
    "\n",
    "Y1 = forward(np.zeros((2, 8, 20, 20)), cls_predictor(5, 10))\n",
    "Y2 = forward(np.zeros((2, 16, 10, 10)), cls_predictor(3, 10))\n",
    "Y1.shape, Y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9004e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def forward(x, block):\n",
    "    return block(x)\n",
    "\n",
    "Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))\n",
    "Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))\n",
    "Y1.shape, Y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d9b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def forward(x, block):\n",
    "    return block(x)\n",
    "\n",
    "Y1 = forward(paddle.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))\n",
    "Y2 = forward(paddle.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))\n",
    "Y1.shape, Y2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ec7d2",
   "metadata": {},
   "source": [
    "正如我们所看到的，除了批量大小这一维度外，其他三个维度都具有不同的尺寸。\n",
    "为了将这两个预测输出链接起来以提高计算效率，我们将把这些张量转换为更一致的格式。\n",
    "\n",
    "通道维包含中心相同的锚框的预测结果。我们首先将通道维移到最后一维。\n",
    "因为不同尺度下批量大小仍保持不变，我们可以将预测结果转成二维的（批量大小，高$\\times$宽$\\times$通道数）的格式，以方便之后在维度$1$上的连结。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427f4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_pred(pred):\n",
    "    return npx.batch_flatten(pred.transpose(0, 2, 3, 1))\n",
    "\n",
    "def concat_preds(preds):\n",
    "    return np.concatenate([flatten_pred(p) for p in preds], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdebfbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def flatten_pred(pred):\n",
    "    return torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)\n",
    "\n",
    "def concat_preds(preds):\n",
    "    return torch.cat([flatten_pred(p) for p in preds], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb9fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def flatten_pred(pred):\n",
    "    return paddle.flatten(pred.transpose([0, 2, 3, 1]), start_axis=1)\n",
    "\n",
    "def concat_preds(preds):\n",
    "    return paddle.concat([flatten_pred(p) for p in preds], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ec068",
   "metadata": {},
   "source": [
    "这样一来，尽管`Y1`和`Y2`在通道数、高度和宽度方面具有不同的大小，我们仍然可以在同一个小批量的两个不同尺度上连接这两个预测输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c256d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "concat_preds([Y1, Y2]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d22add",
   "metadata": {},
   "source": [
    "### [**高和宽减半块**]\n",
    "\n",
    "为了在多个尺度下检测目标，我们在下面定义了高和宽减半块`down_sample_blk`，该模块将输入特征图的高度和宽度减半。\n",
    "事实上，该块应用了在 :numref:`subsec_vgg-blocks`中的VGG模块设计。\n",
    "更具体地说，每个高和宽减半块由两个填充为$1$的$3\\times3$的卷积层、以及步幅为$2$的$2\\times2$最大汇聚层组成。\n",
    "我们知道，填充为$1$的$3\\times3$卷积层不改变特征图的形状。但是，其后的$2\\times2$的最大汇聚层将输入特征图的高度和宽度减少了一半。\n",
    "对于此高和宽减半块的输入和输出特征图，因为$1\\times 2+(3-1)+(3-1)=6$，所以输出中的每个单元在输入上都有一个$6\\times6$的感受野。因此，高和宽减半块会扩大每个单元在其输出特征图中的感受野。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample_blk(num_channels):\n",
    "    blk = nn.Sequential()\n",
    "    for _ in range(2):\n",
    "        blk.add(nn.Conv2D(num_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm(in_channels=num_channels),\n",
    "                nn.Activation('relu'))\n",
    "    blk.add(nn.MaxPool2D(2))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def down_sample_blk(in_channels, out_channels):\n",
    "    blk = []\n",
    "    for _ in range(2):\n",
    "        blk.append(nn.Conv2d(in_channels, out_channels,\n",
    "                             kernel_size=3, padding=1))\n",
    "        blk.append(nn.BatchNorm2d(out_channels))\n",
    "        blk.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    blk.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042cf8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def down_sample_blk(in_channels, out_channels):\n",
    "    blk = []\n",
    "    for _ in range(2):\n",
    "        blk.append(nn.Conv2D(in_channels, out_channels,\n",
    "                             kernel_size=3, padding=1))\n",
    "        blk.append(nn.BatchNorm2D(out_channels))\n",
    "        blk.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    blk.append(nn.MaxPool2D(2))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7669da1",
   "metadata": {},
   "source": [
    "在以下示例中，我们构建的高和宽减半块会更改输入通道的数量，并将输入特征图的高度和宽度减半。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586256a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward(np.zeros((2, 3, 20, 20)), down_sample_blk(10)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "forward(torch.zeros((2, 3, 20, 20)), down_sample_blk(3, 10)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd46b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "forward(paddle.zeros((2, 3, 20, 20)), down_sample_blk(3, 10)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee4645",
   "metadata": {},
   "source": [
    "### [**基本网络块**]\n",
    "\n",
    "基本网络块用于从输入图像中抽取特征。\n",
    "为了计算简洁，我们构造了一个小的基础网络，该网络串联3个高和宽减半块，并逐步将通道数翻倍。\n",
    "给定输入图像的形状为$256\\times256$，此基本网络块输出的特征图形状为$32 \\times 32$（$256/2^3=32$）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80d5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_net():\n",
    "    blk = nn.Sequential()\n",
    "    for num_filters in [16, 32, 64]:\n",
    "        blk.add(down_sample_blk(num_filters))\n",
    "    return blk\n",
    "\n",
    "forward(np.zeros((2, 3, 256, 256)), base_net()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf7009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def base_net():\n",
    "    blk = []\n",
    "    num_filters = [3, 16, 32, 64]\n",
    "    for i in range(len(num_filters) - 1):\n",
    "        blk.append(down_sample_blk(num_filters[i], num_filters[i+1]))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "forward(torch.zeros((2, 3, 256, 256)), base_net()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42810bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def base_net():\n",
    "    blk = []\n",
    "    num_filters = [3, 16, 32, 64]\n",
    "    for i in range(len(num_filters) - 1):\n",
    "        blk.append(down_sample_blk(num_filters[i], num_filters[i+1]))\n",
    "    return nn.Sequential(*blk)\n",
    "\n",
    "forward(paddle.zeros((2, 3, 256, 256)), base_net()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e38874",
   "metadata": {},
   "source": [
    "### 完整的模型\n",
    "\n",
    "[**完整的单发多框检测模型由五个模块组成**]。每个块生成的特征图既用于生成锚框，又用于预测这些锚框的类别和偏移量。在这五个模块中，第一个是基本网络块，第二个到第四个是高和宽减半块，最后一个模块使用全局最大池将高度和宽度都降到1。从技术上讲，第二到第五个区块都是 :numref:`fig_ssd`中的多尺度特征块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d058fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blk(i):\n",
    "    if i == 0:\n",
    "        blk = base_net()\n",
    "    elif i == 4:\n",
    "        blk = nn.GlobalMaxPool2D()\n",
    "    else:\n",
    "        blk = down_sample_blk(128)\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc57063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_blk(i):\n",
    "    if i == 0:\n",
    "        blk = base_net()\n",
    "    elif i == 1:\n",
    "        blk = down_sample_blk(64, 128)\n",
    "    elif i == 4:\n",
    "        blk = nn.AdaptiveMaxPool2d((1,1))\n",
    "    else:\n",
    "        blk = down_sample_blk(128, 128)\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dddccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def get_blk(i):\n",
    "    if i == 0:\n",
    "        blk = base_net()\n",
    "    elif i == 1:\n",
    "        blk = down_sample_blk(64, 128)\n",
    "    elif i == 4:\n",
    "        blk = nn.AdaptiveMaxPool2D((1,1))\n",
    "    else:\n",
    "        blk = down_sample_blk(128, 128)\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0014d6b",
   "metadata": {},
   "source": [
    "现在我们[**为每个块定义前向传播**]。与图像分类任务不同，此处的输出包括：CNN特征图`Y`；在当前尺度下根据`Y`生成的锚框；预测的这些锚框的类别和偏移量（基于`Y`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n",
    "    Y = blk(X)\n",
    "    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)\n",
    "    cls_preds = cls_predictor(Y)\n",
    "    bbox_preds = bbox_predictor(Y)\n",
    "    return (Y, anchors, cls_preds, bbox_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea39fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n",
    "    Y = blk(X)\n",
    "    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)\n",
    "    cls_preds = cls_predictor(Y)\n",
    "    bbox_preds = bbox_predictor(Y)\n",
    "    return (Y, anchors, cls_preds, bbox_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14852ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n",
    "    Y = blk(X)\n",
    "    anchors = d2l.multibox_prior(Y, sizes=size, ratios=ratio)\n",
    "    cls_preds = cls_predictor(Y)\n",
    "    bbox_preds = bbox_predictor(Y)\n",
    "    return (Y, anchors, cls_preds, bbox_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4166c095",
   "metadata": {},
   "source": [
    "回想一下，在 :numref:`fig_ssd`中，一个较接近顶部的多尺度特征块是用于检测较大目标的，因此需要生成更大的锚框。\n",
    "在上面的前向传播中，在每个多尺度特征块上，我们通过调用的`multibox_prior`函数（见 :numref:`sec_anchor`）的`sizes`参数传递两个比例值的列表。\n",
    "在下面，0.2和1.05之间的区间被均匀分成五个部分，以确定五个模块的在不同尺度下的较小值：0.2、0.37、0.54、0.71和0.88。\n",
    "之后，他们较大的值由$\\sqrt{0.2 \\times 0.37} = 0.272$、$\\sqrt{0.37 \\times 0.54} = 0.447$等给出。\n",
    "\n",
    "[~~超参数~~]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ceb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],\n",
    "         [0.88, 0.961]]\n",
    "ratios = [[1, 2, 0.5]] * 5\n",
    "num_anchors = len(sizes[0]) + len(ratios[0]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0b4f1",
   "metadata": {},
   "source": [
    "现在，我们就可以按如下方式[**定义完整的模型**]`TinySSD`了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d04ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinySSD(nn.Block):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(TinySSD, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        for i in range(5):\n",
    "            # 即赋值语句self.blk_i=get_blk(i)\n",
    "            setattr(self, f'blk_{i}', get_blk(i))\n",
    "            setattr(self, f'cls_{i}', cls_predictor(num_anchors, num_classes))\n",
    "            setattr(self, f'bbox_{i}', bbox_predictor(num_anchors))\n",
    "\n",
    "    def forward(self, X):\n",
    "        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n",
    "        for i in range(5):\n",
    "            # getattr(self,'blk_%d'%i)即访问self.blk_i\n",
    "            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\n",
    "                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],\n",
    "                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))\n",
    "        anchors = np.concatenate(anchors, axis=1)\n",
    "        cls_preds = concat_preds(cls_preds)\n",
    "        cls_preds = cls_preds.reshape(\n",
    "            cls_preds.shape[0], -1, self.num_classes + 1)\n",
    "        bbox_preds = concat_preds(bbox_preds)\n",
    "        return anchors, cls_preds, bbox_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1c8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class TinySSD(nn.Module):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(TinySSD, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        idx_to_in_channels = [64, 128, 128, 128, 128]\n",
    "        for i in range(5):\n",
    "            # 即赋值语句self.blk_i=get_blk(i)\n",
    "            setattr(self, f'blk_{i}', get_blk(i))\n",
    "            setattr(self, f'cls_{i}', cls_predictor(idx_to_in_channels[i],\n",
    "                                                    num_anchors, num_classes))\n",
    "            setattr(self, f'bbox_{i}', bbox_predictor(idx_to_in_channels[i],\n",
    "                                                      num_anchors))\n",
    "\n",
    "    def forward(self, X):\n",
    "        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n",
    "        for i in range(5):\n",
    "            # getattr(self,'blk_%d'%i)即访问self.blk_i\n",
    "            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\n",
    "                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],\n",
    "                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))\n",
    "        anchors = torch.cat(anchors, dim=1)\n",
    "        cls_preds = concat_preds(cls_preds)\n",
    "        cls_preds = cls_preds.reshape(\n",
    "            cls_preds.shape[0], -1, self.num_classes + 1)\n",
    "        bbox_preds = concat_preds(bbox_preds)\n",
    "        return anchors, cls_preds, bbox_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bff3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class TinySSD(nn.Layer):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(TinySSD, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        idx_to_in_channels = [64, 128, 128, 128, 128]\n",
    "        for i in range(5):\n",
    "            # 即赋值语句self.blk_i=get_blk(i)\n",
    "            setattr(self, f'blk_{i}', get_blk(i))\n",
    "            setattr(self, f'cls_{i}', cls_predictor(idx_to_in_channels[i],\n",
    "                                                    num_anchors, num_classes))\n",
    "            setattr(self, f'bbox_{i}', bbox_predictor(idx_to_in_channels[i],\n",
    "                                                      num_anchors))\n",
    "\n",
    "    def forward(self, X):\n",
    "        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n",
    "        for i in range(5):\n",
    "            # getattr(self,'blk_%d'%i)即访问self.blk_i\n",
    "            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\n",
    "                X, getattr(self, f'blk_{i}'), sizes[i], ratios[i],\n",
    "                getattr(self, f'cls_{i}'), getattr(self, f'bbox_{i}'))\n",
    "        anchors = paddle.concat(anchors, axis=1)\n",
    "        cls_preds = concat_preds(cls_preds)\n",
    "        cls_preds = cls_preds.reshape(\n",
    "            (cls_preds.shape[0], -1, self.num_classes + 1))\n",
    "        bbox_preds = concat_preds(bbox_preds)\n",
    "        return anchors, cls_preds, bbox_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb248b7",
   "metadata": {},
   "source": [
    "我们[**创建一个模型实例，然后使用它**]对一个$256 \\times 256$像素的小批量图像`X`(**执行前向传播**)。\n",
    "\n",
    "如本节前面部分所示，第一个模块输出特征图的形状为$32 \\times 32$。\n",
    "回想一下，第二到第四个模块为高和宽减半块，第五个模块为全局汇聚层。\n",
    "由于以特征图的每个单元为中心有$4$个锚框生成，因此在所有五个尺度下，每个图像总共生成$(32^2 + 16^2 + 8^2 + 4^2 + 1)\\times 4 = 5444$个锚框。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TinySSD(num_classes=1)\n",
    "net.initialize()\n",
    "X = np.zeros((32, 3, 256, 256))\n",
    "anchors, cls_preds, bbox_preds = net(X)\n",
    "\n",
    "print('output anchors:', anchors.shape)\n",
    "print('output class preds:', cls_preds.shape)\n",
    "print('output bbox preds:', bbox_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c0ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "net = TinySSD(num_classes=1)\n",
    "X = torch.zeros((32, 3, 256, 256))\n",
    "anchors, cls_preds, bbox_preds = net(X)\n",
    "\n",
    "print('output anchors:', anchors.shape)\n",
    "print('output class preds:', cls_preds.shape)\n",
    "print('output bbox preds:', bbox_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8c8065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "net = TinySSD(num_classes=1)\n",
    "X = paddle.zeros((32, 3, 256, 256))\n",
    "anchors, cls_preds, bbox_preds = net(X)\n",
    "\n",
    "print('output anchors:', anchors.shape)\n",
    "print('output class preds:', cls_preds.shape)\n",
    "print('output bbox preds:', bbox_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eced899",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "\n",
    "现在，我们将描述如何训练用于目标检测的单发多框检测模型。\n",
    "\n",
    "### 读取数据集和初始化\n",
    "\n",
    "首先，让我们[**读取**] :numref:`sec_object-detection-dataset`中描述的(**香蕉检测数据集**)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aabffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "batch_size = 32\n",
    "train_iter, _ = d2l.load_data_bananas(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7e98b",
   "metadata": {},
   "source": [
    "香蕉检测数据集中，目标的类别数为1。\n",
    "定义好模型后，我们需要(**初始化其参数并定义优化算法**)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c45ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device, net = d2l.try_gpu(), TinySSD(num_classes=1)\n",
    "net.initialize(init=init.Xavier(), ctx=device)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd',\n",
    "                        {'learning_rate': 0.2, 'wd': 5e-4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4237ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "device, net = d2l.try_gpu(), TinySSD(num_classes=1)\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.2, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "device, net = d2l.try_gpu(), TinySSD(num_classes=1)\n",
    "trainer = paddle.optimizer.SGD(learning_rate=0.2, \n",
    "                               parameters=net.parameters(), \n",
    "                               weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c4660",
   "metadata": {},
   "source": [
    "### [**定义损失函数和评价函数**]\n",
    "\n",
    "目标检测有两种类型的损失。\n",
    "第一种有关锚框类别的损失：我们可以简单地复用之前图像分类问题里一直使用的交叉熵损失函数来计算；\n",
    "第二种有关正类锚框偏移量的损失：预测偏移量是一个回归问题。\n",
    "但是，对于这个回归问题，我们在这里不使用 :numref:`subsec_normal_distribution_and_squared_loss`中描述的平方损失，而是使用$L_1$范数损失，即预测值和真实值之差的绝对值。\n",
    "掩码变量`bbox_masks`令负类锚框和填充锚框不参与损失的计算。\n",
    "最后，我们将锚框类别和偏移量的损失相加，以获得模型的最终损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "bbox_loss = gluon.loss.L1Loss()\n",
    "\n",
    "def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n",
    "    cls = cls_loss(cls_preds, cls_labels)\n",
    "    bbox = bbox_loss(bbox_preds * bbox_masks, bbox_labels * bbox_masks)\n",
    "    return cls + bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d921f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "cls_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "bbox_loss = nn.L1Loss(reduction='none')\n",
    "\n",
    "def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n",
    "    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]\n",
    "    cls = cls_loss(cls_preds.reshape(-1, num_classes),\n",
    "                   cls_labels.reshape(-1)).reshape(batch_size, -1).mean(dim=1)\n",
    "    bbox = bbox_loss(bbox_preds * bbox_masks,\n",
    "                     bbox_labels * bbox_masks).mean(dim=1)\n",
    "    return cls + bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430a77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "cls_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "bbox_loss = nn.L1Loss(reduction='none')\n",
    "\n",
    "def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n",
    "    batch_size, num_classes = cls_preds.shape[0], cls_preds.shape[2]\n",
    "    cls = cls_loss(cls_preds.reshape((-1, num_classes)),\n",
    "                   cls_labels.reshape([-1])).reshape((batch_size, -1)).mean(axis=1)\n",
    "    bbox = bbox_loss(bbox_preds * bbox_masks,\n",
    "                     bbox_labels * bbox_masks).mean(axis=1)\n",
    "    return cls + bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f806834",
   "metadata": {},
   "source": [
    "我们可以沿用准确率评价分类结果。\n",
    "由于偏移量使用了$L_1$范数损失，我们使用*平均绝对误差*来评价边界框的预测结果。这些预测结果是从生成的锚框及其预测偏移量中获得的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_eval(cls_preds, cls_labels):\n",
    "    # 由于类别预测结果放在最后一维，argmax需要指定最后一维。\n",
    "    return float((cls_preds.argmax(axis=-1).astype(\n",
    "        cls_labels.dtype) == cls_labels).sum())\n",
    "\n",
    "def bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n",
    "    return float((np.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def cls_eval(cls_preds, cls_labels):\n",
    "    # 由于类别预测结果放在最后一维，argmax需要指定最后一维。\n",
    "    return float((cls_preds.argmax(dim=-1).type(\n",
    "        cls_labels.dtype) == cls_labels).sum())\n",
    "\n",
    "def bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n",
    "    return float((torch.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def cls_eval(cls_preds, cls_labels):\n",
    "    # 由于类别预测结果放在最后一维，argmax需要指定最后一维。\n",
    "    return float((cls_preds.argmax(axis=-1).astype(\n",
    "        cls_labels.dtype) == cls_labels).sum())\n",
    "\n",
    "def bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n",
    "    return float((paddle.abs((bbox_labels - bbox_preds) * bbox_masks)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b9a88",
   "metadata": {},
   "source": [
    "### [**训练模型**]\n",
    "\n",
    "在训练模型时，我们需要在模型的前向传播过程中生成多尺度锚框（`anchors`），并预测其类别（`cls_preds`）和偏移量（`bbox_preds`）。\n",
    "然后，我们根据标签信息`Y`为生成的锚框标记类别（`cls_labels`）和偏移量（`bbox_labels`）。\n",
    "最后，我们根据类别和偏移量的预测和标注值计算损失函数。为了代码简洁，这里没有评价测试数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, timer = 20, d2l.Timer()\n",
    "animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                        legend=['class error', 'bbox mae'])\n",
    "for epoch in range(num_epochs):\n",
    "    # 指标包括：训练精确度的和，训练精确度的和中的示例数，\n",
    "    # 绝对误差的和，绝对误差的和中的示例数\n",
    "    metric = d2l.Accumulator(4)\n",
    "    for features, target in train_iter:\n",
    "        timer.start()\n",
    "        X = features.as_in_ctx(device)\n",
    "        Y = target.as_in_ctx(device)\n",
    "        with autograd.record():\n",
    "            # 生成多尺度的锚框，为每个锚框预测类别和偏移量\n",
    "            anchors, cls_preds, bbox_preds = net(X)\n",
    "            # 为每个锚框标注类别和偏移量\n",
    "            bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors,\n",
    "                                                                      Y)\n",
    "            # 根据类别和偏移量的预测和标注值计算损失函数\n",
    "            l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n",
    "                          bbox_masks)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.size,\n",
    "                   bbox_eval(bbox_preds, bbox_labels, bbox_masks),\n",
    "                   bbox_labels.size)\n",
    "    cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3]\n",
    "    animator.add(epoch + 1, (cls_err, bbox_mae))\n",
    "print(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')\n",
    "print(f'{len(train_iter._dataset) / timer.stop():.1f} examples/sec on '\n",
    "      f'{str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a005871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "num_epochs, timer = 20, d2l.Timer()\n",
    "animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                        legend=['class error', 'bbox mae'])\n",
    "net = net.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练精确度的和，训练精确度的和中的示例数\n",
    "    # 绝对误差的和，绝对误差的和中的示例数\n",
    "    metric = d2l.Accumulator(4)\n",
    "    net.train()\n",
    "    for features, target in train_iter:\n",
    "        timer.start()\n",
    "        trainer.zero_grad()\n",
    "        X, Y = features.to(device), target.to(device)\n",
    "        # 生成多尺度的锚框，为每个锚框预测类别和偏移量\n",
    "        anchors, cls_preds, bbox_preds = net(X)\n",
    "        # 为每个锚框标注类别和偏移量\n",
    "        bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, Y)\n",
    "        # 根据类别和偏移量的预测和标注值计算损失函数\n",
    "        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n",
    "                      bbox_masks)\n",
    "        l.mean().backward()\n",
    "        trainer.step()\n",
    "        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.numel(),\n",
    "                   bbox_eval(bbox_preds, bbox_labels, bbox_masks),\n",
    "                   bbox_labels.numel())\n",
    "    cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3]\n",
    "    animator.add(epoch + 1, (cls_err, bbox_mae))\n",
    "print(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')\n",
    "print(f'{len(train_iter.dataset) / timer.stop():.1f} examples/sec on '\n",
    "      f'{str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "num_epochs, timer = 20, d2l.Timer()\n",
    "animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                        legend=['class error', 'bbox mae'])\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练精确度的和，训练精确度的和中的示例数\n",
    "    # 绝对误差的和，绝对误差的和中的示例数\n",
    "    metric = d2l.Accumulator(4)\n",
    "    net.train()\n",
    "    for features, target in train_iter:\n",
    "        timer.start()\n",
    "        trainer.clear_grad()\n",
    "        X, Y = features, target\n",
    "        # 生成多尺度的锚框，为每个锚框预测类别和偏移量\n",
    "        anchors, cls_preds, bbox_preds = net(X)\n",
    "        # 为每个锚框标注类别和偏移量\n",
    "        bbox_labels, bbox_masks, cls_labels = d2l.multibox_target(anchors, Y)\n",
    "        # 根据类别和偏移量的预测和标注值计算损失函数\n",
    "        l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n",
    "                      bbox_masks)\n",
    "        l.mean().backward()\n",
    "        trainer.step()\n",
    "        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.numel(),\n",
    "                   bbox_eval(bbox_preds, bbox_labels, bbox_masks),\n",
    "                   bbox_labels.numel())\n",
    "    cls_err, bbox_mae = 1 - metric[0] / metric[1], metric[2] / metric[3]\n",
    "    animator.add(epoch + 1, (cls_err, bbox_mae))\n",
    "print(f'class err {cls_err:.2e}, bbox mae {bbox_mae:.2e}')\n",
    "print(f'{len(train_iter.dataset) / timer.stop():.1f} examples/sec on '\n",
    "      f'{str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52091d5",
   "metadata": {},
   "source": [
    "## [**预测目标**]\n",
    "\n",
    "在预测阶段，我们希望能把图像里面所有我们感兴趣的目标检测出来。在下面，我们读取并调整测试图像的大小，然后将其转成卷积层需要的四维格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad573c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.imread('../img/banana.jpg')\n",
    "feature = image.imresize(img, 256, 256).astype('float32')\n",
    "X = np.expand_dims(feature.transpose(2, 0, 1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c5608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "X = torchvision.io.read_image('../img/banana.jpg').unsqueeze(0).float()\n",
    "img = X.squeeze(0).permute(1, 2, 0).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e9841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "X = paddle.to_tensor(\n",
    "            paddlevision.image.image_load(\n",
    "                '../img/banana.jpg', backend=\"cv2\"\n",
    "                )[..., ::-1].transpose([2,0,1])\n",
    "                ).unsqueeze(0).astype(paddle.float32)\n",
    "img = X.squeeze(0).transpose([1, 2, 0]).astype(paddle.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ac8c1",
   "metadata": {},
   "source": [
    "使用下面的`multibox_detection`函数，我们可以根据锚框及其预测偏移量得到预测边界框。然后，通过非极大值抑制来移除相似的预测边界框。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53bb1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    anchors, cls_preds, bbox_preds = net(X.as_in_ctx(device))\n",
    "    cls_probs = npx.softmax(cls_preds).transpose(0, 2, 1)\n",
    "    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)\n",
    "    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]\n",
    "    return output[0, idx]\n",
    "\n",
    "output = predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69727f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def predict(X):\n",
    "    net.eval()\n",
    "    anchors, cls_preds, bbox_preds = net(X.to(device))\n",
    "    cls_probs = F.softmax(cls_preds, dim=2).permute(0, 2, 1)\n",
    "    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)\n",
    "    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]\n",
    "    return output[0, idx]\n",
    "\n",
    "output = predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def predict(X):\n",
    "    net.eval()\n",
    "    anchors, cls_preds, bbox_preds = net(X)\n",
    "    cls_probs = F.softmax(cls_preds, axis=2).transpose([0, 2, 1])\n",
    "    output = d2l.multibox_detection(cls_probs, bbox_preds, anchors)\n",
    "    idx = [i for i, row in enumerate(output[0]) if row[0] != -1]\n",
    "    return output[0, :][idx]\n",
    "\n",
    "output = predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de12d51",
   "metadata": {},
   "source": [
    "最后，我们[**筛选所有置信度不低于0.9的边界框，做为最终输出**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0057f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img, output, threshold):\n",
    "    d2l.set_figsize((5, 5))\n",
    "    fig = d2l.plt.imshow(img.asnumpy())\n",
    "    for row in output:\n",
    "        score = float(row[1])\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        h, w = img.shape[0:2]\n",
    "        bbox = [row[2:6] * np.array((w, h, w, h), ctx=row.ctx)]\n",
    "        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')\n",
    "\n",
    "display(img, output, threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def display(img, output, threshold):\n",
    "    d2l.set_figsize((5, 5))\n",
    "    fig = d2l.plt.imshow(img)\n",
    "    for row in output:\n",
    "        score = float(row[1])\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        h, w = img.shape[0:2]\n",
    "        bbox = [row[2:6] * torch.tensor((w, h, w, h), device=row.device)]\n",
    "        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')\n",
    "\n",
    "display(img, output.cpu(), threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d7168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def display(img, output, threshold):\n",
    "    d2l.set_figsize((5, 5))\n",
    "    fig = d2l.plt.imshow(img)\n",
    "    for row in output:\n",
    "        score = float(row[1])\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        h, w = img.shape[0:2]\n",
    "        bbox = [row[2:6] * paddle.to_tensor((w, h, w, h))]\n",
    "        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')\n",
    "\n",
    "display(img, output.cpu(), threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529da3ce",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 单发多框检测是一种多尺度目标检测模型。基于基础网络块和各个多尺度特征块，单发多框检测生成不同数量和不同大小的锚框，并通过预测这些锚框的类别和偏移量检测不同大小的目标。\n",
    "* 在训练单发多框检测模型时，损失函数是根据锚框的类别和偏移量的预测及标注值计算得出的。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 能通过改进损失函数来改进单发多框检测吗？例如，将预测偏移量用到的$L_1$范数损失替换为平滑$L_1$范数损失。它在零点附近使用平方函数从而更加平滑，这是通过一个超参数$\\sigma$来控制平滑区域的：\n",
    "\n",
    "$$\n",
    "f(x) =\n",
    "    \\begin{cases}\n",
    "    (\\sigma x)^2/2,& \\text{if }|x| < 1/\\sigma^2\\\\\n",
    "    |x|-0.5/\\sigma^2,& \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "当$\\sigma$非常大时，这种损失类似于$L_1$范数损失。当它的值较小时，损失函数较平滑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60da09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [10, 1, 0.5]\n",
    "lines = ['-', '--', '-.']\n",
    "x = np.arange(-2, 2, 0.1)\n",
    "d2l.set_figsize()\n",
    "\n",
    "for l, s in zip(lines, sigmas):\n",
    "    y = npx.smooth_l1(x, scalar=s)\n",
    "    d2l.plt.plot(x.asnumpy(), y.asnumpy(), l, label='sigma=%.1f' % s)\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa77e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def smooth_l1(data, scalar):\n",
    "    out = []\n",
    "    for i in data:\n",
    "        if abs(i) < 1 / (scalar ** 2):\n",
    "            out.append(((scalar * i) ** 2) / 2)\n",
    "        else:\n",
    "            out.append(abs(i) - 0.5 / (scalar ** 2))\n",
    "    return torch.tensor(out)\n",
    "\n",
    "sigmas = [10, 1, 0.5]\n",
    "lines = ['-', '--', '-.']\n",
    "x = torch.arange(-2, 2, 0.1)\n",
    "d2l.set_figsize()\n",
    "\n",
    "for l, s in zip(lines, sigmas):\n",
    "    y = smooth_l1(x, scalar=s)\n",
    "    d2l.plt.plot(x, y, l, label='sigma=%.1f' % s)\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ab7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def smooth_l1(data, scalar):\n",
    "    out = []\n",
    "    for i in data.numpy():\n",
    "        if abs(i) < 1 / (scalar ** 2):\n",
    "            out.append(((scalar * i) ** 2) / 2)\n",
    "        else:\n",
    "            out.append(abs(i) - 0.5 / (scalar ** 2))\n",
    "    return paddle.to_tensor(out)\n",
    "\n",
    "sigmas = [10, 1, 0.5]\n",
    "lines = ['-', '--', '-.']\n",
    "x = paddle.arange(-2.0, 2.0, 0.1, dtype=paddle.float32)\n",
    "d2l.set_figsize()\n",
    "\n",
    "for l, s in zip(lines, sigmas):\n",
    "    y = smooth_l1(x, scalar=s)\n",
    "    d2l.plt.plot(x, y, l, label='sigma=%.1f' % s)\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433e2bff",
   "metadata": {},
   "source": [
    "此外，在类别预测时，实验中使用了交叉熵损失：设真实类别$j$的预测概率是$p_j$，交叉熵损失为$-\\log p_j$。我们还可以使用焦点损失 :cite:`Lin.Goyal.Girshick.ea.2017`。给定超参数$\\gamma > 0$和$\\alpha > 0$，此损失的定义为：\n",
    "\n",
    "$$ - \\alpha (1-p_j)^{\\gamma} \\log p_j.$$\n",
    "\n",
    "可以看到，增大$\\gamma$可以有效地减少正类预测概率较大时（例如$p_j > 0.5$）的相对损失，因此训练可以更集中在那些错误分类的困难示例上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(gamma, x):\n",
    "    return -(1 - x) ** gamma * np.log(x)\n",
    "\n",
    "x = np.arange(0.01, 1, 0.01)\n",
    "for l, gamma in zip(lines, [0, 1, 5]):\n",
    "    y = d2l.plt.plot(x.asnumpy(), focal_loss(gamma, x).asnumpy(), l,\n",
    "                     label='gamma=%.1f' % gamma)\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8bcbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def focal_loss(gamma, x):\n",
    "    return -(1 - x) ** gamma * torch.log(x)\n",
    "\n",
    "x = torch.arange(0.01, 1, 0.01)\n",
    "for l, gamma in zip(lines, [0, 1, 5]):\n",
    "    y = d2l.plt.plot(x, focal_loss(gamma, x), l, label='gamma=%.1f' % gamma)\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130f0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def focal_loss(gamma, x):\n",
    "    return -(1 - x) ** gamma * paddle.log(x)\n",
    "\n",
    "x = paddle.arange(0.01, 1, 0.01, dtype=paddle.float32)\n",
    "for l, gamma in zip(lines, [0, 1, 5]):\n",
    "    y = d2l.plt.plot(x, focal_loss(gamma, x), l, label='gamma=%.1f' % gamma)\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94eb735",
   "metadata": {},
   "source": [
    "2. 由于篇幅限制，我们在本节中省略了单发多框检测模型的一些实现细节。能否从以下几个方面进一步改进模型：\n",
    "    1. 当目标比图像小得多时，模型可以将输入图像调大；\n",
    "    1. 通常会存在大量的负锚框。为了使类别分布更加平衡，我们可以将负锚框的高和宽减半；\n",
    "    1. 在损失函数中，给类别损失和偏移损失设置不同比重的超参数；\n",
    "    1. 使用其他方法评估目标检测模型，例如单发多框检测论文 :cite:`Liu.Anguelov.Erhan.ea.2016`中的方法。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/3205)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/3204)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11807)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f2ccc3",
   "metadata": {},
   "source": [
    "# 区域卷积神经网络（R-CNN）系列\n",
    ":label:`sec_rcnn`\n",
    "\n",
    "除了 :numref:`sec_ssd`中描述的单发多框检测之外，\n",
    "区域卷积神经网络（region-based CNN或regions with CNN features，R-CNN） :cite:`Girshick.Donahue.Darrell.ea.2014`也是将深度模型应用于目标检测的开创性工作之一。\n",
    "本节将介绍R-CNN及其一系列改进方法：快速的R-CNN（Fast R-CNN） :cite:`Girshick.2015`、更快的R-CNN（Faster R-CNN） :cite:`Ren.He.Girshick.ea.2015`和掩码R-CNN（Mask R-CNN） :cite:`He.Gkioxari.Dollar.ea.2017`。\n",
    "限于篇幅，我们只着重介绍这些模型的设计思路。\n",
    "\n",
    "## R-CNN\n",
    "\n",
    "*R-CNN*首先从输入图像中选取若干（例如2000个）*提议区域*（如锚框也是一种选取方法），并标注它们的类别和边界框（如偏移量）。 :cite:`Girshick.Donahue.Darrell.ea.2014`然后，用卷积神经网络对每个提议区域进行前向传播以抽取其特征。\n",
    "接下来，我们用每个提议区域的特征来预测类别和边界框。\n",
    "\n",
    "![R-CNN模型](../img/r-cnn.svg)\n",
    ":label:`fig_r-cnn`\n",
    "\n",
    " :numref:`fig_r-cnn`展示了R-CNN模型。具体来说，R-CNN包括以下四个步骤：\n",
    "\n",
    "1. 对输入图像使用*选择性搜索*来选取多个高质量的提议区域 :cite:`Uijlings.Van-De-Sande.Gevers.ea.2013`。这些提议区域通常是在多个尺度下选取的，并具有不同的形状和大小。每个提议区域都将被标注类别和真实边界框；\n",
    "1. 选择一个预训练的卷积神经网络，并将其在输出层之前截断。将每个提议区域变形为网络需要的输入尺寸，并通过前向传播输出抽取的提议区域特征；\n",
    "1. 将每个提议区域的特征连同其标注的类别作为一个样本。训练多个支持向量机对目标分类，其中每个支持向量机用来判断样本是否属于某一个类别；\n",
    "1. 将每个提议区域的特征连同其标注的边界框作为一个样本，训练线性回归模型来预测真实边界框。\n",
    "\n",
    "尽管R-CNN模型通过预训练的卷积神经网络有效地抽取了图像特征，但它的速度很慢。\n",
    "想象一下，我们可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测。\n",
    "这种庞大的计算量使得R-CNN在现实世界中难以被广泛应用。\n",
    "\n",
    "## Fast R-CNN\n",
    "\n",
    "R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。\n",
    "由于这些区域通常有重叠，独立的特征抽取会导致重复的计算。\n",
    "*Fast R-CNN* :cite:`Girshick.2015`对R-CNN的主要改进之一，是仅在整张图象上执行卷积神经网络的前向传播。\n",
    "\n",
    "![Fast R-CNN模型](../img/fast-rcnn.svg)\n",
    ":label:`fig_fast_r-cnn`\n",
    "\n",
    " :numref:`fig_fast_r-cnn`中描述了Fast R-CNN模型。它的主要计算如下：\n",
    "\n",
    "1. 与R-CNN相比，Fast R-CNN用来提取特征的卷积神经网络的输入是整个图像，而不是各个提议区域。此外，这个网络通常会参与训练。设输入为一张图像，将卷积神经网络的输出的形状记为$1 \\times c \\times h_1  \\times w_1$；\n",
    "1. 假设选择性搜索生成了$n$个提议区域。这些形状各异的提议区域在卷积神经网络的输出上分别标出了形状各异的兴趣区域。然后，这些感兴趣的区域需要进一步抽取出形状相同的特征（比如指定高度$h_2$和宽度$w_2$），以便于连结后输出。为了实现这一目标，Fast R-CNN引入了*兴趣区域汇聚层*（RoI pooling）：将卷积神经网络的输出和提议区域作为输入，输出连结后的各个提议区域抽取的特征，形状为$n \\times c \\times h_2 \\times w_2$；\n",
    "1. 通过全连接层将输出形状变换为$n \\times d$，其中超参数$d$取决于模型设计；\n",
    "1. 预测$n$个提议区域中每个区域的类别和边界框。更具体地说，在预测类别和边界框时，将全连接层的输出分别转换为形状为$n \\times q$（$q$是类别的数量）的输出和形状为$n \\times 4$的输出。其中预测类别时使用softmax回归。\n",
    "\n",
    "在Fast R-CNN中提出的兴趣区域汇聚层与 :numref:`sec_pooling`中介绍的汇聚层有所不同。在汇聚层中，我们通过设置汇聚窗口、填充和步幅的大小来间接控制输出形状。而兴趣区域汇聚层对每个区域的输出形状是可以直接指定的。\n",
    "\n",
    "例如，指定每个区域输出的高和宽分别为$h_2$和$w_2$。\n",
    "对于任何形状为$h \\times w$的兴趣区域窗口，该窗口将被划分为$h_2 \\times w_2$子窗口网格，其中每个子窗口的大小约为$(h/h_2) \\times (w/w_2)$。\n",
    "在实践中，任何子窗口的高度和宽度都应向上取整，其中的最大元素作为该子窗口的输出。\n",
    "因此，兴趣区域汇聚层可从形状各异的兴趣区域中均抽取出形状相同的特征。\n",
    "\n",
    "作为说明性示例， :numref:`fig_roi`中提到，在$4 \\times 4$的输入中，我们选取了左上角$3\\times 3$的兴趣区域。\n",
    "对于该兴趣区域，我们通过$2\\times 2$的兴趣区域汇聚层得到一个$2\\times 2$的输出。\n",
    "请注意，四个划分后的子窗口中分别含有元素0、1、4、5（5最大）；2、6（6最大）；8、9（9最大）；以及10。\n",
    "\n",
    "![一个 $2\\times 2$ 的兴趣区域汇聚层](../img/roi.svg)\n",
    ":label:`fig_roi`\n",
    "\n",
    "下面，我们演示了兴趣区域汇聚层的计算方法。\n",
    "假设卷积神经网络抽取的特征`X`的高度和宽度都是4，且只有单通道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc92c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "X = np.arange(16).reshape(1, 1, 4, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb584c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "X = torch.arange(16.).reshape(1, 1, 4, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df91f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "import paddle.vision as paddlevision\n",
    "\n",
    "X = paddle.reshape(paddle.arange(16, dtype='float32'), (1,1,4,4))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ed2f6",
   "metadata": {},
   "source": [
    "让我们进一步假设输入图像的高度和宽度都是40像素，且选择性搜索在此图像上生成了两个提议区域。\n",
    "每个区域由5个元素表示：区域目标类别、左上角和右下角的$(x, y)$坐标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af150c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = np.array([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d046676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "rois = torch.Tensor([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0ac8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "rois = paddle.to_tensor([[0, 0, 20, 20], [0, 10, 30, 30]]).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424554eb",
   "metadata": {},
   "source": [
    "由于`X`的高和宽是输入图像高和宽的$1/10$，因此，两个提议区域的坐标先按`spatial_scale`乘以0.1。\n",
    "然后，在`X`上分别标出这两个兴趣区域`X[:, :, 0:3, 0:3]`和`X[:, :, 1:4, 0:4]`。\n",
    "最后，在$2\\times 2$的兴趣区域汇聚层中，每个兴趣区域被划分为子窗口网格，并进一步抽取相同形状$2\\times 2$的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16899786",
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.roi_pooling(X, rois, pooled_size=(2, 2), spatial_scale=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433dbe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "torchvision.ops.roi_pool(X, rois, output_size=(2, 2), spatial_scale=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddaf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "boxes_num = paddle.to_tensor([len(rois)]).astype('int32')\n",
    "paddlevision.ops.roi_pool(X, rois, boxes_num, output_size=(2, 2), spatial_scale=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ecb81d",
   "metadata": {},
   "source": [
    "## Faster R-CNN\n",
    "\n",
    "为了较精确地检测目标结果，Fast R-CNN模型通常需要在选择性搜索中生成大量的提议区域。\n",
    "*Faster R-CNN* :cite:`Ren.He.Girshick.ea.2015`提出将选择性搜索替换为*区域提议网络*（region proposal network），从而减少提议区域的生成数量，并保证目标检测的精度。\n",
    "\n",
    "![Faster R-CNN 模型](../img/faster-rcnn.svg)\n",
    ":label:`fig_faster_r-cnn`\n",
    "\n",
    " :numref:`fig_faster_r-cnn`描述了Faster R-CNN模型。\n",
    "与Fast R-CNN相比，Faster R-CNN只将生成提议区域的方法从选择性搜索改为了区域提议网络，模型的其余部分保持不变。具体来说，区域提议网络的计算步骤如下：\n",
    "\n",
    "1. 使用填充为1的$3\\times 3$的卷积层变换卷积神经网络的输出，并将输出通道数记为$c$。这样，卷积神经网络为图像抽取的特征图中的每个单元均得到一个长度为$c$的新特征。\n",
    "1. 以特征图的每个像素为中心，生成多个不同大小和宽高比的锚框并标注它们。\n",
    "1. 使用锚框中心单元长度为$c$的特征，分别预测该锚框的二元类别（含目标还是背景）和边界框。\n",
    "1. 使用非极大值抑制，从预测类别为目标的预测边界框中移除相似的结果。最终输出的预测边界框即是兴趣区域汇聚层所需的提议区域。\n",
    "\n",
    "值得一提的是，区域提议网络作为Faster R-CNN模型的一部分，是和整个模型一起训练得到的。\n",
    "换句话说，Faster R-CNN的目标函数不仅包括目标检测中的类别和边界框预测，还包括区域提议网络中锚框的二元类别和边界框预测。\n",
    "作为端到端训练的结果，区域提议网络能够学习到如何生成高质量的提议区域，从而在减少了从数据中学习的提议区域的数量的情况下，仍保持目标检测的精度。\n",
    "\n",
    "## Mask R-CNN\n",
    "\n",
    "如果在训练集中还标注了每个目标在图像上的像素级位置，那么*Mask R-CNN* :cite:`He.Gkioxari.Dollar.ea.2017`能够有效地利用这些详尽的标注信息进一步提升目标检测的精度。\n",
    "\n",
    "![Mask R-CNN 模型](../img/mask-rcnn.svg)\n",
    ":label:`fig_mask_r-cnn`\n",
    "\n",
    "如 :numref:`fig_mask_r-cnn`所示，Mask R-CNN是基于Faster R-CNN修改而来的。\n",
    "具体来说，Mask R-CNN将兴趣区域汇聚层替换为了\n",
    "*兴趣区域对齐*层，使用*双线性插值*（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测。\n",
    "兴趣区域对齐层的输出包含了所有与兴趣区域的形状相同的特征图。\n",
    "它们不仅被用于预测每个兴趣区域的类别和边界框，还通过额外的全卷积网络预测目标的像素级位置。\n",
    "本章的后续章节将更详细地介绍如何使用全卷积网络预测图像中像素级的语义。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* R-CNN对图像选取若干提议区域，使用卷积神经网络对每个提议区域执行前向传播以抽取其特征，然后再用这些特征来预测提议区域的类别和边界框。\n",
    "* Fast R-CNN对R-CNN的一个主要改进：只对整个图像做卷积神经网络的前向传播。它还引入了兴趣区域汇聚层，从而为具有不同形状的兴趣区域抽取相同形状的特征。\n",
    "* Faster R-CNN将Fast R-CNN中使用的选择性搜索替换为参与训练的区域提议网络，这样后者可以在减少提议区域数量的情况下仍保证目标检测的精度。\n",
    "* Mask R-CNN在Faster R-CNN的基础上引入了一个全卷积网络，从而借助目标的像素级位置进一步提升目标检测的精度。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 我们能否将目标检测视为回归问题（例如预测边界框和类别的概率）？可以参考YOLO模型 :cite:`Redmon.Divvala.Girshick.ea.2016`的设计。\n",
    "1. 将单发多框检测与本节介绍的方法进行比较。他们的主要区别是什么？可以参考 :cite:`Zhao.Zheng.Xu.ea.2019`中的图2。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[讨论区](https://discuss.d2l.ai/t/3206)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[讨论区](https://discuss.d2l.ai/t/3207)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[讨论区](https://discuss.d2l.ai/t/11808)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99174cc",
   "metadata": {},
   "source": [
    "# 语义分割和数据集\n",
    ":label:`sec_semantic_segmentation`\n",
    "\n",
    "在 :numref:`sec_bbox`— :numref:`sec_rcnn`中讨论的目标检测问题中，我们一直使用方形边界框来标注和预测图像中的目标。\n",
    "本节将探讨*语义分割*（semantic segmentation）问题，它重点关注于如何将图像分割成属于不同语义类别的区域。\n",
    "与目标检测不同，语义分割可以识别并理解图像中每一个像素的内容：其语义区域的标注和预测是像素级的。\n",
    " :numref:`fig_segmentation`展示了语义分割中图像有关狗、猫和背景的标签。\n",
    "与目标检测相比，语义分割标注的像素级的边框显然更加精细。\n",
    "\n",
    "![语义分割中图像有关狗、猫和背景的标签](../img/segmentation.svg)\n",
    ":label:`fig_segmentation`\n",
    "\n",
    "## 图像分割和实例分割\n",
    "\n",
    "计算机视觉领域还有2个与语义分割相似的重要问题，即*图像分割*（image segmentation）和*实例分割*（instance segmentation）。\n",
    "我们在这里将它们同语义分割简单区分一下。\n",
    "\n",
    "* *图像分割*将图像划分为若干组成区域，这类问题的方法通常利用图像中像素之间的相关性。它在训练时不需要有关图像像素的标签信息，在预测时也无法保证分割出的区域具有我们希望得到的语义。以 :numref:`fig_segmentation`中的图像作为输入，图像分割可能会将狗分为两个区域：一个覆盖以黑色为主的嘴和眼睛，另一个覆盖以黄色为主的其余部分身体。\n",
    "* *实例分割*也叫*同时检测并分割*（simultaneous detection and segmentation），它研究如何识别图像中各个目标实例的像素级区域。与语义分割不同，实例分割不仅需要区分语义，还要区分不同的目标实例。例如，如果图像中有两条狗，则实例分割需要区分像素属于的两条狗中的哪一条。\n",
    "\n",
    "## Pascal VOC2012 语义分割数据集\n",
    "\n",
    "[**最重要的语义分割数据集之一是[Pascal VOC2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/)。**]\n",
    "下面我们深入了解一下这个数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a39beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, image, np, npx\n",
    "import os\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4752ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980f60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "import paddle.vision as paddlevision\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b17318",
   "metadata": {},
   "source": [
    "数据集的tar文件大约为2GB，所以下载可能需要一段时间。\n",
    "提取出的数据集位于`../data/VOCdevkit/VOC2012`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',\n",
    "                           '4e443f8a2eca6b1dac8a6c57641b67dd40621a49')\n",
    "\n",
    "voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76c57d",
   "metadata": {},
   "source": [
    "进入路径`../data/VOCdevkit/VOC2012`之后，我们可以看到数据集的不同组件。\n",
    "`ImageSets/Segmentation`路径包含用于训练和测试样本的文本文件，而`JPEGImages`和`SegmentationClass`路径分别存储着每个示例的输入图像和标签。\n",
    "此处的标签也采用图像格式，其尺寸和它所标注的输入图像的尺寸相同。\n",
    "此外，标签中颜色相同的像素属于同一个语义类别。\n",
    "下面将`read_voc_images`函数定义为[**将所有输入的图像和标签读入内存**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65460afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def read_voc_images(voc_dir, is_train=True):\n",
    "    \"\"\"读取所有VOC图像并标注\"\"\"\n",
    "    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',\n",
    "                             'train.txt' if is_train else 'val.txt')\n",
    "    with open(txt_fname, 'r') as f:\n",
    "        images = f.read().split()\n",
    "    features, labels = [], []\n",
    "    for i, fname in enumerate(images):\n",
    "        features.append(image.imread(os.path.join(\n",
    "            voc_dir, 'JPEGImages', f'{fname}.jpg')))\n",
    "        labels.append(image.imread(os.path.join(\n",
    "            voc_dir, 'SegmentationClass', f'{fname}.png')))\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = read_voc_images(voc_dir, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd63572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def read_voc_images(voc_dir, is_train=True):\n",
    "    \"\"\"读取所有VOC图像并标注\"\"\"\n",
    "    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',\n",
    "                             'train.txt' if is_train else 'val.txt')\n",
    "    mode = torchvision.io.image.ImageReadMode.RGB\n",
    "    with open(txt_fname, 'r') as f:\n",
    "        images = f.read().split()\n",
    "    features, labels = [], []\n",
    "    for i, fname in enumerate(images):\n",
    "        features.append(torchvision.io.read_image(os.path.join(\n",
    "            voc_dir, 'JPEGImages', f'{fname}.jpg')))\n",
    "        labels.append(torchvision.io.read_image(os.path.join(\n",
    "            voc_dir, 'SegmentationClass' ,f'{fname}.png'), mode))\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = read_voc_images(voc_dir, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def read_voc_images(voc_dir, is_train=True):\n",
    "    \"\"\"Defined in :numref:`sec_semantic_segmentation`\"\"\"\n",
    "    \"\"\"读取所有VOC图像并标注\n",
    "    Defined in :numref:`sec_semantic_segmentation`\"\"\"\n",
    "    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',\n",
    "                             'train.txt' if is_train else 'val.txt')\n",
    "    with open(txt_fname, 'r') as f:\n",
    "        images = f.read().split()\n",
    "    features, labels = [], []\n",
    "    for i, fname in enumerate(images):\n",
    "        features.append(paddle.vision.image.image_load(os.path.join(\n",
    "            voc_dir, 'JPEGImages', f'{fname}.jpg'), backend='cv2')[..., ::-1].transpose(\n",
    "            [2, 0, 1]))\n",
    "        labels.append(paddle.vision.image.image_load(os.path.join(\n",
    "            voc_dir, 'SegmentationClass', f'{fname}.png'), backend='cv2')[..., ::-1].transpose(\n",
    "            [2, 0, 1]))\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = read_voc_images(voc_dir, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b076cf",
   "metadata": {},
   "source": [
    "下面我们[**绘制前5个输入图像及其标签**]。\n",
    "在标签图像中，白色和黑色分别表示边框和背景，而其他颜色则对应不同的类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39813c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "imgs = train_features[0:n] + train_labels[0:n]\n",
    "d2l.show_images(imgs, 2, n);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "n = 5\n",
    "imgs = train_features[0:n] + train_labels[0:n]\n",
    "imgs = [img.permute(1,2,0) for img in imgs]\n",
    "d2l.show_images(imgs, 2, n);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "n = 5\n",
    "imgs = train_features[0:n] + train_labels[0:n]\n",
    "imgs = [img.transpose([1, 2, 0]) for img in imgs]\n",
    "d2l.show_images(imgs, 2, n);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3c767",
   "metadata": {},
   "source": [
    "接下来，我们[**列举RGB颜色值和类名**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
    "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
    "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
    "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
    "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
    "                [0, 64, 128]]\n",
    "\n",
    "#@save\n",
    "VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "               'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2652a50",
   "metadata": {},
   "source": [
    "通过上面定义的两个常量，我们可以方便地[**查找标签中每个像素的类索引**]。\n",
    "我们定义了`voc_colormap2label`函数来构建从上述RGB颜色值到类别索引的映射，而`voc_label_indices`函数将RGB值映射到在Pascal VOC2012数据集中的类别索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def voc_colormap2label():\n",
    "    \"\"\"构建从RGB到VOC类别索引的映射\"\"\"\n",
    "    colormap2label = np.zeros(256 ** 3)\n",
    "    for i, colormap in enumerate(VOC_COLORMAP):\n",
    "        colormap2label[\n",
    "            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n",
    "    return colormap2label\n",
    "\n",
    "#@save\n",
    "def voc_label_indices(colormap, colormap2label):\n",
    "    \"\"\"将VOC标签中的RGB值映射到它们的类别索引\"\"\"\n",
    "    colormap = colormap.astype(np.int32)\n",
    "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n",
    "           + colormap[:, :, 2])\n",
    "    return colormap2label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fabd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def voc_colormap2label():\n",
    "    \"\"\"构建从RGB到VOC类别索引的映射\"\"\"\n",
    "    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)\n",
    "    for i, colormap in enumerate(VOC_COLORMAP):\n",
    "        colormap2label[\n",
    "            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n",
    "    return colormap2label\n",
    "\n",
    "#@save\n",
    "def voc_label_indices(colormap, colormap2label):\n",
    "    \"\"\"将VOC标签中的RGB值映射到它们的类别索引\"\"\"\n",
    "    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')\n",
    "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n",
    "           + colormap[:, :, 2])\n",
    "    return colormap2label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0bb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def voc_colormap2label():\n",
    "    \"\"\"构建从RGB到VOC类别索引的映射\"\"\"\n",
    "    colormap2label = paddle.zeros([256 ** 3], dtype=paddle.int64)\n",
    "    for i, colormap in enumerate(VOC_COLORMAP):\n",
    "        colormap2label[\n",
    "            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n",
    "    return colormap2label\n",
    "    \n",
    "#@save\n",
    "def voc_label_indices(colormap, colormap2label):\n",
    "    \"\"\"将VOC标签中的RGB值映射到它们的类别索引\"\"\"\n",
    "    colormap = colormap.transpose([1, 2, 0]).astype('int32')\n",
    "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n",
    "           + colormap[:, :, 2])\n",
    "    return colormap2label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5929244",
   "metadata": {},
   "source": [
    "[**例如**]，在第一张样本图像中，飞机头部区域的类别索引为1，而背景索引为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "y = voc_label_indices(train_labels[0], voc_colormap2label())\n",
    "y[105:115, 130:140], VOC_CLASSES[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0c3505",
   "metadata": {},
   "source": [
    "### 预处理数据\n",
    "\n",
    "在之前的实验，例如 :numref:`sec_alexnet`— :numref:`sec_googlenet`中，我们通过再缩放图像使其符合模型的输入形状。\n",
    "然而在语义分割中，这样做需要将预测的像素类别重新映射回原始尺寸的输入图像。\n",
    "这样的映射可能不够精确，尤其在不同语义的分割区域。\n",
    "为了避免这个问题，我们将图像裁剪为固定尺寸，而不是再缩放。\n",
    "具体来说，我们[**使用图像增广中的随机裁剪，裁剪输入图像和标签的相同区域**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf25684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def voc_rand_crop(feature, label, height, width):\n",
    "    \"\"\"随机裁剪特征和标签图像\"\"\"\n",
    "    feature, rect = image.random_crop(feature, (width, height))\n",
    "    label = image.fixed_crop(label, *rect)\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f190dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def voc_rand_crop(feature, label, height, width):\n",
    "    \"\"\"随机裁剪特征和标签图像\"\"\"\n",
    "    rect = torchvision.transforms.RandomCrop.get_params(\n",
    "        feature, (height, width))\n",
    "    feature = torchvision.transforms.functional.crop(feature, *rect)\n",
    "    label = torchvision.transforms.functional.crop(label, *rect)\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f88f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def voc_rand_crop(feature, label, height, width):\n",
    "    \"\"\"随机裁剪特征和标签图像\"\"\"\n",
    "    rect = paddle.vision.transforms.RandomCrop((height, width))._get_param(\n",
    "        img=feature, output_size=(height, width))\n",
    "    feature = paddle.vision.transforms.crop(feature, *rect)\n",
    "    label = paddle.vision.transforms.crop(label, *rect)\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for _ in range(n):\n",
    "    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)\n",
    "d2l.show_images(imgs[::2] + imgs[1::2], 2, n);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e7cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "imgs = []\n",
    "for _ in range(n):\n",
    "    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)\n",
    "\n",
    "imgs = [img.permute(1, 2, 0) for img in imgs]\n",
    "d2l.show_images(imgs[::2] + imgs[1::2], 2, n);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a5d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "imgs = []\n",
    "for _ in range(n):\n",
    "    imgs += voc_rand_crop(train_features[0].transpose([1, 2, 0]), train_labels[0].transpose([1, 2, 0]), 200, 300)\n",
    "    \n",
    "imgs = [img for img in imgs]\n",
    "d2l.show_images(imgs[::2] + imgs[1::2], 2, n);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2a130",
   "metadata": {},
   "source": [
    "### [**自定义语义分割数据集类**]\n",
    "\n",
    "我们通过继承高级API提供的`Dataset`类，自定义了一个语义分割数据集类`VOCSegDataset`。\n",
    "通过实现`__getitem__`函数，我们可以任意访问数据集中索引为`idx`的输入图像及其每个像素的类别索引。\n",
    "由于数据集中有些图像的尺寸可能小于随机裁剪所指定的输出尺寸，这些样本可以通过自定义的`filter`函数移除掉。\n",
    "此外，我们还定义了`normalize_image`函数，从而对输入图像的RGB三个通道的值分别做标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d619bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class VOCSegDataset(gluon.data.Dataset):\n",
    "    \"\"\"一个用于加载VOC数据集的自定义数据集\"\"\"\n",
    "    def __init__(self, is_train, crop_size, voc_dir):\n",
    "        self.rgb_mean = np.array([0.485, 0.456, 0.406])\n",
    "        self.rgb_std = np.array([0.229, 0.224, 0.225])\n",
    "        self.crop_size = crop_size\n",
    "        features, labels = read_voc_images(voc_dir, is_train=is_train)\n",
    "        self.features = [self.normalize_image(feature)\n",
    "                         for feature in self.filter(features)]\n",
    "        self.labels = self.filter(labels)\n",
    "        self.colormap2label = voc_colormap2label()\n",
    "        print('read ' + str(len(self.features)) + ' examples')\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        return (img.astype('float32') / 255 - self.rgb_mean) / self.rgb_std\n",
    "\n",
    "    def filter(self, imgs):\n",
    "        return [img for img in imgs if (\n",
    "            img.shape[0] >= self.crop_size[0] and\n",
    "            img.shape[1] >= self.crop_size[1])]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n",
    "                                       *self.crop_size)\n",
    "        return (feature.transpose(2, 0, 1),\n",
    "                voc_label_indices(label, self.colormap2label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971bdbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class VOCSegDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"一个用于加载VOC数据集的自定义数据集\"\"\"\n",
    "\n",
    "    def __init__(self, is_train, crop_size, voc_dir):\n",
    "        self.transform = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.crop_size = crop_size\n",
    "        features, labels = read_voc_images(voc_dir, is_train=is_train)\n",
    "        self.features = [self.normalize_image(feature)\n",
    "                         for feature in self.filter(features)]\n",
    "        self.labels = self.filter(labels)\n",
    "        self.colormap2label = voc_colormap2label()\n",
    "        print('read ' + str(len(self.features)) + ' examples')\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        return self.transform(img.float() / 255)\n",
    "\n",
    "    def filter(self, imgs):\n",
    "        return [img for img in imgs if (\n",
    "            img.shape[1] >= self.crop_size[0] and\n",
    "            img.shape[2] >= self.crop_size[1])]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n",
    "                                       *self.crop_size)\n",
    "        return (feature, voc_label_indices(label, self.colormap2label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "class VOCSegDataset(paddle.io.Dataset):\n",
    "    \"\"\"一个用于加载VOC数据集的自定义数据集\n",
    "    Defined in :numref:`sec_semantic_segmentation`\"\"\"\n",
    "\n",
    "    def __init__(self, is_train, crop_size, voc_dir):\n",
    "        self.transform = paddle.vision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.crop_size = crop_size\n",
    "        features, labels = read_voc_images(voc_dir, is_train=is_train)\n",
    "        self.features = [self.normalize_image(feature)\n",
    "                         for feature in self.filter(features)]\n",
    "        self.labels = self.filter(labels)\n",
    "        self.colormap2label = voc_colormap2label()\n",
    "        print('read ' + str(len(self.features)) + ' examples')\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        return self.transform(img.astype(\"float32\") / 255)\n",
    "\n",
    "    def filter(self, imgs):\n",
    "        return [img for img in imgs if (\n",
    "            img.shape[1] >= self.crop_size[0] and\n",
    "            img.shape[2] >= self.crop_size[1])]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = paddle.to_tensor(self.features[idx],dtype='float32')\n",
    "        label = paddle.to_tensor(self.labels[idx],dtype='float32')\n",
    "        feature, label = voc_rand_crop(feature,label,\n",
    "                                       *self.crop_size)\n",
    "        return (feature, voc_label_indices(label, self.colormap2label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a38b53",
   "metadata": {},
   "source": [
    "### [**读取数据集**]\n",
    "\n",
    "我们通过自定义的`VOCSegDataset`类来分别创建训练集和测试集的实例。\n",
    "假设我们指定随机裁剪的输出图像的形状为$320\\times 480$，\n",
    "下面我们可以查看训练集和测试集所保留的样本个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7180a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "crop_size = (320, 480)\n",
    "voc_train = VOCSegDataset(True, crop_size, voc_dir)\n",
    "voc_test = VOCSegDataset(False, crop_size, voc_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37b8dd",
   "metadata": {},
   "source": [
    "设批量大小为64，我们定义训练集的迭代器。\n",
    "打印第一个小批量的形状会发现：与图像分类或目标检测不同，这里的标签是一个三维数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e639785",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iter = gluon.data.DataLoader(voc_train, batch_size, shuffle=True,\n",
    "                                   last_batch='discard',\n",
    "                                   num_workers=d2l.get_dataloader_workers())\n",
    "for X, Y in train_iter:\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "batch_size = 64\n",
    "train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=True,\n",
    "                                    drop_last=True,\n",
    "                                    num_workers=d2l.get_dataloader_workers())\n",
    "for X, Y in train_iter:\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "batch_size = 64\n",
    "train_iter = paddle.io.DataLoader(voc_train, batch_size=batch_size, shuffle=True,\n",
    "                                  drop_last=True,\n",
    "                                  return_list=True,\n",
    "                                  num_workers=d2l.get_dataloader_workers())\n",
    "for X, Y in train_iter:\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd24283",
   "metadata": {},
   "source": [
    "### [**整合所有组件**]\n",
    "\n",
    "最后，我们定义以下`load_data_voc`函数来下载并读取Pascal VOC2012语义分割数据集。\n",
    "它返回训练集和测试集的数据迭代器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_voc(batch_size, crop_size):\n",
    "    \"\"\"加载VOC语义分割数据集\"\"\"\n",
    "    voc_dir = d2l.download_extract('voc2012', os.path.join(\n",
    "        'VOCdevkit', 'VOC2012'))\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    train_iter = gluon.data.DataLoader(\n",
    "        VOCSegDataset(True, crop_size, voc_dir), batch_size,\n",
    "        shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "    test_iter = gluon.data.DataLoader(\n",
    "        VOCSegDataset(False, crop_size, voc_dir), batch_size,\n",
    "        last_batch='discard', num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358c3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def load_data_voc(batch_size, crop_size):\n",
    "    \"\"\"加载VOC语义分割数据集\"\"\"\n",
    "    voc_dir = d2l.download_extract('voc2012', os.path.join(\n",
    "        'VOCdevkit', 'VOC2012'))\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    train_iter = torch.utils.data.DataLoader(\n",
    "        VOCSegDataset(True, crop_size, voc_dir), batch_size,\n",
    "        shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(\n",
    "        VOCSegDataset(False, crop_size, voc_dir), batch_size,\n",
    "        drop_last=True, num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def load_data_voc(batch_size, crop_size):\n",
    "    \"\"\"加载VOC语义分割数据集\"\"\"\n",
    "    voc_dir = d2l.download_extract('voc2012', os.path.join(\n",
    "        'VOCdevkit', 'VOC2012'))\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    train_iter = paddle.io.DataLoader(\n",
    "        VOCSegDataset(True, crop_size, voc_dir), batch_size=batch_size,\n",
    "        shuffle=True, return_list=True, drop_last=True, num_workers=num_workers)\n",
    "    test_iter = paddle.io.DataLoader(\n",
    "        VOCSegDataset(False, crop_size, voc_dir), batch_size=batch_size,\n",
    "        drop_last=True, return_list=True, num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cf9b05",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 语义分割通过将图像划分为属于不同语义类别的区域，来识别并理解图像中像素级别的内容。\n",
    "* 语义分割的一个重要的数据集叫做Pascal VOC2012。\n",
    "* 由于语义分割的输入图像和标签在像素上一一对应，输入图像会被随机裁剪为固定尺寸而不是缩放。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如何在自动驾驶和医疗图像诊断中应用语义分割？还能想到其他领域的应用吗？\n",
    "1. 回想一下 :numref:`sec_image_augmentation`中对数据增强的描述。图像分类中使用的哪种图像增强方法是难以用于语义分割的？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/3296)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/3295)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11809)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d890fc",
   "metadata": {},
   "source": [
    "# 转置卷积\n",
    ":label:`sec_transposed_conv`\n",
    "\n",
    "到目前为止，我们所见到的卷积神经网络层，例如卷积层（ :numref:`sec_conv_layer`）和汇聚层（ :numref:`sec_pooling`），通常会减少下采样输入图像的空间维度（高和宽）。\n",
    "然而如果输入和输出图像的空间维度相同，在以像素级分类的语义分割中将会很方便。\n",
    "例如，输出像素所处的通道维可以保有输入像素在同一位置上的分类结果。\n",
    "\n",
    "为了实现这一点，尤其是在空间维度被卷积神经网络层缩小后，我们可以使用另一种类型的卷积神经网络层，它可以增加上采样中间层特征图的空间维度。\n",
    "本节将介绍\n",
    "*转置卷积*（transposed convolution） :cite:`Dumoulin.Visin.2016`，\n",
    "用于逆转下采样导致的空间尺寸减小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973985ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx, init\n",
    "from mxnet.gluon import nn\n",
    "from d2l import mxnet as d2l\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85fd64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import paddle\n",
    "from paddle import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33dc896",
   "metadata": {},
   "source": [
    "## 基本操作\n",
    "\n",
    "让我们暂时忽略通道，从基本的转置卷积开始，设步幅为1且没有填充。\n",
    "假设我们有一个$n_h \\times n_w$的输入张量和一个$k_h \\times k_w$的卷积核。\n",
    "以步幅为1滑动卷积核窗口，每行$n_w$次，每列$n_h$次，共产生$n_h n_w$个中间结果。\n",
    "每个中间结果都是一个$(n_h + k_h - 1) \\times (n_w + k_w - 1)$的张量，初始化为0。\n",
    "为了计算每个中间张量，输入张量中的每个元素都要乘以卷积核，从而使所得的$k_h \\times k_w$张量替换中间张量的一部分。\n",
    "请注意，每个中间张量被替换部分的位置与输入张量中元素的位置相对应。\n",
    "最后，所有中间结果相加以获得最终结果。\n",
    "\n",
    "例如， :numref:`fig_trans_conv`解释了如何为$2\\times 2$的输入张量计算卷积核为$2\\times 2$的转置卷积。\n",
    "\n",
    "![卷积核为 $2\\times 2$ 的转置卷积。阴影部分是中间张量的一部分，也是用于计算的输入和卷积核张量元素。 ](../img/trans_conv.svg)\n",
    ":label:`fig_trans_conv`\n",
    "\n",
    "我们可以对输入矩阵`X`和卷积核矩阵`K`(**实现基本的转置卷积运算**)`trans_conv`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def trans_conv(X, K):\n",
    "    h, w = K.shape\n",
    "    Y = d2l.zeros((X.shape[0] + h - 1, X.shape[1] + w - 1))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Y[i: i + h, j: j + w] += X[i, j] * K\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb7381",
   "metadata": {},
   "source": [
    "与通过卷积核“减少”输入元素的常规卷积（在 :numref:`sec_conv_layer`中）相比，转置卷积通过卷积核“广播”输入元素，从而产生大于输入的输出。\n",
    "我们可以通过 :numref:`fig_trans_conv`来构建输入张量`X`和卷积核张量`K`从而[**验证上述实现输出**]。\n",
    "此实现是基本的二维转置卷积运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca02e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "X = d2l.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "K = d2l.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "trans_conv(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb628d",
   "metadata": {},
   "source": [
    "或者，当输入`X`和卷积核`K`都是四维张量时，我们可以[**使用高级API获得相同的结果**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\n",
    "tconv = nn.Conv2DTranspose(1, kernel_size=2)\n",
    "tconv.initialize(init.Constant(K))\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac88f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "X, K = X.reshape(1, 1, 2, 2), K.reshape(1, 1, 2, 2)\n",
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b789c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "X, K = X.reshape([1, 1, 2, 2]), K.reshape([1, 1, 2, 2])\n",
    "tconv = nn.Conv2DTranspose(1, 1, kernel_size=2, bias_attr=False)\n",
    "K = paddle.create_parameter(shape=K.shape, dtype=\"float32\", \n",
    "        default_initializer=paddle.nn.initializer.Assign(K))\n",
    "tconv.weight = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b93d820",
   "metadata": {},
   "source": [
    "## [**填充、步幅和多通道**]\n",
    "\n",
    "与常规卷积不同，在转置卷积中，填充被应用于的输出（常规卷积将填充应用于输入）。\n",
    "例如，当将高和宽两侧的填充数指定为1时，转置卷积的输出中将删除第一和最后的行与列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e6c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tconv = nn.Conv2DTranspose(1, kernel_size=2, padding=1)\n",
    "tconv.initialize(init.Constant(K))\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f974bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "tconv = nn.Conv2DTranspose(1, 1, kernel_size=2, padding=1, bias_attr=False)\n",
    "tconv.weight = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92660adf",
   "metadata": {},
   "source": [
    "在转置卷积中，步幅被指定为中间结果（输出），而不是输入。\n",
    "使用 :numref:`fig_trans_conv`中相同输入和卷积核张量，将步幅从1更改为2会增加中间张量的高和权重，因此输出张量在 :numref:`fig_trans_conv_stride2`中。\n",
    "\n",
    "![卷积核为$2\\times 2$，步幅为2的转置卷积。阴影部分是中间张量的一部分，也是用于计算的输入和卷积核张量元素。](../img/trans_conv_stride2.svg)\n",
    ":label:`fig_trans_conv_stride2`\n",
    "\n",
    "以下代码可以验证 :numref:`fig_trans_conv_stride2`中步幅为2的转置卷积的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbffea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tconv = nn.Conv2DTranspose(1, kernel_size=2, strides=2)\n",
    "tconv.initialize(init.Constant(K))\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2fc80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, stride=2, bias=False)\n",
    "tconv.weight.data = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a2d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "tconv = nn.Conv2DTranspose(1, 1, kernel_size=2, stride=2, bias_attr=False)\n",
    "tconv.weight = K\n",
    "tconv(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ccfa3b",
   "metadata": {},
   "source": [
    "对于多个输入和输出通道，转置卷积与常规卷积以相同方式运作。\n",
    "假设输入有$c_i$个通道，且转置卷积为每个输入通道分配了一个$k_h\\times k_w$的卷积核张量。\n",
    "当指定多个输出通道时，每个输出通道将有一个$c_i\\times k_h\\times k_w$的卷积核。\n",
    "\n",
    "同样，如果我们将$\\mathsf{X}$代入卷积层$f$来输出$\\mathsf{Y}=f(\\mathsf{X})$，并创建一个与$f$具有相同的超参数、但输出通道数量是$\\mathsf{X}$中通道数的转置卷积层$g$，那么$g(Y)$的形状将与$\\mathsf{X}$相同。\n",
    "下面的示例可以解释这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(size=(1, 10, 16, 16))\n",
    "conv = nn.Conv2D(20, kernel_size=5, padding=2, strides=3)\n",
    "tconv = nn.Conv2DTranspose(10, kernel_size=5, padding=2, strides=3)\n",
    "conv.initialize()\n",
    "tconv.initialize()\n",
    "tconv(conv(X)).shape == X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2907182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "X = torch.rand(size=(1, 10, 16, 16))\n",
    "conv = nn.Conv2d(10, 20, kernel_size=5, padding=2, stride=3)\n",
    "tconv = nn.ConvTranspose2d(20, 10, kernel_size=5, padding=2, stride=3)\n",
    "tconv(conv(X)).shape == X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "X = paddle.rand(shape=(1, 10, 16, 16))\n",
    "conv = nn.Conv2D(10, 20, kernel_size=5, padding=2, stride=3)\n",
    "tconv = nn.Conv2DTranspose(20, 10, kernel_size=5, padding=2, stride=3)\n",
    "tconv(conv(X)).shape == X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f6777e",
   "metadata": {},
   "source": [
    "## [**与矩阵变换的联系**]\n",
    ":label:`subsec-connection-to-mat-transposition`\n",
    "\n",
    "转置卷积为何以矩阵变换命名呢？\n",
    "让我们首先看看如何使用矩阵乘法来实现卷积。\n",
    "在下面的示例中，我们定义了一个$3\\times 3$的输入`X`和$2\\times 2$卷积核`K`，然后使用`corr2d`函数计算卷积输出`Y`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca2906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "X = d2l.arange(9.0).reshape(3, 3)\n",
    "K = d2l.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "Y = d2l.corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "X = d2l.arange(9.0, dtype=\"float32\").reshape((3, 3))\n",
    "K = d2l.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "Y = d2l.corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a781632",
   "metadata": {},
   "source": [
    "接下来，我们将卷积核`K`重写为包含大量0的稀疏权重矩阵`W`。\n",
    "权重矩阵的形状是（$4$，$9$），其中非0元素来自卷积核`K`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0417fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "def kernel2matrix(K):\n",
    "    k, W = d2l.zeros(5), d2l.zeros((4, 9))\n",
    "    k[:2], k[3:5] = K[0, :], K[1, :]\n",
    "    W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k\n",
    "    return W\n",
    "\n",
    "W = kernel2matrix(K)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def kernel2matrix(K):\n",
    "    k, W = d2l.zeros([5]), d2l.zeros((4, 9))\n",
    "    k[:2], k[3:5] = K[0, :], K[1, :]\n",
    "    W[0, :5], W[1, 1:6], W[2, 3:8], W[3, 4:] = k, k, k, k\n",
    "    return W\n",
    "\n",
    "W = kernel2matrix(K)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac3b42",
   "metadata": {},
   "source": [
    "逐行连结输入`X`，获得了一个长度为9的矢量。\n",
    "然后，`W`的矩阵乘法和向量化的`X`给出了一个长度为4的向量。\n",
    "重塑它之后，可以获得与上面的原始卷积操作所得相同的结果`Y`：我们刚刚使用矩阵乘法实现了卷积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab5138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "Y == d2l.matmul(W, d2l.reshape(X, -1)).reshape(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "Y == d2l.matmul(W, d2l.reshape(X, [-1])).reshape((2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba414a6",
   "metadata": {},
   "source": [
    "同样，我们可以使用矩阵乘法来实现转置卷积。\n",
    "在下面的示例中，我们将上面的常规卷积$2 \\times 2$的输出`Y`作为转置卷积的输入。\n",
    "想要通过矩阵相乘来实现它，我们只需要将权重矩阵`W`的形状转置为$(9, 4)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "Z = trans_conv(Y, K)\n",
    "Z == d2l.matmul(W.T, d2l.reshape(Y, -1)).reshape(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ca549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "Z = trans_conv(Y, K)\n",
    "Z == d2l.matmul(W.T, d2l.reshape(Y, [-1])).reshape((3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f29d46",
   "metadata": {},
   "source": [
    "抽象来看，给定输入向量$\\mathbf{x}$和权重矩阵$\\mathbf{W}$，卷积的前向传播函数可以通过将其输入与权重矩阵相乘并输出向量$\\mathbf{y}=\\mathbf{W}\\mathbf{x}$来实现。\n",
    "由于反向传播遵循链式法则和$\\nabla_{\\mathbf{x}}\\mathbf{y}=\\mathbf{W}^\\top$，卷积的反向传播函数可以通过将其输入与转置的权重矩阵$\\mathbf{W}^\\top$相乘来实现。\n",
    "因此，转置卷积层能够交换卷积层的正向传播函数和反向传播函数：它的正向传播和反向传播函数将输入向量分别与$\\mathbf{W}^\\top$和$\\mathbf{W}$相乘。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 与通过卷积核减少输入元素的常规卷积相反，转置卷积通过卷积核广播输入元素，从而产生形状大于输入的输出。\n",
    "* 如果我们将$\\mathsf{X}$输入卷积层$f$来获得输出$\\mathsf{Y}=f(\\mathsf{X})$并创造一个与$f$有相同的超参数、但输出通道数是$\\mathsf{X}$中通道数的转置卷积层$g$，那么$g(Y)$的形状将与$\\mathsf{X}$相同。\n",
    "* 我们可以使用矩阵乘法来实现卷积。转置卷积层能够交换卷积层的正向传播函数和反向传播函数。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 在 :numref:`subsec-connection-to-mat-transposition`中，卷积输入`X`和转置的卷积输出`Z`具有相同的形状。他们的数值也相同吗？为什么？\n",
    "1. 使用矩阵乘法来实现卷积是否有效率？为什么？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/3301)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/3302)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11810)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4de6df2",
   "metadata": {},
   "source": [
    "# 全卷积网络\n",
    ":label:`sec_fcn`\n",
    "\n",
    "如 :numref:`sec_semantic_segmentation`中所介绍的那样，语义分割是对图像中的每个像素分类。\n",
    "*全卷积网络*（fully convolutional network，FCN）采用卷积神经网络实现了从图像像素到像素类别的变换 :cite:`Long.Shelhamer.Darrell.2015`。\n",
    "与我们之前在图像分类或目标检测部分介绍的卷积神经网络不同，全卷积网络将中间层特征图的高和宽变换回输入图像的尺寸：这是通过在 :numref:`sec_transposed_conv`中引入的*转置卷积*（transposed convolution）实现的。\n",
    "因此，输出的类别预测与输入图像在像素级别上具有一一对应关系：通道维的输出即该位置对应像素的类别预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c688069",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, image, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f671e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec215e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "from paddle import nn\n",
    "from paddle.nn import functional as F\n",
    "import paddle.vision as paddlevision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7613df8a",
   "metadata": {},
   "source": [
    "## 构造模型\n",
    "\n",
    "下面我们了解一下全卷积网络模型最基本的设计。\n",
    "如 :numref:`fig_fcn`所示，全卷积网络先使用卷积神经网络抽取图像特征，然后通过$1\\times 1$卷积层将通道数变换为类别个数，最后在 :numref:`sec_transposed_conv`中通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。\n",
    "因此，模型输出与输入图像的高和宽相同，且最终输出通道包含了该空间位置像素的类别预测。\n",
    "\n",
    "![全卷积网络](../img/fcn.svg)\n",
    ":label:`fig_fcn`\n",
    "\n",
    "下面，我们[**使用在ImageNet数据集上预训练的ResNet-18模型来提取图像特征**]，并将该网络记为`pretrained_net`。\n",
    "ResNet-18模型的最后几层包括全局平均汇聚层和全连接层，然而全卷积网络中不需要它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_net = gluon.model_zoo.vision.resnet18_v2(pretrained=True)\n",
    "pretrained_net.features[-3:], pretrained_net.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66700072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "pretrained_net = torchvision.models.resnet18(pretrained=True)\n",
    "list(pretrained_net.children())[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45c9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "pretrained_net = paddlevision.models.resnet18(pretrained=True)\n",
    "list(pretrained_net.children())[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb76ec",
   "metadata": {},
   "source": [
    "接下来，我们[**创建一个全卷积网络`net`**]。\n",
    "它复制了ResNet-18中大部分的预训练层，除了最后的全局平均汇聚层和最接近输出的全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f47750",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.HybridSequential()\n",
    "for layer in pretrained_net.features[:-2]:\n",
    "    net.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "net = nn.Sequential(*list(pretrained_net.children())[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38dbc9a",
   "metadata": {},
   "source": [
    "给定高度为320和宽度为480的输入，`net`的前向传播将输入的高和宽减小至原来的$1/32$，即10和15。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(size=(1, 3, 320, 480))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9736119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "X = torch.rand(size=(1, 3, 320, 480))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c48e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "X = paddle.rand(shape=(1, 3, 320, 480))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cba9a5",
   "metadata": {},
   "source": [
    "接下来[**使用$1\\times1$卷积层将输出通道数转换为Pascal VOC2012数据集的类数（21类）。**]\n",
    "最后需要(**将特征图的高度和宽度增加32倍**)，从而将其变回输入图像的高和宽。\n",
    "回想一下 :numref:`sec_padding`中卷积层输出形状的计算方法：\n",
    "由于$(320-64+16\\times2+32)/32=10$且$(480-64+16\\times2+32)/32=15$，我们构造一个步幅为$32$的转置卷积层，并将卷积核的高和宽设为$64$，填充为$16$。\n",
    "我们可以看到如果步幅为$s$，填充为$s/2$（假设$s/2$是整数）且卷积核的高和宽为$2s$，转置卷积核会将输入的高和宽分别放大$s$倍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1573088",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 21\n",
    "net.add(nn.Conv2D(num_classes, kernel_size=1),\n",
    "        nn.Conv2DTranspose(\n",
    "            num_classes, kernel_size=64, padding=16, strides=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "num_classes = 21\n",
    "net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))\n",
    "net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes, num_classes,\n",
    "                                    kernel_size=64, padding=16, stride=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea0e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "num_classes = 21\n",
    "net.add_sublayer('final_conv', nn.Conv2D(512, num_classes, kernel_size=1))\n",
    "net.add_sublayer('transpose_conv', nn.Conv2DTranspose(num_classes, num_classes,\n",
    "                                    kernel_size=64, padding=16, stride=32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a95201a",
   "metadata": {},
   "source": [
    "## [**初始化转置卷积层**]\n",
    "\n",
    "在图像处理中，我们有时需要将图像放大，即*上采样*（upsampling）。\n",
    "*双线性插值*（bilinear interpolation）\n",
    "是常用的上采样方法之一，它也经常用于初始化转置卷积层。\n",
    "\n",
    "为了解释双线性插值，假设给定输入图像，我们想要计算上采样输出图像上的每个像素。\n",
    "\n",
    "1. 将输出图像的坐标$(x,y)$映射到输入图像的坐标$(x',y')$上。\n",
    "例如，根据输入与输出的尺寸之比来映射。\n",
    "请注意，映射后的$x′$和$y′$是实数。\n",
    "2. 在输入图像上找到离坐标$(x',y')$最近的4个像素。\n",
    "3. 输出图像在坐标$(x,y)$上的像素依据输入图像上这4个像素及其与$(x',y')$的相对距离来计算。\n",
    "\n",
    "双线性插值的上采样可以通过转置卷积层实现，内核由以下`bilinear_kernel`函数构造。\n",
    "限于篇幅，我们只给出`bilinear_kernel`函数的实现，不讨论算法的原理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e26461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (np.arange(kernel_size).reshape(-1, 1),\n",
    "          np.arange(kernel_size).reshape(1, -1))\n",
    "    filt = (1 - np.abs(og[0] - center) / factor) * \\\n",
    "           (1 - np.abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return np.array(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b6308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (torch.arange(kernel_size).reshape(-1, 1),\n",
    "          torch.arange(kernel_size).reshape(1, -1))\n",
    "    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n",
    "           (1 - torch.abs(og[1] - center) / factor)\n",
    "    weight = torch.zeros((in_channels, out_channels,\n",
    "                          kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (paddle.arange(kernel_size).reshape([-1, 1]),\n",
    "          paddle.arange(kernel_size).reshape([1, -1]))\n",
    "    filt = (1 - paddle.abs(og[0] - center) / factor) * \\\n",
    "           (1 - paddle.abs(og[1] - center) / factor)\n",
    "    weight = paddle.zeros((in_channels, out_channels,\n",
    "                          kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc4d6ad",
   "metadata": {},
   "source": [
    "让我们用[**双线性插值的上采样实验**]它由转置卷积层实现。\n",
    "我们构造一个将输入的高和宽放大2倍的转置卷积层，并将其卷积核用`bilinear_kernel`函数初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e875809",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_trans = nn.Conv2DTranspose(3, kernel_size=4, padding=1, strides=2)\n",
    "conv_trans.initialize(init.Constant(bilinear_kernel(3, 3, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "conv_trans = nn.ConvTranspose2d(3, 3, kernel_size=4, padding=1, stride=2,\n",
    "                                bias=False)\n",
    "conv_trans.weight.data.copy_(bilinear_kernel(3, 3, 4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ab88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "conv_trans = nn.Conv2DTranspose(3, 3, kernel_size=4, padding=1, stride=2,\n",
    "                                bias_attr=False)\n",
    "conv_trans.weight.set_value(bilinear_kernel(3, 3, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f636cae",
   "metadata": {},
   "source": [
    "读取图像`X`，将上采样的结果记作`Y`。为了打印图像，我们需要调整通道维的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.imread('../img/catdog.jpg')\n",
    "X = np.expand_dims(img.astype('float32').transpose(2, 0, 1), axis=0) / 255\n",
    "Y = conv_trans(X)\n",
    "out_img = Y[0].transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "img = torchvision.transforms.ToTensor()(d2l.Image.open('../img/catdog.jpg'))\n",
    "X = img.unsqueeze(0)\n",
    "Y = conv_trans(X)\n",
    "out_img = Y[0].permute(1, 2, 0).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5858400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "img = paddlevision.transforms.ToTensor()(d2l.Image.open('../img/catdog.jpg'))\n",
    "X = img.unsqueeze(0)\n",
    "Y = conv_trans(X)\n",
    "out_img = Y[0].transpose([1, 2, 0]).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcfae04",
   "metadata": {},
   "source": [
    "可以看到，转置卷积层将图像的高和宽分别放大了2倍。\n",
    "除了坐标刻度不同，双线性插值放大的图像和在 :numref:`sec_bbox`中打印出的原图看上去没什么两样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "print('input image shape:', img.shape)\n",
    "d2l.plt.imshow(img.asnumpy());\n",
    "print('output image shape:', out_img.shape)\n",
    "d2l.plt.imshow(out_img.asnumpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed546163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "d2l.set_figsize()\n",
    "print('input image shape:', img.permute(1, 2, 0).shape)\n",
    "d2l.plt.imshow(img.permute(1, 2, 0));\n",
    "print('output image shape:', out_img.shape)\n",
    "d2l.plt.imshow(out_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827d5255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "d2l.set_figsize()\n",
    "print('input image shape:', img.transpose([1, 2, 0]).shape)\n",
    "d2l.plt.imshow(img.transpose([1, 2, 0]));\n",
    "print('output image shape:', out_img.shape)\n",
    "d2l.plt.imshow(out_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a4955a",
   "metadata": {},
   "source": [
    "全卷积网络[**用双线性插值的上采样初始化转置卷积层。对于$1\\times 1$卷积层，我们使用Xavier初始化参数。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b20a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = bilinear_kernel(num_classes, num_classes, 64)\n",
    "net[-1].initialize(init.Constant(W))\n",
    "net[-2].initialize(init=init.Xavier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac04503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "W = bilinear_kernel(num_classes, num_classes, 64)\n",
    "net.transpose_conv.weight.data.copy_(W);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "W = bilinear_kernel(num_classes, num_classes, 64)\n",
    "net.transpose_conv.weight.set_value(W);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f676329",
   "metadata": {},
   "source": [
    "## [**读取数据集**]\n",
    "\n",
    "我们用 :numref:`sec_semantic_segmentation`中介绍的语义分割读取数据集。\n",
    "指定随机裁剪的输出图像的形状为$320\\times 480$：高和宽都可以被$32$整除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc606194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "batch_size, crop_size = 32, (320, 480)\n",
    "train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9383a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import os    \n",
    "def load_data_voc(batch_size, crop_size):\n",
    "    \"\"\"加载VOC语义分割数据集\n",
    "    Defined in :numref:`sec_semantic_segmentation`\"\"\"\n",
    "    voc_dir = d2l.download_extract('voc2012', os.path.join(\n",
    "        'VOCdevkit', 'VOC2012'))\n",
    "    train_iter = paddle.io.DataLoader(\n",
    "        d2l.VOCSegDataset(True, crop_size, voc_dir), batch_size=batch_size,\n",
    "        shuffle=True, return_list=True, drop_last=True, num_workers=0)\n",
    "    test_iter = paddle.io.DataLoader(\n",
    "        d2l.VOCSegDataset(False, crop_size, voc_dir), batch_size=batch_size,\n",
    "        drop_last=True, return_list=True, num_workers=0)\n",
    "    return train_iter, test_iter\n",
    "\n",
    "batch_size, crop_size = 32, (320, 480)\n",
    "train_iter, test_iter = load_data_voc(batch_size, crop_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93665dea",
   "metadata": {},
   "source": [
    "## [**训练**]\n",
    "\n",
    "现在我们可以训练全卷积网络了。\n",
    "这里的损失函数和准确率计算与图像分类中的并没有本质上的不同，因为我们使用转置卷积层的通道来预测像素的类别，所以需要在损失计算中指定通道维。\n",
    "此外，模型基于每个像素的预测类别是否正确来计算准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4531fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr, wd, devices = 5, 0.1, 1e-3, d2l.try_all_gpus()\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss(axis=1)\n",
    "net.collect_params().reset_ctx(devices)\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd',\n",
    "                        {'learning_rate': lr, 'wd': wd})\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3c9c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def loss(inputs, targets):\n",
    "    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)\n",
    "\n",
    "num_epochs, lr, wd, devices = 5, 0.001, 1e-3, d2l.try_all_gpus()\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def loss(inputs, targets):\n",
    "    return F.cross_entropy(inputs.transpose([0, 2, 3, 1]), targets, reduction='none').mean(1).mean(1)\n",
    "\n",
    "num_epochs, lr, wd, devices = 5, 0.001, 1e-3, d2l.try_all_gpus()\n",
    "trainer = paddle.optimizer.SGD(learning_rate=lr, parameters=net.parameters(), weight_decay=wd)\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6effac87",
   "metadata": {},
   "source": [
    "## [**预测**]\n",
    "\n",
    "在预测时，我们需要将输入图像在各个通道做标准化，并转成卷积神经网络所需要的四维输入格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    X = test_iter._dataset.normalize_image(img)\n",
    "    X = np.expand_dims(X.transpose(2, 0, 1), axis=0)\n",
    "    pred = net(X.as_in_ctx(devices[0])).argmax(axis=1)\n",
    "    return pred.reshape(pred.shape[1], pred.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c230c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def predict(img):\n",
    "    X = test_iter.dataset.normalize_image(img).unsqueeze(0)\n",
    "    pred = net(X.to(devices[0])).argmax(dim=1)\n",
    "    return pred.reshape(pred.shape[1], pred.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fbc5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def predict(img):\n",
    "    X = paddle.to_tensor(test_iter.dataset.normalize_image(img),dtype='float32').unsqueeze(0)\n",
    "    pred = net(X).argmax(axis=1)\n",
    "    return pred.reshape([pred.shape[1], pred.shape[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f27500",
   "metadata": {},
   "source": [
    "为了[**可视化预测的类别**]给每个像素，我们将预测类别映射回它们在数据集中的标注颜色。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169deb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2image(pred):\n",
    "    colormap = np.array(d2l.VOC_COLORMAP, ctx=devices[0], dtype='uint8')\n",
    "    X = pred.astype('int32')\n",
    "    return colormap[X, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb76e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def label2image(pred):\n",
    "    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[0])\n",
    "    X = pred.long()\n",
    "    return colormap[X, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69681c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def label2image(pred):\n",
    "    colormap = paddle.to_tensor(d2l.VOC_COLORMAP)\n",
    "    X = pred.astype(paddle.int32)\n",
    "    return colormap[X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033cc2b",
   "metadata": {},
   "source": [
    "测试数据集中的图像大小和形状各异。\n",
    "由于模型使用了步幅为32的转置卷积层，因此当输入图像的高或宽无法被32整除时，转置卷积层输出的高或宽会与输入图像的尺寸有偏差。\n",
    "为了解决这个问题，我们可以在图像中截取多块高和宽为32的整数倍的矩形区域，并分别对这些区域中的像素做前向传播。\n",
    "请注意，这些区域的并集需要完整覆盖输入图像。\n",
    "当一个像素被多个区域所覆盖时，它在不同区域前向传播中转置卷积层输出的平均值可以作为`softmax`运算的输入，从而预测类别。\n",
    "\n",
    "为简单起见，我们只读取几张较大的测试图像，并从图像的左上角开始截取形状为$320\\times480$的区域用于预测。\n",
    "对于这些测试图像，我们逐一打印它们截取的区域，再打印预测结果，最后打印标注的类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\n",
    "test_images, test_labels = d2l.read_voc_images(voc_dir, False)\n",
    "n, imgs = 4, []\n",
    "for i in range(n):\n",
    "    crop_rect = (0, 0, 480, 320)\n",
    "    X = image.fixed_crop(test_images[i], *crop_rect)\n",
    "    pred = label2image(predict(X))\n",
    "    imgs += [X, pred, image.fixed_crop(test_labels[i], *crop_rect)]\n",
    "d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31811ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\n",
    "test_images, test_labels = d2l.read_voc_images(voc_dir, False)\n",
    "n, imgs = 4, []\n",
    "for i in range(n):\n",
    "    crop_rect = (0, 0, 320, 480)\n",
    "    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)\n",
    "    pred = label2image(predict(X))\n",
    "    imgs += [X.permute(1,2,0), pred.cpu(),\n",
    "             torchvision.transforms.functional.crop(\n",
    "                 test_labels[i], *crop_rect).permute(1,2,0)]\n",
    "d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cde37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\n",
    "test_images, test_labels = d2l.read_voc_images(voc_dir, False)\n",
    "n, imgs = 4, []\n",
    "for i in range(n):\n",
    "    crop_rect = (0, 0, 320, 480)\n",
    "    X = paddlevision.transforms.crop(test_images[i], *crop_rect)\n",
    "    pred = label2image(predict(X))\n",
    "    imgs += [X.transpose([1,2,0]).astype('uint8'), pred,\n",
    "             paddlevision.transforms.crop(\n",
    "                 test_labels[i], *crop_rect).transpose([1, 2, 0]).astype(\"uint8\")]\n",
    "d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986dc4d1",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 全卷积网络先使用卷积神经网络抽取图像特征，然后通过$1\\times 1$卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸。\n",
    "* 在全卷积网络中，我们可以将转置卷积层初始化为双线性插值的上采样。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果将转置卷积层改用Xavier随机初始化，结果有什么变化？\n",
    "1. 调节超参数，能进一步提升模型的精度吗？\n",
    "1. 预测测试图像中所有像素的类别。\n",
    "1. 最初的全卷积网络的论文中 :cite:`Long.Shelhamer.Darrell.2015`还使用了某些卷积神经网络中间层的输出。试着实现这个想法。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/3298)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/3297)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11811)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d4d0e",
   "metadata": {},
   "source": [
    "# 风格迁移\n",
    "\n",
    "摄影爱好者也许接触过滤波器。它能改变照片的颜色风格，从而使风景照更加锐利或者令人像更加美白。但一个滤波器通常只能改变照片的某个方面。如果要照片达到理想中的风格，可能需要尝试大量不同的组合。这个过程的复杂程度不亚于模型调参。\n",
    "\n",
    "本节将介绍如何使用卷积神经网络，自动将一个图像中的风格应用在另一图像之上，即*风格迁移*（style transfer） :cite:`Gatys.Ecker.Bethge.2016`。\n",
    "这里我们需要两张输入图像：一张是*内容图像*，另一张是*风格图像*。\n",
    "我们将使用神经网络修改内容图像，使其在风格上接近风格图像。\n",
    "例如， :numref:`fig_style_transfer`中的内容图像为本书作者在西雅图郊区的雷尼尔山国家公园拍摄的风景照，而风格图像则是一幅主题为秋天橡树的油画。\n",
    "最终输出的合成图像应用了风格图像的油画笔触让整体颜色更加鲜艳，同时保留了内容图像中物体主体的形状。\n",
    "\n",
    "![输入内容图像和风格图像，输出风格迁移后的合成图像](../img/style-transfer.svg)\n",
    ":label:`fig_style_transfer`\n",
    "\n",
    "## 方法\n",
    "\n",
    " :numref:`fig_style_transfer_model`用简单的例子阐述了基于卷积神经网络的风格迁移方法。\n",
    "首先，我们初始化合成图像，例如将其初始化为内容图像。\n",
    "该合成图像是风格迁移过程中唯一需要更新的变量，即风格迁移所需迭代的模型参数。\n",
    "然后，我们选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中无须更新。\n",
    "这个深度卷积神经网络凭借多个层逐级抽取图像的特征，我们可以选择其中某些层的输出作为内容特征或风格特征。\n",
    "以 :numref:`fig_style_transfer_model`为例，这里选取的预训练的神经网络含有3个卷积层，其中第二层输出内容特征，第一层和第三层输出风格特征。\n",
    "\n",
    "![基于卷积神经网络的风格迁移。实线箭头和虚线箭头分别表示前向传播和反向传播](../img/neural-style.svg)\n",
    ":label:`fig_style_transfer_model`\n",
    "\n",
    "接下来，我们通过前向传播（实线箭头方向）计算风格迁移的损失函数，并通过反向传播（虚线箭头方向）迭代模型参数，即不断更新合成图像。\n",
    "风格迁移常用的损失函数由3部分组成：\n",
    "\n",
    "1. *内容损失*使合成图像与内容图像在内容特征上接近；\n",
    "1. *风格损失*使合成图像与风格图像在风格特征上接近；\n",
    "1. *全变分损失*则有助于减少合成图像中的噪点。\n",
    "\n",
    "最后，当模型训练结束时，我们输出风格迁移的模型参数，即得到最终的合成图像。\n",
    "\n",
    "在下面，我们将通过代码来进一步了解风格迁移的技术细节。\n",
    "\n",
    "## [**阅读内容和风格图像**]\n",
    "\n",
    "首先，我们读取内容和风格图像。\n",
    "从打印出的图像坐标轴可以看出，它们的尺寸并不一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, image, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "d2l.set_figsize()\n",
    "content_img = image.imread('../img/rainier.jpg')\n",
    "d2l.plt.imshow(content_img.asnumpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46262ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "d2l.set_figsize()\n",
    "content_img = d2l.Image.open('../img/rainier.jpg')\n",
    "d2l.plt.imshow(content_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "%matplotlib inline\n",
    "from d2l import paddle as d2l\n",
    "import paddle\n",
    "import paddle.vision as paddlevision\n",
    "import paddle.nn as nn\n",
    "\n",
    "d2l.set_figsize()\n",
    "content_img = d2l.Image.open('../img/rainier.jpg')\n",
    "d2l.plt.imshow(content_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43777f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_img = image.imread('../img/autumn-oak.jpg')\n",
    "d2l.plt.imshow(style_img.asnumpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "style_img = d2l.Image.open('../img/autumn-oak.jpg')\n",
    "d2l.plt.imshow(style_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51a735f",
   "metadata": {},
   "source": [
    "## [**预处理和后处理**]\n",
    "\n",
    "下面，定义图像的预处理函数和后处理函数。\n",
    "预处理函数`preprocess`对输入图像在RGB三个通道分别做标准化，并将结果变换成卷积神经网络接受的输入格式。\n",
    "后处理函数`postprocess`则将输出图像中的像素值还原回标准化之前的值。\n",
    "由于图像打印函数要求每个像素的浮点数值在0～1之间，我们对小于0和大于1的值分别取0和1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09609f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_mean = np.array([0.485, 0.456, 0.406])\n",
    "rgb_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess(img, image_shape):\n",
    "    img = image.imresize(img, *image_shape)\n",
    "    img = (img.astype('float32') / 255 - rgb_mean) / rgb_std\n",
    "    return np.expand_dims(img.transpose(2, 0, 1), axis=0)\n",
    "\n",
    "def postprocess(img):\n",
    "    img = img[0].as_in_ctx(rgb_std.ctx)\n",
    "    return (img.transpose(1, 2, 0) * rgb_std + rgb_mean).clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f62434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "rgb_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess(img, image_shape):\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_shape),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n",
    "    return transforms(img).unsqueeze(0)\n",
    "\n",
    "def postprocess(img):\n",
    "    img = img[0].to(rgb_std.device)\n",
    "    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)\n",
    "    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a991930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "rgb_mean = paddle.to_tensor([0.485, 0.456, 0.406])\n",
    "rgb_std = paddle.to_tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess(img, image_shape):\n",
    "    transforms = paddlevision.transforms.Compose([\n",
    "        paddlevision.transforms.Resize(image_shape),\n",
    "        paddlevision.transforms.ToTensor(),\n",
    "        paddlevision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n",
    "    return transforms(img).unsqueeze(0)\n",
    "\n",
    "def postprocess(img):\n",
    "    img = img[0]\n",
    "    img = paddle.clip(img.transpose((1, 2, 0)) * rgb_std + rgb_mean, 0, 1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0951d8d",
   "metadata": {},
   "source": [
    "## [**抽取图像特征**]\n",
    "\n",
    "我们使用基于ImageNet数据集预训练的VGG-19模型来抽取图像特征 :cite:`Gatys.Ecker.Bethge.2016`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf8b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_net = gluon.model_zoo.vision.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "pretrained_net = torchvision.models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac02ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "pretrained_net = paddlevision.models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244e8f9",
   "metadata": {},
   "source": [
    "为了抽取图像的内容特征和风格特征，我们可以选择VGG网络中某些层的输出。\n",
    "一般来说，越靠近输入层，越容易抽取图像的细节信息；反之，则越容易抽取图像的全局信息。\n",
    "为了避免合成图像过多保留内容图像的细节，我们选择VGG较靠近输出的层，即*内容层*，来输出图像的内容特征。\n",
    "我们还从VGG中选择不同层的输出来匹配局部和全局的风格，这些图层也称为*风格层*。\n",
    "正如 :numref:`sec_vgg`中所介绍的，VGG网络使用了5个卷积块。\n",
    "实验中，我们选择第四卷积块的最后一个卷积层作为内容层，选择每个卷积块的第一个卷积层作为风格层。\n",
    "这些层的索引可以通过打印`pretrained_net`实例获取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "style_layers, content_layers = [0, 5, 10, 19, 28], [25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbe05a3",
   "metadata": {},
   "source": [
    "使用VGG层抽取特征时，我们只需要用到从输入层到最靠近输出层的内容层或风格层之间的所有层。\n",
    "下面构建一个新的网络`net`，它只保留需要用到的VGG的所有层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec045b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "for i in range(max(content_layers + style_layers) + 1):\n",
    "    net.add(pretrained_net.features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0945a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "net = nn.Sequential(*[pretrained_net.features[i] for i in\n",
    "                      range(max(content_layers + style_layers) + 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8681312",
   "metadata": {},
   "source": [
    "给定输入`X`，如果我们简单地调用前向传播`net(X)`，只能获得最后一层的输出。\n",
    "由于我们还需要中间层的输出，因此这里我们逐层计算，并保留内容层和风格层的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def extract_features(X, content_layers, style_layers):\n",
    "    contents = []\n",
    "    styles = []\n",
    "    for i in range(len(net)):\n",
    "        X = net[i](X)\n",
    "        if i in style_layers:\n",
    "            styles.append(X)\n",
    "        if i in content_layers:\n",
    "            contents.append(X)\n",
    "    return contents, styles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2eb155",
   "metadata": {},
   "source": [
    "下面定义两个函数：`get_contents`函数对内容图像抽取内容特征；\n",
    "`get_styles`函数对风格图像抽取风格特征。\n",
    "因为在训练时无须改变预训练的VGG的模型参数，所以我们可以在训练开始之前就提取出内容特征和风格特征。\n",
    "由于合成图像是风格迁移所需迭代的模型参数，我们只能在训练过程中通过调用`extract_features`函数来抽取合成图像的内容特征和风格特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97850b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(image_shape, device):\n",
    "    content_X = preprocess(content_img, image_shape).copyto(device)\n",
    "    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n",
    "    return content_X, contents_Y\n",
    "\n",
    "def get_styles(image_shape, device):\n",
    "    style_X = preprocess(style_img, image_shape).copyto(device)\n",
    "    _, styles_Y = extract_features(style_X, content_layers, style_layers)\n",
    "    return style_X, styles_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e65cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_contents(image_shape, device):\n",
    "    content_X = preprocess(content_img, image_shape).to(device)\n",
    "    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n",
    "    return content_X, contents_Y\n",
    "\n",
    "def get_styles(image_shape, device):\n",
    "    style_X = preprocess(style_img, image_shape).to(device)\n",
    "    _, styles_Y = extract_features(style_X, content_layers, style_layers)\n",
    "    return style_X, styles_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d826c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def get_contents(image_shape):\n",
    "    content_X = preprocess(content_img, image_shape)\n",
    "    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n",
    "    return content_X, contents_Y\n",
    "\n",
    "def get_styles(image_shape):\n",
    "    style_X = preprocess(style_img, image_shape)\n",
    "    _, styles_Y = extract_features(style_X, content_layers, style_layers)\n",
    "    return style_X, styles_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5673588",
   "metadata": {},
   "source": [
    "## [**定义损失函数**]\n",
    "\n",
    "下面我们来描述风格迁移的损失函数。\n",
    "它由内容损失、风格损失和全变分损失3部分组成。\n",
    "\n",
    "### 内容损失\n",
    "\n",
    "与线性回归中的损失函数类似，内容损失通过平方误差函数衡量合成图像与内容图像在内容特征上的差异。\n",
    "平方误差函数的两个输入均为`extract_features`函数计算所得到的内容层的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecce7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(Y_hat, Y):\n",
    "    return np.square(Y_hat - Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a56d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def content_loss(Y_hat, Y):\n",
    "    # 我们从动态计算梯度的树中分离目标：\n",
    "    # 这是一个规定的值，而不是一个变量。\n",
    "    return torch.square(Y_hat - Y.detach()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad56d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def content_loss(Y_hat, Y):\n",
    "    # 我们从动态计算梯度的树中分离目标：\n",
    "    # 这是一个规定的值，而不是一个变量。\n",
    "    return paddle.square(Y_hat - Y.detach()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a815a6d",
   "metadata": {},
   "source": [
    "### 风格损失\n",
    "\n",
    "风格损失与内容损失类似，也通过平方误差函数衡量合成图像与风格图像在风格上的差异。\n",
    "为了表达风格层输出的风格，我们先通过`extract_features`函数计算风格层的输出。\n",
    "假设该输出的样本数为1，通道数为$c$，高和宽分别为$h$和$w$，我们可以将此输出转换为矩阵$\\mathbf{X}$，其有$c$行和$hw$列。\n",
    "这个矩阵可以被看作由$c$个长度为$hw$的向量$\\mathbf{x}_1, \\ldots, \\mathbf{x}_c$组合而成的。其中向量$\\mathbf{x}_i$代表了通道$i$上的风格特征。\n",
    "\n",
    "在这些向量的*格拉姆矩阵*$\\mathbf{X}\\mathbf{X}^\\top \\in \\mathbb{R}^{c \\times c}$中，$i$行$j$列的元素$x_{ij}$即向量$\\mathbf{x}_i$和$\\mathbf{x}_j$的内积。它表达了通道$i$和通道$j$上风格特征的相关性。我们用这样的格拉姆矩阵来表达风格层输出的风格。\n",
    "需要注意的是，当$hw$的值较大时，格拉姆矩阵中的元素容易出现较大的值。\n",
    "此外，格拉姆矩阵的高和宽皆为通道数$c$。\n",
    "为了让风格损失不受这些值的大小影响，下面定义的`gram`函数将格拉姆矩阵除以了矩阵中元素的个数，即$chw$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def gram(X):\n",
    "    num_channels, n = X.shape[1], d2l.size(X) // X.shape[1]\n",
    "    X = d2l.reshape(X, (num_channels, n))\n",
    "    return d2l.matmul(X, X.T) / (num_channels * n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea882588",
   "metadata": {},
   "source": [
    "自然地，风格损失的平方误差函数的两个格拉姆矩阵输入分别基于合成图像与风格图像的风格层输出。这里假设基于风格图像的格拉姆矩阵`gram_Y`已经预先计算好了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38cc45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(Y_hat, gram_Y):\n",
    "    return np.square(gram(Y_hat) - gram_Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf661960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def style_loss(Y_hat, gram_Y):\n",
    "    return torch.square(gram(Y_hat) - gram_Y.detach()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876de044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def style_loss(Y_hat, gram_Y):\n",
    "    return paddle.square(gram(Y_hat) - gram_Y.detach()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6975e25a",
   "metadata": {},
   "source": [
    "### 全变分损失\n",
    "\n",
    "有时候，我们学到的合成图像里面有大量高频噪点，即有特别亮或者特别暗的颗粒像素。\n",
    "一种常见的去噪方法是*全变分去噪*（total variation denoising）：\n",
    "假设$x_{i, j}$表示坐标$(i, j)$处的像素值，降低全变分损失\n",
    "\n",
    "$$\\sum_{i, j} \\left|x_{i, j} - x_{i+1, j}\\right| + \\left|x_{i, j} - x_{i, j+1}\\right|$$\n",
    "\n",
    "能够尽可能使邻近的像素值相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e432ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def tv_loss(Y_hat):\n",
    "    return 0.5 * (d2l.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +\n",
    "                  d2l.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df09cee",
   "metadata": {},
   "source": [
    "### 损失函数\n",
    "\n",
    "[**风格转移的损失函数是内容损失、风格损失和总变化损失的加权和**]。\n",
    "通过调节这些权重超参数，我们可以权衡合成图像在保留内容、迁移风格以及去噪三方面的相对重要性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "content_weight, style_weight, tv_weight = 1, 1e3, 10\n",
    "\n",
    "def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\n",
    "    # 分别计算内容损失、风格损失和全变分损失\n",
    "    contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(\n",
    "        contents_Y_hat, contents_Y)]\n",
    "    styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(\n",
    "        styles_Y_hat, styles_Y_gram)]\n",
    "    tv_l = tv_loss(X) * tv_weight\n",
    "    # 对所有损失求和\n",
    "    l = sum(10 * styles_l + contents_l + [tv_l])\n",
    "    return contents_l, styles_l, tv_l, l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ca633",
   "metadata": {},
   "source": [
    "## [**初始化合成图像**]\n",
    "\n",
    "在风格迁移中，合成的图像是训练期间唯一需要更新的变量。因此，我们可以定义一个简单的模型`SynthesizedImage`，并将合成的图像视为模型参数。模型的前向传播只需返回模型参数即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthesizedImage(nn.Block):\n",
    "    def __init__(self, img_shape, **kwargs):\n",
    "        super(SynthesizedImage, self).__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape=img_shape)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class SynthesizedImage(nn.Module):\n",
    "    def __init__(self, img_shape, **kwargs):\n",
    "        super(SynthesizedImage, self).__init__(**kwargs)\n",
    "        self.weight = nn.Parameter(torch.rand(*img_shape))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class SynthesizedImage(nn.Layer):\n",
    "    def __init__(self, img_shape, **kwargs):\n",
    "        super(SynthesizedImage, self).__init__(**kwargs)\n",
    "        self.weight = paddle.create_parameter(shape=img_shape,\n",
    "                                            dtype=\"float32\")\n",
    "\n",
    "    def forward(self):\n",
    "        return self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38651d1",
   "metadata": {},
   "source": [
    "下面，我们定义`get_inits`函数。该函数创建了合成图像的模型实例，并将其初始化为图像`X`。风格图像在各个风格层的格拉姆矩阵`styles_Y_gram`将在训练前预先计算好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inits(X, device, lr, styles_Y):\n",
    "    gen_img = SynthesizedImage(X.shape)\n",
    "    gen_img.initialize(init.Constant(X), ctx=device, force_reinit=True)\n",
    "    trainer = gluon.Trainer(gen_img.collect_params(), 'adam',\n",
    "                            {'learning_rate': lr})\n",
    "    styles_Y_gram = [gram(Y) for Y in styles_Y]\n",
    "    return gen_img(), styles_Y_gram, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41211bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_inits(X, device, lr, styles_Y):\n",
    "    gen_img = SynthesizedImage(X.shape).to(device)\n",
    "    gen_img.weight.data.copy_(X.data)\n",
    "    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)\n",
    "    styles_Y_gram = [gram(Y) for Y in styles_Y]\n",
    "    return gen_img(), styles_Y_gram, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def get_inits(X, lr, styles_Y):\n",
    "    gen_img = SynthesizedImage(X.shape)\n",
    "    gen_img.weight.set_value(X)\n",
    "    trainer = paddle.optimizer.Adam(parameters = gen_img.parameters(), learning_rate=lr)\n",
    "    styles_Y_gram = [gram(Y) for Y in styles_Y]\n",
    "    return gen_img(), styles_Y_gram, trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf138f78",
   "metadata": {},
   "source": [
    "## [**训练模型**]\n",
    "\n",
    "在训练模型进行风格迁移时，我们不断抽取合成图像的内容特征和风格特征，然后计算损失函数。下面定义了训练循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b121ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\n",
    "    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[10, num_epochs], ylim=[0, 20],\n",
    "                            legend=['content', 'style', 'TV'],\n",
    "                            ncols=2, figsize=(7, 2.5))\n",
    "    for epoch in range(num_epochs):\n",
    "        with autograd.record():\n",
    "            contents_Y_hat, styles_Y_hat = extract_features(\n",
    "                X, content_layers, style_layers)\n",
    "            contents_l, styles_l, tv_l, l = compute_loss(\n",
    "                X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n",
    "        l.backward()\n",
    "        trainer.step(1)\n",
    "        if (epoch + 1) % lr_decay_epoch == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.8)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.axes[1].imshow(postprocess(X).asnumpy())\n",
    "            animator.add(epoch + 1, [float(sum(contents_l)),\n",
    "                                     float(sum(styles_l)), float(tv_l)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97019268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\n",
    "    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[10, num_epochs],\n",
    "                            legend=['content', 'style', 'TV'],\n",
    "                            ncols=2, figsize=(7, 2.5))\n",
    "    for epoch in range(num_epochs):\n",
    "        trainer.zero_grad()\n",
    "        contents_Y_hat, styles_Y_hat = extract_features(\n",
    "            X, content_layers, style_layers)\n",
    "        contents_l, styles_l, tv_l, l = compute_loss(\n",
    "            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.axes[1].imshow(postprocess(X))\n",
    "            animator.add(epoch + 1, [float(sum(contents_l)),\n",
    "                                     float(sum(styles_l)), float(tv_l)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67900178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def train(X, contents_Y, styles_Y, lr, num_epochs, step_size):\n",
    "    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=lr, gamma=0.8, step_size=step_size)\n",
    "    X, styles_Y_gram, trainer = get_inits(X, scheduler, styles_Y)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[10, num_epochs],\n",
    "                            legend=['content', 'style', 'TV'],\n",
    "                            ncols=2, figsize=(7, 2.5))\n",
    "    for epoch in range(num_epochs):\n",
    "        trainer.clear_grad()\n",
    "        contents_Y_hat, styles_Y_hat = extract_features(\n",
    "            X, content_layers, style_layers)\n",
    "        contents_l, styles_l, tv_l, l = compute_loss(\n",
    "            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.axes[1].imshow(postprocess(X))\n",
    "            animator.add(epoch + 1, [float(sum(contents_l)),\n",
    "                                     float(sum(styles_l)), float(tv_l)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce7d06",
   "metadata": {},
   "source": [
    "现在我们[**训练模型**]：\n",
    "首先将内容图像和风格图像的高和宽分别调整为300和450像素，用内容图像来初始化合成图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc055a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device, image_shape = d2l.try_gpu(), (450, 300)\n",
    "net.collect_params().reset_ctx(device)\n",
    "content_X, contents_Y = get_contents(image_shape, device)\n",
    "_, styles_Y = get_styles(image_shape, device)\n",
    "output = train(content_X, contents_Y, styles_Y, device, 0.9, 500, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19109c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "device, image_shape = d2l.try_gpu(), (300, 450)\n",
    "net = net.to(device)\n",
    "content_X, contents_Y = get_contents(image_shape, device)\n",
    "_, styles_Y = get_styles(image_shape, device)\n",
    "output = train(content_X, contents_Y, styles_Y, device, 0.3, 500, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2fc4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "device, image_shape = d2l.try_gpu(),(300, 450)\n",
    "content_X, contents_Y = get_contents(image_shape)\n",
    "_, styles_Y = get_styles(image_shape)\n",
    "output = train(content_X, contents_Y, styles_Y, 0.3, 500, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35412e20",
   "metadata": {},
   "source": [
    "我们可以看到，合成图像保留了内容图像的风景和物体，并同时迁移了风格图像的色彩。例如，合成图像具有与风格图像中一样的色彩块，其中一些甚至具有画笔笔触的细微纹理。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 风格迁移常用的损失函数由3部分组成：（1）内容损失使合成图像与内容图像在内容特征上接近；（2）风格损失令合成图像与风格图像在风格特征上接近；（3）全变分损失则有助于减少合成图像中的噪点。\n",
    "* 我们可以通过预训练的卷积神经网络来抽取图像的特征，并通过最小化损失函数来不断更新合成图像来作为模型参数。\n",
    "* 我们使用格拉姆矩阵表达风格层输出的风格。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 选择不同的内容和风格层，输出有什么变化？\n",
    "1. 调整损失函数中的权重超参数。输出是否保留更多内容或减少更多噪点？\n",
    "1. 替换实验中的内容图像和风格图像，能创作出更有趣的合成图像吗？\n",
    "1. 我们可以对文本使用风格迁移吗？提示:可以参阅调查报告 :cite:`Hu.Lee.Aggarwal.ea.2020`。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/3299)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/3300)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11813)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29331a",
   "metadata": {},
   "source": [
    "# 实战 Kaggle 比赛：图像分类 (CIFAR-10)\n",
    ":label:`sec_kaggle_cifar10`\n",
    "\n",
    "之前几节中，我们一直在使用深度学习框架的高级API直接获取张量格式的图像数据集。\n",
    "但是在实践中，图像数据集通常以图像文件的形式出现。\n",
    "本节将从原始图像文件开始，然后逐步组织、读取并将它们转换为张量格式。\n",
    "\n",
    "我们在 :numref:`sec_image_augmentation`中对CIFAR-10数据集做了一个实验。CIFAR-10是计算机视觉领域中的一个重要的数据集。\n",
    "本节将运用我们在前几节中学到的知识来参加CIFAR-10图像分类问题的Kaggle竞赛，(**比赛的网址是https://www.kaggle.com/c/cifar-10**)。\n",
    "\n",
    " :numref:`fig_kaggle_cifar10`显示了竞赛网站页面上的信息。\n",
    "为了能提交结果，首先需要注册一个Kaggle账户。\n",
    "\n",
    "![CIFAR-10 图像分类竞赛页面上的信息。竞赛用的数据集可通过点击“Data”选项卡获取。](../img/kaggle-cifar10.png)\n",
    ":width:`600px`\n",
    ":label:`fig_kaggle_cifar10`\n",
    "\n",
    "首先，导入竞赛所需的包和模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1fd787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from d2l import mxnet as d2l\n",
    "import math\n",
    "from mxnet import gluon, init, npx\n",
    "from mxnet.gluon import nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0db77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import collections\n",
    "from d2l import torch as d2l\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea9f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import paddle\n",
    "from paddle import nn\n",
    "import paddle.vision as paddlevision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79ef47a",
   "metadata": {},
   "source": [
    "## 获取并组织数据集\n",
    "\n",
    "比赛数据集分为训练集和测试集，其中训练集包含50000张、测试集包含300000张图像。\n",
    "在测试集中，10000张图像将被用于评估，而剩下的290000张图像将不会被进行评估，包含它们只是为了防止手动标记测试集并提交标记结果。\n",
    "两个数据集中的图像都是png格式，高度和宽度均为32像素并有三个颜色通道（RGB）。\n",
    "这些图片共涵盖10个类别：飞机、汽车、鸟类、猫、鹿、狗、青蛙、马、船和卡车。\n",
    " :numref:`fig_kaggle_cifar10`的左上角显示了数据集中飞机、汽车和鸟类的一些图像。\n",
    "\n",
    "### 下载数据集\n",
    "\n",
    "登录Kaggle后，我们可以点击 :numref:`fig_kaggle_cifar10`中显示的CIFAR-10图像分类竞赛网页上的“Data”选项卡，然后单击“Download All”按钮下载数据集。\n",
    "在`../data`中解压下载的文件并在其中解压缩`train.7z`和`test.7z`后，在以下路径中可以找到整个数据集：\n",
    "\n",
    "* `../data/cifar-10/train/[1-50000].png`\n",
    "* `../data/cifar-10/test/[1-300000].png`\n",
    "* `../data/cifar-10/trainLabels.csv`\n",
    "* `../data/cifar-10/sampleSubmission.csv`\n",
    "\n",
    "`train`和`test`文件夹分别包含训练和测试图像，`trainLabels.csv`含有训练图像的标签，\n",
    "`sample_submission.csv`是提交文件的范例。\n",
    "\n",
    "为了便于入门，[**我们提供包含前1000个训练图像和5个随机测试图像的数据集的小规模样本**]。\n",
    "要使用Kaggle竞赛的完整数据集，需要将以下`demo`变量设置为`False`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277d415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',\n",
    "                                '2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')\n",
    "\n",
    "# 如果使用完整的Kaggle竞赛的数据集，设置demo为False\n",
    "demo = True\n",
    "\n",
    "if demo:\n",
    "    data_dir = d2l.download_extract('cifar10_tiny')\n",
    "else:\n",
    "    data_dir = '../data/cifar-10/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6142e",
   "metadata": {},
   "source": [
    "### [**整理数据集**]\n",
    "\n",
    "我们需要整理数据集来训练和测试模型。\n",
    "首先，我们用以下函数读取CSV文件中的标签，它返回一个字典，该字典将文件名中不带扩展名的部分映射到其标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def read_csv_labels(fname):\n",
    "    \"\"\"读取fname来给标签字典返回一个文件名\"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        # 跳过文件头行(列名)\n",
    "        lines = f.readlines()[1:]\n",
    "    tokens = [l.rstrip().split(',') for l in lines]\n",
    "    return dict(((name, label) for name, label in tokens))\n",
    "\n",
    "labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n",
    "print('# 训练样本 :', len(labels))\n",
    "print('# 类别 :', len(set(labels.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc27dcd",
   "metadata": {},
   "source": [
    "接下来，我们定义`reorg_train_valid`函数来[**将验证集从原始的训练集中拆分出来**]。\n",
    "此函数中的参数`valid_ratio`是验证集中的样本数与原始训练集中的样本数之比。\n",
    "更具体地说，令$n$等于样本最少的类别中的图像数量，而$r$是比率。\n",
    "验证集将为每个类别拆分出$\\max(\\lfloor nr\\rfloor,1)$张图像。\n",
    "让我们以`valid_ratio=0.1`为例，由于原始的训练集有50000张图像，因此`train_valid_test/train`路径中将有45000张图像用于训练，而剩下5000张图像将作为路径`train_valid_test/valid`中的验证集。\n",
    "组织数据集后，同类别的图像将被放置在同一文件夹下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def copyfile(filename, target_dir):\n",
    "    \"\"\"将文件复制到目标目录\"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    shutil.copy(filename, target_dir)\n",
    "\n",
    "#@save\n",
    "def reorg_train_valid(data_dir, labels, valid_ratio):\n",
    "    \"\"\"将验证集从原始的训练集中拆分出来\"\"\"\n",
    "    # 训练数据集中样本最少的类别中的样本数\n",
    "    n = collections.Counter(labels.values()).most_common()[-1][1]\n",
    "    # 验证集中每个类别的样本数\n",
    "    n_valid_per_label = max(1, math.floor(n * valid_ratio))\n",
    "    label_count = {}\n",
    "    for train_file in os.listdir(os.path.join(data_dir, 'train')):\n",
    "        label = labels[train_file.split('.')[0]]\n",
    "        fname = os.path.join(data_dir, 'train', train_file)\n",
    "        copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
    "                                     'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < n_valid_per_label:\n",
    "            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
    "                                         'valid', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
    "                                         'train', label))\n",
    "    return n_valid_per_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66fc11",
   "metadata": {},
   "source": [
    "下面的`reorg_test`函数用来[**在预测期间整理测试集，以方便读取**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def reorg_test(data_dir):\n",
    "    \"\"\"在预测期间整理测试集，以方便读取\"\"\"\n",
    "    for test_file in os.listdir(os.path.join(data_dir, 'test')):\n",
    "        copyfile(os.path.join(data_dir, 'test', test_file),\n",
    "                 os.path.join(data_dir, 'train_valid_test', 'test',\n",
    "                              'unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5900beb",
   "metadata": {},
   "source": [
    "最后，我们使用一个函数来[**调用前面定义的函数**]`read_csv_labels`、`reorg_train_valid`和`reorg_test`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c39d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def reorg_cifar10_data(data_dir, valid_ratio):\n",
    "    labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n",
    "    reorg_train_valid(data_dir, labels, valid_ratio)\n",
    "    reorg_test(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79bbf4d",
   "metadata": {},
   "source": [
    "在这里，我们只将样本数据集的批量大小设置为32。\n",
    "在实际训练和测试中，应该使用Kaggle竞赛的完整数据集，并将`batch_size`设置为更大的整数，例如128。\n",
    "我们将10％的训练样本作为调整超参数的验证集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0829c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "batch_size = 32 if demo else 128\n",
    "valid_ratio = 0.1\n",
    "reorg_cifar10_data(data_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dbec4a",
   "metadata": {},
   "source": [
    "## [**图像增广**]\n",
    "\n",
    "我们使用图像增广来解决过拟合的问题。例如在训练中，我们可以随机水平翻转图像。\n",
    "我们还可以对彩色图像的三个RGB通道执行标准化。\n",
    "下面，我们列出了其中一些可以调整的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = gluon.data.vision.transforms.Compose([\n",
    "    # 在高度和宽度上将图像放大到40像素的正方形\n",
    "    gluon.data.vision.transforms.Resize(40),\n",
    "    # 随机裁剪出一个高度和宽度均为40像素的正方形图像，\n",
    "    # 生成一个面积为原始图像面积0.64～1倍的小正方形，\n",
    "    # 然后将其缩放为高度和宽度均为32像素的正方形\n",
    "    gluon.data.vision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n",
    "                                                   ratio=(1.0, 1.0)),\n",
    "    gluon.data.vision.transforms.RandomFlipLeftRight(),\n",
    "    gluon.data.vision.transforms.ToTensor(),\n",
    "    # 标准化图像的每个通道\n",
    "    gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                           [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579720e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "transform_train = torchvision.transforms.Compose([\n",
    "    # 在高度和宽度上将图像放大到40像素的正方形\n",
    "    torchvision.transforms.Resize(40),\n",
    "    # 随机裁剪出一个高度和宽度均为40像素的正方形图像，\n",
    "    # 生成一个面积为原始图像面积0.64～1倍的小正方形，\n",
    "    # 然后将其缩放为高度和宽度均为32像素的正方形\n",
    "    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n",
    "                                                   ratio=(1.0, 1.0)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # 标准化图像的每个通道\n",
    "    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                     [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded65ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "transform_train = paddlevision.transforms.Compose([\n",
    "    # 在高度和宽度上将图像放大到40像素的正方形\n",
    "    paddlevision.transforms.Resize(40),\n",
    "    # 随机裁剪出一个高度和宽度均为40像素的正方形图像，\n",
    "    # 生成一个面积为原始图像面积0.64到1倍的小正方形，\n",
    "    # 然后将其缩放为高度和宽度均为32像素的正方形\n",
    "    paddlevision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n",
    "                                              ratio=(1.0, 1.0)),\n",
    "    paddlevision.transforms.RandomHorizontalFlip(),\n",
    "    paddlevision.transforms.ToTensor(),\n",
    "    # 标准化图像的每个通道\n",
    "    paddlevision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                     [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65a05c",
   "metadata": {},
   "source": [
    "在测试期间，我们只对图像执行标准化，以消除评估结果中的随机性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = gluon.data.vision.transforms.Compose([\n",
    "    gluon.data.vision.transforms.ToTensor(),\n",
    "    gluon.data.vision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                           [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99369a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                     [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ebe16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "transform_test = paddlevision.transforms.Compose([\n",
    "    paddlevision.transforms.ToTensor(),\n",
    "    paddlevision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
    "                                     [0.2023, 0.1994, 0.2010])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13573e5e",
   "metadata": {},
   "source": [
    "## 读取数据集\n",
    "\n",
    "接下来，我们[**读取由原始图像组成的数据集**]，每个样本都包括一张图片和一个标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds, train_valid_ds, test_ds = [\n",
    "    gluon.data.vision.ImageFolderDataset(\n",
    "        os.path.join(data_dir, 'train_valid_test', folder))\n",
    "    for folder in ['train', 'valid', 'train_valid', 'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5777f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_train) for folder in ['train', 'train_valid']]\n",
    "\n",
    "valid_ds, test_ds = [torchvision.datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_test) for folder in ['valid', 'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "train_ds, train_valid_ds = [paddlevision.datasets.DatasetFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_train) for folder in ['train', 'train_valid']]\n",
    "    \n",
    "valid_ds, test_ds = [paddlevision.datasets.DatasetFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_test) for folder in ['valid', 'test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301232fb",
   "metadata": {},
   "source": [
    "在训练期间，我们需要[**指定上面定义的所有图像增广操作**]。\n",
    "当验证集在超参数调整过程中用于模型评估时，不应引入图像增广的随机性。\n",
    "在最终预测之前，我们根据训练集和验证集组合而成的训练模型进行训练，以充分利用所有标记的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, train_valid_iter = [gluon.data.DataLoader(\n",
    "    dataset.transform_first(transform_train), batch_size, shuffle=True,\n",
    "    last_batch='discard') for dataset in (train_ds, train_valid_ds)]\n",
    "\n",
    "valid_iter = gluon.data.DataLoader(\n",
    "    valid_ds.transform_first(transform_test), batch_size, shuffle=False,\n",
    "    last_batch='discard')\n",
    "\n",
    "test_iter = gluon.data.DataLoader(\n",
    "    test_ds.transform_first(transform_test), batch_size, shuffle=False,\n",
    "    last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d46a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "train_iter, train_valid_iter = [torch.utils.data.DataLoader(\n",
    "    dataset, batch_size, shuffle=True, drop_last=True)\n",
    "    for dataset in (train_ds, train_valid_ds)]\n",
    "\n",
    "valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\n",
    "                                         drop_last=True)\n",
    "\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\n",
    "                                        drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "train_iter, train_valid_iter = [paddle.io.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    for dataset in (train_ds, train_valid_ds)]\n",
    "\n",
    "valid_iter = paddle.io.DataLoader(valid_ds, batch_size=batch_size, shuffle=False,\n",
    "                                  drop_last=True)\n",
    "\n",
    "test_iter = paddle.io.DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
    "                                 drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5ac97",
   "metadata": {},
   "source": [
    "## 定义[**模型**]\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "在这里，我们基于`HybridBlock`类构建剩余块，这与 :numref:`sec_resnet`中描述的实现方法略有不同，是为了提高计算效率。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\n",
    "                               strides=strides)\n",
    "        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,\n",
    "                                   strides=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm()\n",
    "        self.bn2 = nn.BatchNorm()\n",
    "\n",
    "    def hybrid_forward(self, F, X):\n",
    "        Y = F.npx.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return F.npx.relu(Y + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121fbfbc",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "接下来，我们定义Resnet-18模型。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(num_classes):\n",
    "    net = nn.HybridSequential()\n",
    "    net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),\n",
    "            nn.BatchNorm(), nn.Activation('relu'))\n",
    "\n",
    "    def resnet_block(num_channels, num_residuals, first_block=False):\n",
    "        blk = nn.HybridSequential()\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.add(Residual(num_channels))\n",
    "        return blk\n",
    "\n",
    "    net.add(resnet_block(64, 2, first_block=True),\n",
    "            resnet_block(128, 2),\n",
    "            resnet_block(256, 2),\n",
    "            resnet_block(512, 2))\n",
    "    net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ecc06",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "在训练开始之前，我们使用 :numref:`subsec_xavier`中描述的Xavier初始化。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "我们定义了 :numref:`sec_resnet`中描述的Resnet-18模型。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "我们定义了 :numref:`sec_resnet`中描述的Resnet-18模型。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(devices):\n",
    "    num_classes = 10\n",
    "    net = resnet18(num_classes)\n",
    "    net.initialize(ctx=devices, init=init.Xavier())\n",
    "    return net\n",
    "\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013eb9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_net():\n",
    "    num_classes = 10\n",
    "    net = d2l.resnet18(num_classes, 3)\n",
    "    return net\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def get_net():\n",
    "    num_classes = 10\n",
    "    net = d2l.resnet18(num_classes, 3)\n",
    "    return net\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e838ee9",
   "metadata": {},
   "source": [
    "## 定义[**训练函数**]\n",
    "\n",
    "我们将根据模型在验证集上的表现来选择模型并调整超参数。\n",
    "下面我们定义了模型训练函数`train`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375fc648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "          lr_decay):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd',\n",
    "                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    num_batches, timer = len(train_iter), d2l.Timer()\n",
    "    legend = ['train loss', 'train acc']\n",
    "    if valid_iter is not None:\n",
    "        legend.append('valid acc')\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=legend)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(3)\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = d2l.train_batch_ch13(\n",
    "                net, features, labels.astype('float32'), loss, trainer,\n",
    "                devices, d2l.split_batch)\n",
    "            metric.add(l, acc, labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[2],\n",
    "                              None))\n",
    "        if valid_iter is not None:\n",
    "            valid_acc = d2l.evaluate_accuracy_gpus(net, valid_iter,\n",
    "                                                   d2l.split_batch)\n",
    "            animator.add(epoch + 1, (None, None, valid_acc))\n",
    "    measures = (f'train loss {metric[0] / metric[2]:.3f}, '\n",
    "                f'train acc {metric[1] / metric[2]:.3f}')\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid acc {valid_acc:.3f}'\n",
    "    print(measures + f'\\n{metric[2] * num_epochs / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b00a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "          lr_decay):\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,\n",
    "                              weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\n",
    "    num_batches, timer = len(train_iter), d2l.Timer()\n",
    "    legend = ['train loss', 'train acc']\n",
    "    if valid_iter is not None:\n",
    "        legend.append('valid acc')\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=legend)\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        metric = d2l.Accumulator(3)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = d2l.train_batch_ch13(net, features, labels,\n",
    "                                          loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[2],\n",
    "                              None))\n",
    "        if valid_iter is not None:\n",
    "            valid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)\n",
    "            animator.add(epoch + 1, (None, None, valid_acc))\n",
    "        scheduler.step()\n",
    "    measures = (f'train loss {metric[0] / metric[2]:.3f}, '\n",
    "                f'train acc {metric[1] / metric[2]:.3f}')\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid acc {valid_acc:.3f}'\n",
    "    print(measures + f'\\n{metric[2] * num_epochs / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810674aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "          lr_decay):\n",
    "    scheduler = paddle.optimizer.lr.StepDecay(lr, lr_period, lr_decay)\n",
    "    trainer = paddle.optimizer.Momentum(learning_rate=scheduler, momentum=0.9, parameters=net.parameters(),\n",
    "                              weight_decay=wd)\n",
    "    num_batches, timer = len(train_iter), d2l.Timer()\n",
    "    legend = ['train loss', 'train acc']\n",
    "    if valid_iter is not None:\n",
    "        legend.append('valid acc')\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=legend)\n",
    "    net = paddle.DataParallel(net)\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        metric = d2l.Accumulator(3)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = d2l.train_batch_ch13(net, features, labels,\n",
    "                                          loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[2],\n",
    "                              None))\n",
    "        if valid_iter is not None:\n",
    "            valid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)\n",
    "            animator.add(epoch + 1, (None, None, valid_acc))\n",
    "        scheduler.step()\n",
    "    measures = (f'train loss {metric[0] / metric[2]:.3f}, '\n",
    "                f'train acc {metric[1] / metric[2]:.3f}')\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid acc {valid_acc:.3f}'\n",
    "    print(measures + f'\\n{metric[2] * num_epochs / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ed121",
   "metadata": {},
   "source": [
    "## [**训练和验证模型**]\n",
    "\n",
    "现在，我们可以训练和验证模型了，而以下所有超参数都可以调整。\n",
    "例如，我们可以增加周期的数量。当`lr_period`和`lr_decay`分别设置为4和0.9时，优化算法的学习速率将在每4个周期乘以0.9。\n",
    "为便于演示，我们在这里只训练20个周期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 0.02, 5e-4\n",
    "lr_period, lr_decay, net = 4, 0.9, get_net(devices)\n",
    "net.hybridize()\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da383079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "devices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 2e-4, 5e-4\n",
    "lr_period, lr_decay, net = 4, 0.9, get_net()\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a499fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "devices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 2e-4, 5e-4\n",
    "lr_period, lr_decay, net = 4, 0.9, get_net()\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214e976",
   "metadata": {},
   "source": [
    "## 在 Kaggle 上[**对测试集进行分类并提交结果**]\n",
    "\n",
    "在获得具有超参数的满意的模型后，我们使用所有标记的数据（包括验证集）来重新训练模型并对测试集进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net, preds = get_net(devices), []\n",
    "net.hybridize()\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)\n",
    "\n",
    "for X, _ in test_iter:\n",
    "    y_hat = net(X.as_in_ctx(devices[0]))\n",
    "    preds.extend(y_hat.argmax(axis=1).astype(int).asnumpy())\n",
    "sorted_ids = list(range(1, len(test_ds) + 1))\n",
    "sorted_ids.sort(key=lambda x: str(x))\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n",
    "df['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x])\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d7593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "net, preds = get_net(), []\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)\n",
    "\n",
    "for X, _ in test_iter:\n",
    "    y_hat = net(X.to(devices[0]))\n",
    "    preds.extend(y_hat.argmax(dim=1).type(torch.int32).cpu().numpy())\n",
    "sorted_ids = list(range(1, len(test_ds) + 1))\n",
    "sorted_ids.sort(key=lambda x: str(x))\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n",
    "df['label'] = df['label'].apply(lambda x: train_valid_ds.classes[x])\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3084cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "net, preds = get_net(), []\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)\n",
    "\n",
    "for X, _ in test_iter:\n",
    "    y_hat = net(X)\n",
    "    preds.extend(y_hat.argmax(axis=1).astype(paddle.int32).numpy())\n",
    "sorted_ids = list(range(1, len(test_ds) + 1))\n",
    "sorted_ids.sort(key=lambda x: str(x))\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n",
    "df['label'] = df['label'].apply(lambda x: train_valid_ds.classes[x])\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abffd375",
   "metadata": {},
   "source": [
    "向Kaggle提交结果的方法与 :numref:`sec_kaggle_house`中的方法类似，上面的代码将生成一个\n",
    "`submission.csv`文件，其格式符合Kaggle竞赛的要求。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 将包含原始图像文件的数据集组织为所需格式后，我们可以读取它们。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "* 我们可以在图像分类竞赛中使用卷积神经网络、图像增广和混合编程。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "* 我们可以在图像分类竞赛中使用卷积神经网络和图像增广。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "* 我们可以在图像分类竞赛中使用卷积神经网络和图像增广。\n",
    ":end_tab:\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 在这场Kaggle竞赛中使用完整的CIFAR-10数据集。将超参数设为`batch_size = 128`，`num_epochs = 100`，`lr = 0.1`，`lr_period = 50`，`lr_decay = 0.1`。看看在这场比赛中能达到什么准确度和排名。能进一步改进吗？\n",
    "1. 不使用图像增广时，能获得怎样的准确度？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2830)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2831)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11814)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c19a87",
   "metadata": {},
   "source": [
    "# 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）\n",
    "\n",
    "本节我们将在Kaggle上实战狗品种识别问题。\n",
    "本次(**比赛网址是https://www.kaggle.com/c/dog-breed-identification**)。\n",
    " :numref:`fig_kaggle_dog`显示了鉴定比赛网页上的信息。\n",
    "需要一个Kaggle账户才能提交结果。\n",
    "\n",
    "在这场比赛中，我们将识别120类不同品种的狗。\n",
    "这个数据集实际上是著名的ImageNet的数据集子集。与 :numref:`sec_kaggle_cifar10`中CIFAR-10数据集中的图像不同，\n",
    "ImageNet数据集中的图像更高更宽，且尺寸不一。\n",
    "\n",
    "![狗的品种鉴定比赛网站，可以通过单击“数据”选项卡来获得比赛数据集。](../img/kaggle-dog.jpg)\n",
    ":width:`400px`\n",
    ":label:`fig_kaggle_dog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66008083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, init, npx\n",
    "from mxnet.gluon import nn\n",
    "import os\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e038fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb29496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "import paddle.vision as paddlevision\n",
    "from paddle import nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b8c42c",
   "metadata": {},
   "source": [
    "## 获取和整理数据集\n",
    "\n",
    "比赛数据集分为训练集和测试集，分别包含RGB（彩色）通道的10222张、10357张JPEG图像。\n",
    "在训练数据集中，有120种犬类，如拉布拉多、贵宾、腊肠、萨摩耶、哈士奇、吉娃娃和约克夏等。\n",
    "\n",
    "### 下载数据集\n",
    "\n",
    "登录Kaggle后，可以点击 :numref:`fig_kaggle_dog`中显示的竞争网页上的“数据”选项卡，然后点击“全部下载”按钮下载数据集。在`../data`中解压下载的文件后，将在以下路径中找到整个数据集：\n",
    "\n",
    "* ../data/dog-breed-identification/labels.csv\n",
    "* ../data/dog-breed-identification/sample_submission.csv\n",
    "* ../data/dog-breed-identification/train\n",
    "* ../data/dog-breed-identification/test\n",
    "\n",
    "\n",
    "上述结构与 :numref:`sec_kaggle_cifar10`的CIFAR-10类似，其中文件夹`train/`和`test/`分别包含训练和测试狗图像，`labels.csv`包含训练图像的标签。\n",
    "\n",
    "同样，为了便于入门，[**我们提供完整数据集的小规模样本**]：`train_valid_test_tiny.zip`。\n",
    "如果要在Kaggle比赛中使用完整的数据集，则需要将下面的`demo`变量更改为`False`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54394c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save \n",
    "d2l.DATA_HUB['dog_tiny'] = (d2l.DATA_URL + 'kaggle_dog_tiny.zip',\n",
    "                            '0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d')\n",
    "\n",
    "# 如果使用Kaggle比赛的完整数据集，请将下面的变量更改为False\n",
    "demo = True\n",
    "if demo:\n",
    "    data_dir = d2l.download_extract('dog_tiny')\n",
    "else:\n",
    "    data_dir = os.path.join('..', 'data', 'dog-breed-identification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad9160",
   "metadata": {},
   "source": [
    "### [**整理数据集**]\n",
    "\n",
    "我们可以像 :numref:`sec_kaggle_cifar10`中所做的那样整理数据集，即从原始训练集中拆分验证集，然后将图像移动到按标签分组的子文件夹中。\n",
    "\n",
    "下面的`reorg_dog_data`函数读取训练数据标签、拆分验证集并整理训练集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682141ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def reorg_dog_data(data_dir, valid_ratio):\n",
    "    labels = d2l.read_csv_labels(os.path.join(data_dir, 'labels.csv'))\n",
    "    d2l.reorg_train_valid(data_dir, labels, valid_ratio)\n",
    "    d2l.reorg_test(data_dir)\n",
    "\n",
    "\n",
    "batch_size = 32 if demo else 128\n",
    "valid_ratio = 0.1\n",
    "reorg_dog_data(data_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28614d5f",
   "metadata": {},
   "source": [
    "## [**图像增广**]\n",
    "\n",
    "回想一下，这个狗品种数据集是ImageNet数据集的子集，其图像大于 :numref:`sec_kaggle_cifar10`中CIFAR-10数据集的图像。\n",
    "下面我们看一下如何在相对较大的图像上使用图像增广。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f531c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = gluon.data.vision.transforms.Compose([\n",
    "    # 随机裁剪图像，所得图像为原始面积的0.08～1之间，高宽比在3/4和4/3之间。\n",
    "    # 然后，缩放图像以创建224x224的新图像\n",
    "    gluon.data.vision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\n",
    "                                                   ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    gluon.data.vision.transforms.RandomFlipLeftRight(),\n",
    "    # 随机更改亮度，对比度和饱和度\n",
    "    gluon.data.vision.transforms.RandomColorJitter(brightness=0.4,\n",
    "                                                   contrast=0.4,\n",
    "                                                   saturation=0.4),\n",
    "    # 添加随机噪声\n",
    "    gluon.data.vision.transforms.RandomLighting(0.1),\n",
    "    gluon.data.vision.transforms.ToTensor(),\n",
    "    # 标准化图像的每个通道\n",
    "    gluon.data.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                           [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80493d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "transform_train = torchvision.transforms.Compose([\n",
    "    # 随机裁剪图像，所得图像为原始面积的0.08～1之间，高宽比在3/4和4/3之间。\n",
    "    # 然后，缩放图像以创建224x224的新图像\n",
    "    torchvision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\n",
    "                                             ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    # 随机更改亮度，对比度和饱和度\n",
    "    torchvision.transforms.ColorJitter(brightness=0.4,\n",
    "                                       contrast=0.4,\n",
    "                                       saturation=0.4),\n",
    "    # 添加随机噪声\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # 标准化图像的每个通道\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d71db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "transform_train = paddlevision.transforms.Compose([\n",
    "    # 随机裁剪图像，所得图像为原始面积的0.08到1之间，高宽比在3/4和4/3之间。\n",
    "    # 然后，缩放图像以创建224x224的新图像\n",
    "    paddlevision.transforms.RandomResizedCrop(224, scale=(0.08, 1.0),\n",
    "                                             ratio=(3.0/4.0, 4.0/3.0)),\n",
    "    paddlevision.transforms.RandomHorizontalFlip(),\n",
    "    # 随机更改亮度，对比度和饱和度\n",
    "    paddlevision.transforms.ColorJitter(brightness=0.4,\n",
    "                                       contrast=0.4,\n",
    "                                       saturation=0.4),\n",
    "    # 添加随机噪声\n",
    "    paddlevision.transforms.ToTensor(),\n",
    "    # 标准化图像的每个通道\n",
    "    paddlevision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6955d975",
   "metadata": {},
   "source": [
    "测试时，我们只使用确定性的图像预处理操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5264688",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = gluon.data.vision.transforms.Compose([\n",
    "    gluon.data.vision.transforms.Resize(256),\n",
    "    # 从图像中心裁切224x224大小的图片\n",
    "    gluon.data.vision.transforms.CenterCrop(224),\n",
    "    gluon.data.vision.transforms.ToTensor(),\n",
    "    gluon.data.vision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                           [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d06bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "transform_test = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    # 从图像中心裁切224x224大小的图片\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "transform_test = paddlevision.transforms.Compose([\n",
    "    paddlevision.transforms.Resize(256),\n",
    "    # 从图像中心裁切224x224大小的图片\n",
    "    paddlevision.transforms.CenterCrop(224),\n",
    "    paddlevision.transforms.ToTensor(),\n",
    "    paddlevision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a5241",
   "metadata": {},
   "source": [
    "## [**读取数据集**]\n",
    "\n",
    "与 :numref:`sec_kaggle_cifar10`一样，我们可以读取整理后的含原始图像文件的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds, train_valid_ds, test_ds = [\n",
    "    gluon.data.vision.ImageFolderDataset(\n",
    "        os.path.join(data_dir, 'train_valid_test', folder))\n",
    "    for folder in ('train', 'valid', 'train_valid', 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd6132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_train) for folder in ['train', 'train_valid']]\n",
    "\n",
    "valid_ds, test_ds = [torchvision.datasets.ImageFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_test) for folder in ['valid', 'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f703a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "train_ds, train_valid_ds = [paddlevision.datasets.DatasetFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_train) for folder in ['train', 'train_valid']]\n",
    "\n",
    "valid_ds, test_ds = [paddlevision.datasets.DatasetFolder(\n",
    "    os.path.join(data_dir, 'train_valid_test', folder),\n",
    "    transform=transform_test) for folder in ['valid', 'test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89644529",
   "metadata": {},
   "source": [
    "下面我们创建数据加载器实例的方式与 :numref:`sec_kaggle_cifar10`相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, train_valid_iter = [gluon.data.DataLoader(\n",
    "    dataset.transform_first(transform_train), batch_size, shuffle=True, \n",
    "    last_batch='discard') for dataset in (train_ds, train_valid_ds)]\n",
    "\n",
    "valid_iter = gluon.data.DataLoader(\n",
    "    valid_ds.transform_first(transform_test), batch_size, shuffle=False, \n",
    "    last_batch='discard')\n",
    "\n",
    "test_iter = gluon.data.DataLoader(\n",
    "    test_ds.transform_first(transform_test), batch_size, shuffle=False, \n",
    "    last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e826e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "train_iter, train_valid_iter = [torch.utils.data.DataLoader(\n",
    "    dataset, batch_size, shuffle=True, drop_last=True)\n",
    "    for dataset in (train_ds, train_valid_ds)]\n",
    "\n",
    "valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\n",
    "                                         drop_last=True)\n",
    "\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\n",
    "                                        drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "train_iter, train_valid_iter = [paddle.io.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    for dataset in (train_ds, train_valid_ds)]\n",
    "\n",
    "valid_iter = paddle.io.DataLoader(valid_ds, batch_size=batch_size, shuffle=False,\n",
    "                                  drop_last=True)\n",
    "\n",
    "test_iter = paddle.io.DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
    "                                 drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b6d99",
   "metadata": {},
   "source": [
    "## [**微调预训练模型**]\n",
    "\n",
    "同样，本次比赛的数据集是ImageNet数据集的子集。\n",
    "因此，我们可以使用 :numref:`sec_fine_tuning`中讨论的方法在完整ImageNet数据集上选择预训练的模型，然后使用该模型提取图像特征，以便将其输入到定制的小规模输出网络中。\n",
    "深度学习框架的高级API提供了在ImageNet数据集上预训练的各种模型。\n",
    "在这里，我们选择预训练的ResNet-34模型，我们只需重复使用此模型的输出层（即提取的特征）的输入。\n",
    "然后，我们可以用一个可以训练的小型自定义输出网络替换原始输出层，例如堆叠两个完全连接的图层。\n",
    "与 :numref:`sec_fine_tuning`中的实验不同，以下内容不重新训练用于特征提取的预训练模型，这节省了梯度下降的时间和内存空间。\n",
    "\n",
    "回想一下，我们使用三个RGB通道的均值和标准差来对完整的ImageNet数据集进行图像标准化。\n",
    "事实上，这也符合ImageNet上预训练模型的标准化操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c0973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(devices):\n",
    "    finetune_net = gluon.model_zoo.vision.resnet34_v2(pretrained=True)\n",
    "    # 定义一个新的输出网络\n",
    "    finetune_net.output_new = nn.HybridSequential(prefix='')\n",
    "    finetune_net.output_new.add(nn.Dense(256, activation='relu'))\n",
    "    # 共有120个输出类别\n",
    "    finetune_net.output_new.add(nn.Dense(120))\n",
    "    # 初始化输出网络\n",
    "    finetune_net.output_new.initialize(init.Xavier(), ctx=devices)\n",
    "    # 将模型参数分配给用于计算的CPU或GPU\n",
    "    finetune_net.collect_params().reset_ctx(devices)\n",
    "    return finetune_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f5cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_net(devices):\n",
    "    finetune_net = nn.Sequential()\n",
    "    finetune_net.features = torchvision.models.resnet34(pretrained=True)\n",
    "    # 定义一个新的输出网络，共有120个输出类别\n",
    "    finetune_net.output_new = nn.Sequential(nn.Linear(1000, 256),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(256, 120))\n",
    "    # 将模型参数分配给用于计算的CPU或GPU\n",
    "    finetune_net = finetune_net.to(devices[0])\n",
    "    # 冻结参数\n",
    "    for param in finetune_net.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    return finetune_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def get_net(devices):\n",
    "    finetune_net = nn.Sequential()\n",
    "    finetune_net.features = paddlevision.models.resnet34(pretrained=True)\n",
    "    # 定义一个新的输出网络，共有120个输出类别\n",
    "    finetune_net.output_new = nn.Sequential(nn.Linear(1000, 256),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(256, 120))\n",
    "    # 冻结参数\n",
    "    for param in finetune_net.features.parameters():\n",
    "        param.stop_gradient = True\n",
    "    return finetune_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2e16e",
   "metadata": {},
   "source": [
    "在[**计算损失**]之前，我们首先获取预训练模型的输出层的输入，即提取的特征。\n",
    "然后我们使用此特征作为我们小型自定义输出网络的输入来计算损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea99f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "def evaluate_loss(data_iter, net, devices):\n",
    "    l_sum, n = 0.0, 0\n",
    "    for features, labels in data_iter:\n",
    "        X_shards, y_shards = d2l.split_batch(features, labels, devices)\n",
    "        output_features = [net.features(X_shard) for X_shard in X_shards]\n",
    "        outputs = [net.output_new(feature) for feature in output_features]\n",
    "        ls = [loss(output, y_shard).sum() for output, y_shard\n",
    "              in zip(outputs, y_shards)]\n",
    "        l_sum += sum([float(l.sum()) for l in ls])\n",
    "        n += labels.size\n",
    "    return l_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2921db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "def evaluate_loss(data_iter, net, devices):\n",
    "    l_sum, n = 0.0, 0\n",
    "    for features, labels in data_iter:\n",
    "        features, labels = features.to(devices[0]), labels.to(devices[0])\n",
    "        outputs = net(features)\n",
    "        l = loss(outputs, labels)\n",
    "        l_sum += l.sum()\n",
    "        n += labels.numel()\n",
    "    return (l_sum / n).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "def evaluate_loss(data_iter, net, devices):\n",
    "    l_sum, n = 0.0, 0\n",
    "    for features, labels in data_iter:\n",
    "        outputs = net(features)\n",
    "        l = loss(outputs, labels)\n",
    "        l_sum += l.sum()\n",
    "        n += labels.numel()\n",
    "    return l_sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58587964",
   "metadata": {},
   "source": [
    "## 定义[**训练函数**]\n",
    "\n",
    "我们将根据模型在验证集上的表现选择模型并调整超参数。\n",
    "模型训练函数`train`只迭代小型自定义输出网络的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424305e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "          lr_decay):\n",
    "    # 只训练小型自定义输出网络\n",
    "    trainer = gluon.Trainer(net.output_new.collect_params(), 'sgd',\n",
    "                            {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    num_batches, timer = len(train_iter), d2l.Timer()\n",
    "    legend = ['train loss']\n",
    "    if valid_iter is not None:\n",
    "        legend.append('valid loss')\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=legend)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(2)\n",
    "        if epoch > 0 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            X_shards, y_shards = d2l.split_batch(features, labels, devices)\n",
    "            output_features = [net.features(X_shard) for X_shard in X_shards]\n",
    "            with autograd.record():\n",
    "                outputs = [net.output_new(feature)\n",
    "                           for feature in output_features]\n",
    "                ls = [loss(output, y_shard).sum() for output, y_shard\n",
    "                      in zip(outputs, y_shards)]\n",
    "            for l in ls:\n",
    "                l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            metric.add(sum([float(l.sum()) for l in ls]), labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches, \n",
    "                             (metric[0] / metric[1], None))\n",
    "        if valid_iter is not None:\n",
    "            valid_loss = evaluate_loss(valid_iter, net, devices)\n",
    "            animator.add(epoch + 1, (None, valid_loss))\n",
    "    measures = f'train loss {metric[0] / metric[1]:.3f}'\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid loss {valid_loss:.3f}'\n",
    "    print(measures + f'\\n{metric[1] * num_epochs / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbed4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "          lr_decay):\n",
    "    # 只训练小型自定义输出网络\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    trainer = torch.optim.SGD((param for param in net.parameters()\n",
    "                               if param.requires_grad), lr=lr,\n",
    "                              momentum=0.9, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\n",
    "    num_batches, timer = len(train_iter), d2l.Timer()\n",
    "    legend = ['train loss']\n",
    "    if valid_iter is not None:\n",
    "        legend.append('valid loss')\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=legend)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(2)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            features, labels = features.to(devices[0]), labels.to(devices[0])\n",
    "            trainer.zero_grad()\n",
    "            output = net(features)\n",
    "            l = loss(output, labels).sum()\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            metric.add(l, labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches, \n",
    "                             (metric[0] / metric[1], None))\n",
    "        measures = f'train loss {metric[0] / metric[1]:.3f}'\n",
    "        if valid_iter is not None:\n",
    "            valid_loss = evaluate_loss(valid_iter, net, devices)\n",
    "            animator.add(epoch + 1, (None, valid_loss.detach().cpu()))\n",
    "        scheduler.step()\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid loss {valid_loss:.3f}'\n",
    "    print(measures + f'\\n{metric[1] * num_epochs / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd260fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "          lr_decay):\n",
    "    # 只训练小型自定义输出网络\n",
    "    net = paddle.DataParallel(net)\n",
    "    scheduler = paddle.optimizer.lr.StepDecay(lr, lr_period, lr_decay)\n",
    "    trainer = paddle.optimizer.Momentum(learning_rate=scheduler, \n",
    "                                        parameters=(param for param in net.parameters() if not param.stop_gradient), \n",
    "                                        momentum=0.9, \n",
    "                                        weight_decay=wd)\n",
    "    num_batches, timer = len(train_iter), d2l.Timer()\n",
    "    legend = ['train loss']\n",
    "    if valid_iter is not None:\n",
    "        legend.append('valid loss')\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=legend)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(2)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            trainer.clear_grad()\n",
    "            output = net(features)\n",
    "            l = loss(output, labels).sum()\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            metric.add(l, labels.shape[0])\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches, \n",
    "                             (metric[0] / metric[1], None))\n",
    "        measures = f'train loss {metric[0] / metric[1]:.3f}'\n",
    "        if valid_iter is not None:\n",
    "            valid_loss = evaluate_loss(valid_iter, net, devices)\n",
    "            animator.add(epoch + 1, (None, valid_loss.detach()))\n",
    "        scheduler.step()\n",
    "    if valid_iter is not None:\n",
    "        measures += f', valid loss {float(valid_loss):.3f}'\n",
    "    print(measures + f'\\n{metric[1] * num_epochs / timer.sum():.1f}'\n",
    "          f' examples/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8fb40",
   "metadata": {},
   "source": [
    "## [**训练和验证模型**]\n",
    "\n",
    "现在我们可以训练和验证模型了，以下超参数都是可调的。\n",
    "例如，我们可以增加迭代轮数。\n",
    "另外，由于`lr_period`和`lr_decay`分别设置为2和0.9，\n",
    "因此优化算法的学习速率将在每2个迭代后乘以0.9。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f32642",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices, num_epochs, lr, wd = d2l.try_all_gpus(), 10, 5e-3, 1e-4\n",
    "lr_period, lr_decay, net = 2, 0.9, get_net(devices)\n",
    "net.hybridize()\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5229a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "devices, num_epochs, lr, wd = d2l.try_all_gpus(), 10, 1e-4, 1e-4\n",
    "lr_period, lr_decay, net = 2, 0.9, get_net(devices)\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd810622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "devices, num_epochs, lr, wd = d2l.try_all_gpus(), 10, 1e-4, 1e-4\n",
    "lr_period, lr_decay, net = 2, 0.9, get_net(devices)\n",
    "train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769e644",
   "metadata": {},
   "source": [
    "## [**对测试集分类**]并在Kaggle提交结果\n",
    "\n",
    "与 :numref:`sec_kaggle_cifar10`中的最后一步类似，最终所有标记的数据（包括验证集）都用于训练模型和对测试集进行分类。\n",
    "我们将使用训练好的自定义输出网络进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cccbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = get_net(devices)\n",
    "net.hybridize()\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)\n",
    "\n",
    "preds = []\n",
    "for data, label in test_iter:\n",
    "    output_features = net.features(data.as_in_ctx(devices[0]))\n",
    "    output = npx.softmax(net.output_new(output_features))\n",
    "    preds.extend(output.asnumpy())\n",
    "ids = sorted(os.listdir(\n",
    "    os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.synsets) + '\\n')\n",
    "    for i, output in zip(ids, preds):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db24074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "net = get_net(devices)\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)\n",
    "\n",
    "preds = []\n",
    "for data, label in test_iter:\n",
    "    output = torch.nn.functional.softmax(net(data.to(devices[0])), dim=1)\n",
    "    preds.extend(output.cpu().detach().numpy())\n",
    "ids = sorted(os.listdir(\n",
    "    os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.classes) + '\\n')\n",
    "    for i, output in zip(ids, preds):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11781369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "net = get_net(devices)\n",
    "train(net, train_valid_iter, None, num_epochs, lr, wd, devices, lr_period,\n",
    "      lr_decay)\n",
    "\n",
    "preds = []\n",
    "for data, label in test_iter:\n",
    "    output = paddle.nn.functional.softmax(net(data), axis=0)\n",
    "    preds.extend(output.detach().numpy())\n",
    "ids = sorted(os.listdir(\n",
    "    os.path.join(data_dir, 'train_valid_test', 'test', 'unknown')))\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.classes) + '\\n')\n",
    "    for i, output in zip(ids, preds):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325c346",
   "metadata": {},
   "source": [
    "上面的代码将生成一个`submission.csv`文件，以 :numref:`sec_kaggle_house`中描述的方式提在Kaggle上提交。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* ImageNet数据集中的图像比CIFAR-10图像尺寸大，我们可能会修改不同数据集上任务的图像增广操作。\n",
    "* 要对ImageNet数据集的子集进行分类，我们可以利用完整ImageNet数据集上的预训练模型来提取特征并仅训练小型自定义输出网络，这将减少计算时间和节省内存空间。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 试试使用完整Kaggle比赛数据集，增加`batch_size`（批量大小）和`num_epochs`（迭代轮数），或者设计其它超参数为`lr = 0.01`，`lr_period = 10`，和`lr_decay = 0.1`时，能取得什么结果？\n",
    "1. 如果使用更深的预训练模型，会得到更好的结果吗？如何调整超参数？能进一步改善结果吗？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/2832)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/2833)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11815)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd88c45f",
   "metadata": {},
   "source": [
    "# 自然语言处理：应用\n",
    ":label:`chap_nlp_app`\n",
    "\n",
    "前面我们学习了如何在文本序列中表示词元，\n",
    "并在 :numref:`chap_nlp_pretrain`中训练了词元的表示。\n",
    "这样的预训练文本表示可以通过不同模型架构，放入不同的下游自然语言处理任务。\n",
    "\n",
    "前一章我们提及到一些自然语言处理应用，这些应用没有预训练，只是为了解释深度学习架构。\n",
    "例如，在 :numref:`chap_rnn`中，\n",
    "我们依赖循环神经网络设计语言模型来生成类似中篇小说的文本。\n",
    "在 :numref:`chap_modern_rnn`和 :numref:`chap_attention`中，\n",
    "我们还设计了基于循环神经网络和注意力机制的机器翻译模型。\n",
    "\n",
    "然而，本书并不打算全面涵盖所有此类应用。\n",
    "相反，我们的重点是*如何应用深度语言表征学习来解决自然语言处理问题*。\n",
    "在给定预训练的文本表示的情况下，\n",
    "本章将探讨两种流行且具有代表性的下游自然语言处理任务：\n",
    "情感分析和自然语言推断，它们分别分析单个文本和文本对之间的关系。\n",
    "\n",
    "![预训练文本表示可以通过不同模型架构，放入不同的下游自然语言处理应用（本章重点介绍如何为不同的下游应用设计模型）](../img/nlp-map-app.svg)\n",
    ":label:`fig_nlp-map-app`\n",
    "\n",
    "如 :numref:`fig_nlp-map-app`所述，\n",
    "本章将重点描述然后使用不同类型的深度学习架构\n",
    "（如多层感知机、卷积神经网络、循环神经网络和注意力）\n",
    "设计自然语言处理模型。\n",
    "尽管在 :numref:`fig_nlp-map-app`中，\n",
    "可以将任何预训练的文本表示与任何应用的架构相结合，\n",
    "但我们选择了一些具有代表性的组合。\n",
    "具体来说，我们将探索基于循环神经网络和卷积神经网络的流行架构进行情感分析。\n",
    "对于自然语言推断，我们选择注意力和多层感知机来演示如何分析文本对。\n",
    "最后，我们介绍了如何为广泛的自然语言处理应用，\n",
    "如在序列级（单文本分类和文本对分类）和词元级（文本标注和问答）上\n",
    "对预训练BERT模型进行微调。\n",
    "作为一个具体的经验案例，我们将针对自然语言推断对BERT进行微调。\n",
    "\n",
    "正如我们在 :numref:`sec_bert`中介绍的那样，\n",
    "对于广泛的自然语言处理应用，BERT只需要最少的架构更改。\n",
    "然而，这一好处是以微调下游应用的大量BERT参数为代价的。\n",
    "当空间或时间有限时，基于多层感知机、卷积神经网络、循环神经网络\n",
    "和注意力的精心构建的模型更具可行性。\n",
    "下面，我们从情感分析应用开始，分别解读基于循环神经网络和卷积神经网络的模型设计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0cbf32",
   "metadata": {
    "attributes": {
     "classes": [
      "toc"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    ":maxdepth: 2\n",
    "\n",
    "sentiment-analysis-and-dataset\n",
    "sentiment-analysis-rnn\n",
    "sentiment-analysis-cnn\n",
    "natural-language-inference-and-dataset\n",
    "natural-language-inference-attention\n",
    "finetuning-bert\n",
    "natural-language-inference-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d6b52",
   "metadata": {},
   "source": [
    "# 情感分析及数据集\n",
    ":label:`sec_sentiment`\n",
    "\n",
    "随着在线社交媒体和评论平台的快速发展，大量评论的数据被记录下来。这些数据具有支持决策过程的巨大潜力。\n",
    "*情感分析*（sentiment analysis）研究人们在文本中\n",
    "（如产品评论、博客评论和论坛讨论等）“隐藏”的情绪。\n",
    "它在广泛应用于政治（如公众对政策的情绪分析）、\n",
    "金融（如市场情绪分析）和营销（如产品研究和品牌管理）等领域。\n",
    "\n",
    "由于情感可以被分类为离散的极性或尺度（例如，积极的和消极的），我们可以将情感分析看作一项文本分类任务，它将可变长度的文本序列转换为固定长度的文本类别。在本章中，我们将使用斯坦福大学的[大型电影评论数据集（large movie review dataset）](https://ai.stanford.edu/~amaas/data/sentiment/)进行情感分析。它由一个训练集和一个测试集组成，其中包含从IMDb下载的25000个电影评论。在这两个数据集中，“积极”和“消极”标签的数量相同，表示不同的情感极性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5079a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "import os\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05065f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1fd395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "from paddle import nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff91afb",
   "metadata": {},
   "source": [
    "##  读取数据集\n",
    "\n",
    "首先，下载并提取路径`../data/aclImdb`中的IMDb评论数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ddf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "d2l.DATA_HUB['aclImdb'] = (\n",
    "    'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz',\n",
    "    '01ada507287d82875905620988597833ad4e0903')\n",
    "\n",
    "data_dir = d2l.download_extract('aclImdb', 'aclImdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38704db0",
   "metadata": {},
   "source": [
    "接下来，读取训练和测试数据集。每个样本都是一个评论及其标签：1表示“积极”，0表示“消极”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def read_imdb(data_dir, is_train):\n",
    "    \"\"\"读取IMDb评论数据集文本序列和标签\"\"\"\n",
    "    data, labels = [], []\n",
    "    for label in ('pos', 'neg'):\n",
    "        folder_name = os.path.join(data_dir, 'train' if is_train else 'test',\n",
    "                                   label)\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "train_data = read_imdb(data_dir, is_train=True)\n",
    "print('训练集数目：', len(train_data[0]))\n",
    "for x, y in zip(train_data[0][:3], train_data[1][:3]):\n",
    "    print('标签：', y, 'review:', x[0:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6213e8",
   "metadata": {},
   "source": [
    "## 预处理数据集\n",
    "\n",
    "将每个单词作为一个词元，过滤掉出现不到5次的单词，我们从训练数据集中创建一个词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803128af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "vocab = d2l.Vocab(train_tokens, min_freq=5, reserved_tokens=['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5935321",
   "metadata": {},
   "source": [
    "在词元化之后，让我们绘制评论词元长度的直方图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "d2l.set_figsize()\n",
    "d2l.plt.xlabel('# tokens per review')\n",
    "d2l.plt.ylabel('count')\n",
    "d2l.plt.hist([len(line) for line in train_tokens], bins=range(0, 1000, 50));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe5373",
   "metadata": {},
   "source": [
    "正如我们所料，评论的长度各不相同。为了每次处理一小批量这样的评论，我们通过截断和填充将每个评论的长度设置为500。这类似于 :numref:`sec_machine_translation`中对机器翻译数据集的预处理步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99134436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "num_steps = 500  # 序列长度\n",
    "train_features = d2l.tensor([d2l.truncate_pad(\n",
    "    vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d79bd3",
   "metadata": {},
   "source": [
    "## 创建数据迭代器\n",
    "\n",
    "现在我们可以创建数据迭代器了。在每次迭代中，都会返回一小批量样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28dfb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = d2l.load_array((train_features, train_data[1]), 64)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X:', X.shape, ', y:', y.shape)\n",
    "    break\n",
    "print('小批量数目：', len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c525217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "train_iter = d2l.load_array((train_features, \n",
    "    torch.tensor(train_data[1])), 64)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X:', X.shape, ', y:', y.shape)\n",
    "    break\n",
    "print('小批量数目：', len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "train_iter = d2l.load_array((train_features,\n",
    "    d2l.tensor(train_data[1])), 64)\n",
    "\n",
    "for X, y in train_iter:\n",
    "    print('X:', X.shape, ', y:', y.shape)\n",
    "    break\n",
    "print('小批量数目：', len(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b1099e",
   "metadata": {},
   "source": [
    "## 整合代码\n",
    "\n",
    "最后，我们将上述步骤封装到`load_data_imdb`函数中。它返回训练和测试数据迭代器以及IMDb评论数据集的词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_imdb(batch_size, num_steps=500):\n",
    "    \"\"\"返回数据迭代器和IMDb评论数据集的词表\"\"\"\n",
    "    data_dir = d2l.download_extract('aclImdb', 'aclImdb')\n",
    "    train_data = read_imdb(data_dir, True)\n",
    "    test_data = read_imdb(data_dir, False)\n",
    "    train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "    test_tokens = d2l.tokenize(test_data[0], token='word')\n",
    "    vocab = d2l.Vocab(train_tokens, min_freq=5)\n",
    "    train_features = np.array([d2l.truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "    test_features = np.array([d2l.truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n",
    "    train_iter = d2l.load_array((train_features, train_data[1]), batch_size)\n",
    "    test_iter = d2l.load_array((test_features, test_data[1]), batch_size,\n",
    "                               is_train=False)\n",
    "    return train_iter, test_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1382f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def load_data_imdb(batch_size, num_steps=500):\n",
    "    \"\"\"返回数据迭代器和IMDb评论数据集的词表\"\"\"\n",
    "    data_dir = d2l.download_extract('aclImdb', 'aclImdb')\n",
    "    train_data = read_imdb(data_dir, True)\n",
    "    test_data = read_imdb(data_dir, False)\n",
    "    train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "    test_tokens = d2l.tokenize(test_data[0], token='word')\n",
    "    vocab = d2l.Vocab(train_tokens, min_freq=5)\n",
    "    train_features = torch.tensor([d2l.truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "    test_features = torch.tensor([d2l.truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n",
    "    train_iter = d2l.load_array((train_features, torch.tensor(train_data[1])),\n",
    "                                batch_size)\n",
    "    test_iter = d2l.load_array((test_features, torch.tensor(test_data[1])),\n",
    "                               batch_size,\n",
    "                               is_train=False)\n",
    "    return train_iter, test_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87974b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def load_data_imdb(batch_size, num_steps=500):\n",
    "    \"\"\"返回数据迭代器和IMDb评论数据集的词表\"\"\"\n",
    "    data_dir = d2l.download_extract('aclImdb', 'aclImdb')\n",
    "    train_data = read_imdb(data_dir, True)\n",
    "    test_data = read_imdb(data_dir, False)\n",
    "    train_tokens = d2l.tokenize(train_data[0], token='word')\n",
    "    test_tokens = d2l.tokenize(test_data[0], token='word')\n",
    "    vocab = d2l.Vocab(train_tokens, min_freq=5)\n",
    "    train_features = d2l.tensor([d2l.truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in train_tokens])\n",
    "    test_features = d2l.tensor([d2l.truncate_pad(\n",
    "        vocab[line], num_steps, vocab['<pad>']) for line in test_tokens])\n",
    "    train_iter = d2l.load_array((train_features, d2l.tensor(train_data[1])),\n",
    "                                batch_size)\n",
    "    test_iter = d2l.load_array((test_features, d2l.tensor(test_data[1])),\n",
    "                               batch_size,\n",
    "                               is_train=False)\n",
    "    return train_iter, test_iter, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51a8c6",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 情感分析研究人们在文本中的情感，这被认为是一个文本分类问题，它将可变长度的文本序列进行转换转换为固定长度的文本类别。\n",
    "* 经过预处理后，我们可以使用词表将IMDb评论数据集加载到数据迭代器中。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 我们可以修改本节中的哪些超参数来加速训练情感分析模型？\n",
    "1. 请实现一个函数来将[Amazon reviews](https://snap.stanford.edu/data/web-Amazon.html)的数据集加载到数据迭代器中进行情感分析。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/5725)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/5726)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11825)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027e231",
   "metadata": {},
   "source": [
    "# 情感分析：使用循环神经网络\n",
    ":label:`sec_sentiment_rnn`\n",
    "\n",
    "与词相似度和类比任务一样，我们也可以将预先训练的词向量应用于情感分析。由于 :numref:`sec_sentiment`中的IMDb评论数据集不是很大，使用在大规模语料库上预训练的文本表示可以减少模型的过拟合。作为 :numref:`fig_nlp-map-sa-rnn`中所示的具体示例，我们将使用预训练的GloVe模型来表示每个词元，并将这些词元表示送入多层双向循环神经网络以获得文本序列表示，该文本序列表示将被转换为情感分析输出 :cite:`Maas.Daly.Pham.ea.2011`。对于相同的下游应用，我们稍后将考虑不同的架构选择。\n",
    "\n",
    "![将GloVe送入基于循环神经网络的架构，用于情感分析](../img/nlp-map-sa-rnn.svg)\n",
    ":label:`fig_nlp-map-sa-rnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08824d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, init, np, npx\n",
    "from mxnet.gluon import nn, rnn\n",
    "npx.set_np()\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49668c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "from paddle import nn\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3066e90",
   "metadata": {},
   "source": [
    "## 使用循环神经网络表示单个文本\n",
    "\n",
    "在文本分类任务（如情感分析）中，可变长度的文本序列将被转换为固定长度的类别。在下面的`BiRNN`类中，虽然文本序列的每个词元经由嵌入层（`self.embedding`）获得其单独的预训练GloVe表示，但是整个序列由双向循环神经网络（`self.encoder`）编码。更具体地说，双向长短期记忆网络在初始和最终时间步的隐状态（在最后一层）被连结起来作为文本序列的表示。然后，通过一个具有两个输出（“积极”和“消极”）的全连接层（`self.decoder`），将此单一文本表示转换为输出类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca45c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens,\n",
    "                 num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 将bidirectional设置为True以获取双向循环神经网络\n",
    "        self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers,\n",
    "                                bidirectional=True, input_size=embed_size)\n",
    "        self.decoder = nn.Dense(2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是（批量大小，时间步数）\n",
    "        # 因为长短期记忆网络要求其输入的第一个维度是时间维，\n",
    "        # 所以在获得词元表示之前，输入会被转置。\n",
    "        # 输出形状为（时间步数，批量大小，词向量维度）\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        # 返回上一个隐藏层在不同时间步的隐状态，\n",
    "        # outputs的形状是（时间步数，批量大小，2*隐藏单元数）\n",
    "        outputs = self.encoder(embeddings)\n",
    "        # 连结初始和最终时间步的隐状态，作为全连接层的输入，\n",
    "        # 其形状为（批量大小，4*隐藏单元数）\n",
    "        encoding = np.concatenate((outputs[0], outputs[-1]), axis=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f62c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens,\n",
    "                 num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 将bidirectional设置为True以获取双向循环神经网络\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是（批量大小，时间步数）\n",
    "        # 因为长短期记忆网络要求其输入的第一个维度是时间维，\n",
    "        # 所以在获得词元表示之前，输入会被转置。\n",
    "        # 输出形状为（时间步数，批量大小，词向量维度）\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        self.encoder.flatten_parameters()\n",
    "        # 返回上一个隐藏层在不同时间步的隐状态，\n",
    "        # outputs的形状是（时间步数，批量大小，2*隐藏单元数）\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "        # 连结初始和最终时间步的隐状态，作为全连接层的输入，\n",
    "        # 其形状为（批量大小，4*隐藏单元数）\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class BiRNN(nn.Layer):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens,\n",
    "                 num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 将direction设置为'bidirect'或'bidirectional'以获取双向循环神经网络\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,\n",
    "                                direction='bidirect',time_major=True)\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是（批量大小，时间步数）\n",
    "        # 因为长短期记忆网络要求其输入的第一个维度是时间维，\n",
    "        # 所以在获得词元表示之前，输入会被转置。\n",
    "        # 输出形状为（时间步数，批量大小，词向量维度）\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        self.encoder.flatten_parameters()\n",
    "        # 返回上一个隐藏层在不同时间步的隐状态，\n",
    "        # outputs的形状是（时间步数，批量大小，2*隐藏单元数）\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "        # 连结初始和最终时间步的隐状态，作为全连接层的输入，\n",
    "        # 其形状为（批量大小，4*隐藏单元数）\n",
    "        encoding = paddle.concat((outputs[0], outputs[-1]), axis=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3693d0",
   "metadata": {},
   "source": [
    "让我们构造一个具有两个隐藏层的双向循环神经网络来表示单个文本以进行情感分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "devices = d2l.try_all_gpus()\n",
    "net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a48912",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(init.Xavier(), ctx=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7dfcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.LSTM:\n",
    "        for param in m._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(m._parameters[param])\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccc714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def init_weights(layer):\n",
    "    if isinstance(layer,(nn.Linear, nn.Embedding)):\n",
    "        if isinstance(layer.weight, paddle.Tensor):\n",
    "            nn.initializer.XavierUniform()(layer.weight)\n",
    "    if isinstance(layer, nn.LSTM):\n",
    "        for n, p in layer.named_parameters():\n",
    "            if \"weigth\" in n:\n",
    "                nn.initializer.XavierUniform()(p)\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab5622",
   "metadata": {},
   "source": [
    "## 加载预训练的词向量\n",
    "\n",
    "下面，我们为词表中的单词加载预训练的100维（需要与`embed_size`一致）的GloVe嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df8634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "glove_embedding = d2l.TokenEmbedding('glove.6b.100d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8c74c",
   "metadata": {},
   "source": [
    "打印词表中所有词元向量的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e68991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f11ff",
   "metadata": {},
   "source": [
    "我们使用这些预训练的词向量来表示评论中的词元，并且在训练期间不要更新这些向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce39d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.embedding.weight.set_data(embeds)\n",
    "net.embedding.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb6098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "net.embedding.weight.data.copy_(embeds)\n",
    "net.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62698344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "net.embedding.weight.set_value(embeds)\n",
    "net.embedding.weight.stop_gradient = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54ba8c",
   "metadata": {},
   "source": [
    "## 训练和评估模型\n",
    "\n",
    "现在我们可以训练双向循环神经网络进行情感分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.01, 5\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, \n",
    "    devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "lr, num_epochs = 0.01, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "    devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e34f128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "lr, num_epochs = 0.01, 2\n",
    "trainer = paddle.optimizer.Adam(learning_rate=lr,parameters=net.parameters())\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "    devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42818a",
   "metadata": {},
   "source": [
    "我们定义以下函数来使用训练好的模型`net`预测文本序列的情感。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def predict_sentiment(net, vocab, sequence):\n",
    "    \"\"\"预测文本序列的情感\"\"\"\n",
    "    sequence = np.array(vocab[sequence.split()], ctx=d2l.try_gpu())\n",
    "    label = np.argmax(net(sequence.reshape(1, -1)), axis=1)\n",
    "    return 'positive' if label == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91675a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def predict_sentiment(net, vocab, sequence):\n",
    "    \"\"\"预测文本序列的情感\"\"\"\n",
    "    sequence = torch.tensor(vocab[sequence.split()], device=d2l.try_gpu())\n",
    "    label = torch.argmax(net(sequence.reshape(1, -1)), dim=1)\n",
    "    return 'positive' if label == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2218740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def predict_sentiment(net, vocab, sequence):\n",
    "    \"\"\"预测文本序列的情感\"\"\"\n",
    "    sequence = paddle.to_tensor(vocab[sequence.split()], place=d2l.try_gpu())\n",
    "    label = paddle.argmax(net(sequence.reshape((1, -1))), axis=1)\n",
    "    return 'positive' if label == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5bc808",
   "metadata": {},
   "source": [
    "最后，让我们使用训练好的模型对两个简单的句子进行情感预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "predict_sentiment(net, vocab, 'this movie is so great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "predict_sentiment(net, vocab, 'this movie is so bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e509ca",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 预训练的词向量可以表示文本序列中的各个词元。\n",
    "* 双向循环神经网络可以表示文本序列。例如通过连结初始和最终时间步的隐状态，可以使用全连接的层将该单个文本表示转换为类别。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 增加迭代轮数可以提高训练和测试的准确性吗？调优其他超参数怎么样？\n",
    "1. 使用较大的预训练词向量，例如300维的GloVe嵌入。它是否提高了分类精度？\n",
    "1. 是否可以通过spaCy词元化来提高分类精度？需要安装Spacy（`pip install spacy`）和英语语言包（`python -m spacy download en`）。在代码中，首先导入Spacy（`import spacy`）。然后，加载Spacy英语软件包（`spacy_en = spacy.load('en')`）。最后，定义函数`def tokenizer(text): return [tok.text for tok in spacy_en.tokenizer(text)]`并替换原来的`tokenizer`函数。请注意GloVe和spaCy中短语标记的不同形式。例如，短语标记“new york”在GloVe中的形式是“new-york”，而在spaCy词元化之后的形式是“new york”。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/5723)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/5724)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11826)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb9df1",
   "metadata": {},
   "source": [
    "# 情感分析：使用卷积神经网络\n",
    ":label:`sec_sentiment_cnn`\n",
    "\n",
    "在 :numref:`chap_cnn`中，我们探讨了使用二维卷积神经网络处理二维图像数据的机制，并将其应用于局部特征，如相邻像素。虽然卷积神经网络最初是为计算机视觉设计的，但它也被广泛用于自然语言处理。简单地说，只要将任何文本序列想象成一维图像即可。通过这种方式，一维卷积神经网络可以处理文本中的局部特征，例如$n$元语法。\n",
    "\n",
    "本节将使用*textCNN*模型来演示如何设计一个表示单个文本 :cite:`Kim.2014`的卷积神经网络架构。与 :numref:`fig_nlp-map-sa-rnn`中使用带有GloVe预训练的循环神经网络架构进行情感分析相比， :numref:`fig_nlp-map-sa-cnn`中唯一的区别在于架构的选择。\n",
    "\n",
    "![将GloVe放入卷积神经网络架构进行情感分析](../img/nlp-map-sa-cnn.svg)\n",
    ":label:`fig_nlp-map-sa-cnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b66f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020fa675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19919b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "from paddle import nn\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e3b04c",
   "metadata": {},
   "source": [
    "## 一维卷积\n",
    "\n",
    "在介绍该模型之前，让我们先看看一维卷积是如何工作的。请记住，这只是基于互相关运算的二维卷积的特例。\n",
    "\n",
    "![一维互相关运算。阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素：$0\\times1+1\\times2=2$](../img/conv1d.svg)\n",
    ":label:`fig_conv1d`\n",
    "\n",
    "如 :numref:`fig_conv1d`中所示，在一维情况下，卷积窗口在输入张量上从左向右滑动。在滑动期间，卷积窗口中某个位置包含的输入子张量（例如， :numref:`fig_conv1d`中的$0$和$1$）和核张量（例如， :numref:`fig_conv1d`中的$1$和$2$）按元素相乘。这些乘法的总和在输出张量的相应位置给出单个标量值（例如， :numref:`fig_conv1d`中的$0\\times1+1\\times2=2$）。\n",
    "\n",
    "我们在下面的`corr1d`函数中实现了一维互相关。给定输入张量`X`和核张量`K`，它返回输出张量`Y`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdec55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "def corr1d(X, K):\n",
    "    w = K.shape[0]\n",
    "    Y = d2l.zeros((X.shape[0] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y[i] = (X[i: i + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21945590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def corr1d(X, K):\n",
    "    w = K.shape[0]\n",
    "    Y = d2l.zeros([X.shape[0] - w + 1], dtype=X.dtype)\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y[i] = (X[i: i + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f21da9f",
   "metadata": {},
   "source": [
    "我们可以从 :numref:`fig_conv1d`构造输入张量`X`和核张量`K`来验证上述一维互相关实现的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aba178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "X, K = d2l.tensor([0, 1, 2, 3, 4, 5, 6]), d2l.tensor([1, 2])\n",
    "corr1d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ed3d18",
   "metadata": {},
   "source": [
    "对于任何具有多个通道的一维输入，卷积核需要具有相同数量的输入通道。然后，对于每个通道，对输入的一维张量和卷积核的一维张量执行互相关运算，将所有通道上的结果相加以产生一维输出张量。 :numref:`fig_conv1d_channel`演示了具有3个输入通道的一维互相关操作。\n",
    "\n",
    "![具有3个输入通道的一维互相关运算。阴影部分是第一个输出元素以及用于输出计算的输入和核张量元素：$2\\times(-1)+3\\times(-3)+1\\times3+2\\times4+0\\times1+1\\times2=2$](../img/conv1d-channel.svg)\n",
    ":label:`fig_conv1d_channel`\n",
    "\n",
    "我们可以实现多个输入通道的一维互相关运算，并在 :numref:`fig_conv1d_channel`中验证结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5249907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def corr1d_multi_in(X, K):\n",
    "    # 首先，遍历'X'和'K'的第0维（通道维）。然后，把它们加在一起\n",
    "    return sum(corr1d(x, k) for x, k in zip(X, K))\n",
    "\n",
    "X = d2l.tensor([[0, 1, 2, 3, 4, 5, 6],\n",
    "              [1, 2, 3, 4, 5, 6, 7],\n",
    "              [2, 3, 4, 5, 6, 7, 8]])\n",
    "K = d2l.tensor([[1, 2], [3, 4], [-1, -3]])\n",
    "corr1d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360dbf91",
   "metadata": {},
   "source": [
    "注意，多输入通道的一维互相关等同于单输入通道的二维互相关。举例说明， :numref:`fig_conv1d_channel`中的多输入通道一维互相关的等价形式是 :numref:`fig_conv1d_2d`中的单输入通道二维互相关，其中卷积核的高度必须与输入张量的高度相同。\n",
    "\n",
    "![具有单个输入通道的二维互相关操作。阴影部分是第一个输出元素以及用于输出计算的输入和内核张量元素： $2\\times(-1)+3\\times(-3)+1\\times3+2\\times4+0\\times1+1\\times2=2$](../img/conv1d-2d.svg)\n",
    ":label:`fig_conv1d_2d`\n",
    "\n",
    " :numref:`fig_conv1d`和 :numref:`fig_conv1d_channel`中的输出都只有一个通道。与 :numref:`subsec_multi-output-channels`中描述的具有多个输出通道的二维卷积相同，我们也可以为一维卷积指定多个输出通道。\n",
    "\n",
    "## 最大时间汇聚层\n",
    "\n",
    "类似地，我们可以使用汇聚层从序列表示中提取最大值，作为跨时间步的最重要特征。textCNN中使用的*最大时间汇聚层*的工作原理类似于一维全局汇聚 :cite:`Collobert.Weston.Bottou.ea.2011`。对于每个通道在不同时间步存储值的多通道输入，每个通道的输出是该通道的最大值。请注意，最大时间汇聚允许在不同通道上使用不同数量的时间步。\n",
    "\n",
    "## textCNN模型\n",
    "\n",
    "使用一维卷积和最大时间汇聚，textCNN模型将单个预训练的词元表示作为输入，然后获得并转换用于下游应用的序列表示。\n",
    "\n",
    "对于具有由$d$维向量表示的$n$个词元的单个文本序列，输入张量的宽度、高度和通道数分别为$n$、$1$和$d$。textCNN模型将输入转换为输出，如下所示：\n",
    "\n",
    "1. 定义多个一维卷积核，并分别对输入执行卷积运算。具有不同宽度的卷积核可以捕获不同数目的相邻词元之间的局部特征。\n",
    "1. 在所有输出通道上执行最大时间汇聚层，然后将所有标量汇聚输出连结为向量。\n",
    "1. 使用全连接层将连结后的向量转换为输出类别。Dropout可以用来减少过拟合。\n",
    "\n",
    "![textCNN的模型架构](../img/textcnn.svg)\n",
    ":label:`fig_conv1d_textcnn`\n",
    "\n",
    " :numref:`fig_conv1d_textcnn`通过一个具体的例子说明了textCNN的模型架构。输入是具有11个词元的句子，其中每个词元由6维向量表示。因此，我们有一个宽度为11的6通道输入。定义两个宽度为2和4的一维卷积核，分别具有4个和5个输出通道。它们产生4个宽度为$11-2+1=10$的输出通道和5个宽度为$11-4+1=8$的输出通道。尽管这9个通道的宽度不同，但最大时间汇聚层给出了一个连结的9维向量，该向量最终被转换为用于二元情感预测的2维输出向量。\n",
    "\n",
    "### 定义模型\n",
    "\n",
    "我们在下面的类中实现textCNN模型。与 :numref:`sec_sentiment_rnn`的双向循环神经网络模型相比，除了用卷积层代替循环神经网络层外，我们还使用了两个嵌入层：一个是可训练权重，另一个是固定权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb462e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,\n",
    "                 **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 这个嵌入层不需要训练\n",
    "        self.constant_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Dense(2)\n",
    "        # 最大时间汇聚层没有参数，因此可以共享此实例\n",
    "        self.pool = nn.GlobalMaxPool1D()\n",
    "        # 创建多个一维卷积层\n",
    "        self.convs = nn.Sequential()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.add(nn.Conv1D(c, k, activation='relu'))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 沿着向量维度将两个嵌入层连结起来，\n",
    "        # 每个嵌入层的输出形状都是（批量大小，词元数量，词元向量维度）连结起来\n",
    "        embeddings = np.concatenate((\n",
    "            self.embedding(inputs), self.constant_embedding(inputs)), axis=2)\n",
    "        # 根据一维卷积层的输入格式，重新排列张量，以便通道作为第2维\n",
    "        embeddings = embeddings.transpose(0, 2, 1)\n",
    "        # 每个一维卷积层在最大时间汇聚层合并后，获得的张量形状是（批量大小，通道数，1）\n",
    "        # 删除最后一个维度并沿通道维度连结\n",
    "        encoding = np.concatenate([\n",
    "            np.squeeze(self.pool(conv(embeddings)), axis=-1)\n",
    "            for conv in self.convs], axis=1)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,\n",
    "                 **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 这个嵌入层不需要训练\n",
    "        self.constant_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        # 最大时间汇聚层没有参数，因此可以共享此实例\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # 创建多个一维卷积层\n",
    "        self.convs = nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(2 * embed_size, c, k))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 沿着向量维度将两个嵌入层连结起来，\n",
    "        # 每个嵌入层的输出形状都是（批量大小，词元数量，词元向量维度）连结起来\n",
    "        embeddings = torch.cat((\n",
    "            self.embedding(inputs), self.constant_embedding(inputs)), dim=2)\n",
    "        # 根据一维卷积层的输入格式，重新排列张量，以便通道作为第2维\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        # 每个一维卷积层在最大时间汇聚层合并后，获得的张量形状是（批量大小，通道数，1）\n",
    "        # 删除最后一个维度并沿通道维度连结\n",
    "        encoding = torch.cat([\n",
    "            torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)\n",
    "            for conv in self.convs], dim=1)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47871dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class TextCNN(nn.Layer):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,\n",
    "                 **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # 这个嵌入层不需要训练\n",
    "        self.constant_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        # 最大时间汇聚层没有参数，因此可以共享此实例\n",
    "        self.pool = nn.AdaptiveAvgPool1D(1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # 创建多个一维卷积层\n",
    "        self.convs = nn.LayerList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1D(2 * embed_size, c, k))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 沿着向量维度将两个嵌入层连结起来，\n",
    "        # 每个嵌入层的输出形状都是（批量大小，词元数量，词元向量维度）连结起来\n",
    "        embeddings = paddle.concat((\n",
    "            self.embedding(inputs), self.constant_embedding(inputs)), axis=2)\n",
    "        # 根据一维卷积层的输入格式，重新排列张量，以便通道作为第2维\n",
    "        embeddings = embeddings.transpose([0, 2, 1])\n",
    "        # 每个一维卷积层在最大时间汇聚层合并后，获得的张量形状是（批量大小，通道数，1）\n",
    "        # 删除最后一个维度并沿通道维度连结\n",
    "        encoding = paddle.concat([\n",
    "            paddle.squeeze(self.relu(self.pool(conv(embeddings))), axis=-1)\n",
    "            for conv in self.convs], axis=1)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c639d913",
   "metadata": {},
   "source": [
    "让我们创建一个textCNN实例。它有3个卷积层，卷积核宽度分别为3、4和5，均有100个输出通道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ac122",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "devices = d2l.try_all_gpus()\n",
    "net = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels)\n",
    "net.initialize(init.Xavier(), ctx=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ebc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "devices = d2l.try_all_gpus()\n",
    "net = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) in (nn.Linear, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776abe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "devices = d2l.try_all_gpus()\n",
    "net = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels)\n",
    "\n",
    "def init_weights(net):\n",
    "    init_normal = nn.initializer.XavierUniform()\n",
    "    for i in net.sublayers():\n",
    "        if type(i) in [nn.Linear, nn.Conv1D]:  \n",
    "            init_normal(i.weight)\n",
    "            \n",
    "init_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e63a57",
   "metadata": {},
   "source": [
    "### 加载预训练词向量\n",
    "\n",
    "与 :numref:`sec_sentiment_rnn`相同，我们加载预训练的100维GloVe嵌入作为初始化的词元表示。这些词元表示（嵌入权重）在`embedding`中将被训练，在`constant_embedding`中将被固定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "net.embedding.weight.set_data(embeds)\n",
    "net.constant_embedding.weight.set_data(embeds)\n",
    "net.constant_embedding.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "glove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "net.embedding.weight.data.copy_(embeds)\n",
    "net.constant_embedding.weight.data.copy_(embeds)\n",
    "net.constant_embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bedb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "glove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "net.embedding.weight.set_value(embeds)\n",
    "net.constant_embedding.weight.set_value(embeds)\n",
    "net.constant_embedding.weight.stop_gradient = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea291cb6",
   "metadata": {},
   "source": [
    "### 训练和评估模型\n",
    "\n",
    "现在我们可以训练textCNN模型进行情感分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb588dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "lr, num_epochs = 0.001, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e834de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "lr, num_epochs = 0.001, 5\n",
    "trainer = paddle.optimizer.Adam(learning_rate=lr, parameters=net.parameters())\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f686d4b",
   "metadata": {},
   "source": [
    "下面，我们使用训练好的模型来预测两个简单句子的情感。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef31199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "d2l.predict_sentiment(net, vocab, 'this movie is so great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fabf02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "d2l.predict_sentiment(net, vocab, 'this movie is so bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d47511a",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 一维卷积神经网络可以处理文本中的局部特征，例如$n$元语法。\n",
    "* 多输入通道的一维互相关等价于单输入通道的二维互相关。\n",
    "* 最大时间汇聚层允许在不同通道上使用不同数量的时间步长。\n",
    "* textCNN模型使用一维卷积层和最大时间汇聚层将单个词元表示转换为下游应用输出。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 调整超参数，并比较 :numref:`sec_sentiment_rnn`中用于情感分析的架构和本节中用于情感分析的架构，例如在分类精度和计算效率方面。\n",
    "1. 请试着用 :numref:`sec_sentiment_rnn`练习中介绍的方法进一步提高模型的分类精度。\n",
    "1. 在输入表示中添加位置编码。它是否提高了分类的精度？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/5719)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/5720)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11827)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd082b5",
   "metadata": {},
   "source": [
    "# 自然语言推断与数据集\n",
    ":label:`sec_natural-language-inference-and-dataset`\n",
    "\n",
    "在 :numref:`sec_sentiment`中，我们讨论了情感分析问题。这个任务的目的是将单个文本序列分类到预定义的类别中，例如一组情感极性中。然而，当需要决定一个句子是否可以从另一个句子推断出来，或者需要通过识别语义等价的句子来消除句子间冗余时，知道如何对一个文本序列进行分类是不够的。相反，我们需要能够对成对的文本序列进行推断。\n",
    "\n",
    "## 自然语言推断\n",
    "\n",
    "*自然语言推断*（natural language inference）主要研究\n",
    "*假设*（hypothesis）是否可以从*前提*（premise）中推断出来，\n",
    "其中两者都是文本序列。\n",
    "换言之，自然语言推断决定了一对文本序列之间的逻辑关系。这类关系通常分为三种类型：\n",
    "\n",
    "* *蕴涵*（entailment）：假设可以从前提中推断出来。\n",
    "* *矛盾*（contradiction）：假设的否定可以从前提中推断出来。\n",
    "* *中性*（neutral）：所有其他情况。\n",
    "\n",
    "自然语言推断也被称为识别文本蕴涵任务。\n",
    "例如，下面的一个文本对将被贴上“蕴涵”的标签，因为假设中的“表白”可以从前提中的“拥抱”中推断出来。\n",
    "\n",
    ">前提：两个女人拥抱在一起。\n",
    "\n",
    ">假设：两个女人在示爱。\n",
    "\n",
    "下面是一个“矛盾”的例子，因为“运行编码示例”表示“不睡觉”，而不是“睡觉”。\n",
    "\n",
    ">前提：一名男子正在运行Dive Into Deep Learning的编码示例。\n",
    "\n",
    ">假设：该男子正在睡觉。\n",
    "\n",
    "第三个例子显示了一种“中性”关系，因为“正在为我们表演”这一事实无法推断出“出名”或“不出名”。\n",
    "\n",
    ">前提：音乐家们正在为我们表演。\n",
    "\n",
    ">假设：音乐家很有名。\n",
    "\n",
    "自然语言推断一直是理解自然语言的中心话题。它有着广泛的应用，从信息检索到开放领域的问答。为了研究这个问题，我们将首先研究一个流行的自然语言推断基准数据集。\n",
    "\n",
    "## 斯坦福自然语言推断（SNLI）数据集\n",
    "\n",
    "[**斯坦福自然语言推断语料库（Stanford Natural Language Inference，SNLI）**]是由500000多个带标签的英语句子对组成的集合 :cite:`Bowman.Angeli.Potts.ea.2015`。我们在路径`../data/snli_1.0`中下载并存储提取的SNLI数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ad4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, np, npx\n",
    "import os\n",
    "import re\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['SNLI'] = (\n",
    "    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
    "    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "\n",
    "data_dir = d2l.download_extract('SNLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import re\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['SNLI'] = (\n",
    "    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
    "    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "\n",
    "data_dir = d2l.download_extract('SNLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fbd894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "from paddle import nn\n",
    "import os\n",
    "import re\n",
    "\n",
    "#@save\n",
    "d2l.DATA_HUB['SNLI'] = (\n",
    "    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
    "    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "\n",
    "data_dir = d2l.download_extract('SNLI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ddbe43",
   "metadata": {},
   "source": [
    "### [**读取数据集**]\n",
    "\n",
    "原始的SNLI数据集包含的信息比我们在实验中真正需要的信息丰富得多。因此，我们定义函数`read_snli`以仅提取数据集的一部分，然后返回前提、假设及其标签的列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d0c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "def read_snli(data_dir, is_train):\n",
    "    \"\"\"将SNLI数据集解析为前提、假设和标签\"\"\"\n",
    "    def extract_text(s):\n",
    "        # 删除我们不会使用的信息\n",
    "        s = re.sub('\\\\(', '', s) \n",
    "        s = re.sub('\\\\)', '', s)\n",
    "        # 用一个空格替换两个或多个连续的空格\n",
    "        s = re.sub('\\\\s{2,}', ' ', s)\n",
    "        return s.strip()\n",
    "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt'\n",
    "                             if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
    "    hypotheses = [extract_text(row[2]) for row in rows if row[0] \\\n",
    "                in label_set]\n",
    "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
    "    return premises, hypotheses, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a9ff6",
   "metadata": {},
   "source": [
    "现在让我们[**打印前3对**]前提和假设，以及它们的标签（“0”“1”和“2”分别对应于“蕴涵”“矛盾”和“中性”）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83569a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "train_data = read_snli(data_dir, is_train=True)\n",
    "for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n",
    "    print('前提：', x0)\n",
    "    print('假设：', x1)\n",
    "    print('标签：', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8367c85f",
   "metadata": {},
   "source": [
    "训练集约有550000对，测试集约有10000对。下面显示了训练集和测试集中的三个[**标签“蕴涵”“矛盾”和“中性”是平衡的**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5b5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "test_data = read_snli(data_dir, is_train=False)\n",
    "for data in [train_data, test_data]:\n",
    "    print([[row for row in data[2]].count(i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6086db48",
   "metadata": {},
   "source": [
    "### [**定义用于加载数据集的类**]\n",
    "\n",
    "下面我们来定义一个用于加载SNLI数据集的类。类构造函数中的变量`num_steps`指定文本序列的长度，使得每个小批量序列将具有相同的形状。换句话说，在较长序列中的前`num_steps`个标记之后的标记被截断，而特殊标记“&lt;pad&gt;”将被附加到较短的序列后，直到它们的长度变为`num_steps`。通过实现`__getitem__`功能，我们可以任意访问带有索引`idx`的前提、假设和标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class SNLIDataset(gluon.data.Dataset):\n",
    "    \"\"\"用于加载SNLI数据集的自定义数据集\"\"\"\n",
    "    def __init__(self, dataset, num_steps, vocab=None):\n",
    "        self.num_steps = num_steps\n",
    "        all_premise_tokens = d2l.tokenize(dataset[0])\n",
    "        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n",
    "        if vocab is None:\n",
    "            self.vocab = d2l.Vocab(all_premise_tokens + \\\n",
    "                all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        self.premises = self._pad(all_premise_tokens)\n",
    "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
    "        self.labels = np.array(dataset[2])\n",
    "        print('read ' + str(len(self.premises)) + ' examples')\n",
    "\n",
    "    def _pad(self, lines):\n",
    "        return np.array([d2l.truncate_pad(\n",
    "            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n",
    "                         for line in lines])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class SNLIDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"用于加载SNLI数据集的自定义数据集\"\"\"\n",
    "    def __init__(self, dataset, num_steps, vocab=None):\n",
    "        self.num_steps = num_steps\n",
    "        all_premise_tokens = d2l.tokenize(dataset[0])\n",
    "        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n",
    "        if vocab is None:\n",
    "            self.vocab = d2l.Vocab(all_premise_tokens + \\\n",
    "                all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        self.premises = self._pad(all_premise_tokens)\n",
    "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        print('read ' + str(len(self.premises)) + ' examples')\n",
    "\n",
    "    def _pad(self, lines):\n",
    "        return torch.tensor([d2l.truncate_pad(\n",
    "            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n",
    "                         for line in lines])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "class SNLIDataset(paddle.io.Dataset):\n",
    "    \"\"\"用于加载SNLI数据集的自定义数据集\"\"\"\n",
    "    def __init__(self, dataset, num_steps, vocab=None):\n",
    "        self.num_steps = num_steps\n",
    "        all_premise_tokens = d2l.tokenize(dataset[0])\n",
    "        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n",
    "        if vocab is None:\n",
    "            self.vocab = d2l.Vocab(all_premise_tokens + \\\n",
    "                all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        self.premises = self._pad(all_premise_tokens)\n",
    "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
    "        self.labels = paddle.to_tensor(dataset[2])\n",
    "        print('read ' + str(len(self.premises)) + ' examples')\n",
    "\n",
    "    def _pad(self, lines):\n",
    "        return paddle.to_tensor([d2l.truncate_pad(\n",
    "            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n",
    "                         for line in lines])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.premises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2159f5",
   "metadata": {},
   "source": [
    "### [**整合代码**]\n",
    "\n",
    "现在，我们可以调用`read_snli`函数和`SNLIDataset`类来下载SNLI数据集，并返回训练集和测试集的`DataLoader`实例，以及训练集的词表。值得注意的是，我们必须使用从训练集构造的词表作为测试集的词表。因此，在训练集中训练的模型将不知道来自测试集的任何新词元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bcb8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_snli(batch_size, num_steps=50):\n",
    "    \"\"\"下载SNLI数据集并返回数据迭代器和词表\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    data_dir = d2l.download_extract('SNLI')\n",
    "    train_data = read_snli(data_dir, True)\n",
    "    test_data = read_snli(data_dir, False)\n",
    "    train_set = SNLIDataset(train_data, num_steps)\n",
    "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,\n",
    "                                       num_workers=num_workers)\n",
    "    test_iter = gluon.data.DataLoader(test_set, batch_size, shuffle=False,\n",
    "                                      num_workers=num_workers)\n",
    "    return train_iter, test_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601eb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def load_data_snli(batch_size, num_steps=50):\n",
    "    \"\"\"下载SNLI数据集并返回数据迭代器和词表\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    data_dir = d2l.download_extract('SNLI')\n",
    "    train_data = read_snli(data_dir, True)\n",
    "    test_data = read_snli(data_dir, False)\n",
    "    train_set = SNLIDataset(train_data, num_steps)\n",
    "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "    return train_iter, test_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae09ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def load_data_snli(batch_size, num_steps=50):\n",
    "    \"\"\"下载SNLI数据集并返回数据迭代器和词表\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    data_dir = d2l.download_extract('SNLI')\n",
    "    train_data = read_snli(data_dir, True)\n",
    "    test_data = read_snli(data_dir, False)\n",
    "    train_set = SNLIDataset(train_data, num_steps)\n",
    "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    train_iter = paddle.io.DataLoader(train_set,batch_size=batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=num_workers,\n",
    "                                      return_list=True)\n",
    "                                             \n",
    "    test_iter = paddle.io.DataLoader(test_set, batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=num_workers,\n",
    "                                     return_list=True)\n",
    "    return train_iter, test_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e758857b",
   "metadata": {},
   "source": [
    "在这里，我们将批量大小设置为128时，将序列长度设置为50，并调用`load_data_snli`函数来获取数据迭代器和词表。然后我们打印词表大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1adf1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "train_iter, test_iter, vocab = load_data_snli(128, 50)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d40cf",
   "metadata": {},
   "source": [
    "现在我们打印第一个小批量的形状。与情感分析相反，我们有分别代表前提和假设的两个输入`X[0]`和`X[1]`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fe186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "for X, Y in train_iter:\n",
    "    print(X[0].shape)\n",
    "    print(X[1].shape)\n",
    "    print(Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165ec54",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 自然语言推断研究“假设”是否可以从“前提”推断出来，其中两者都是文本序列。\n",
    "* 在自然语言推断中，前提和假设之间的关系包括蕴涵关系、矛盾关系和中性关系。\n",
    "* 斯坦福自然语言推断（SNLI）语料库是一个比较流行的自然语言推断基准数据集。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 机器翻译长期以来一直是基于翻译输出和翻译真实值之间的表面$n$元语法匹配来进行评估的。可以设计一种用自然语言推断来评价机器翻译结果的方法吗？\n",
    "1. 我们如何更改超参数以减小词表大小？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/5721)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/5722)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11828)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbca92a",
   "metadata": {},
   "source": [
    "# 自然语言推断：使用注意力\n",
    ":label:`sec_natural-language-inference-attention`\n",
    "\n",
    "我们在 :numref:`sec_natural-language-inference-and-dataset`中介绍了自然语言推断任务和SNLI数据集。鉴于许多模型都是基于复杂而深度的架构，Parikh等人提出用注意力机制解决自然语言推断问题，并称之为“可分解注意力模型” :cite:`Parikh.Tackstrom.Das.ea.2016`。这使得模型没有循环层或卷积层，在SNLI数据集上以更少的参数实现了当时的最佳结果。本节将描述并实现这种基于注意力的自然语言推断方法（使用MLP），如 :numref:`fig_nlp-map-nli-attention`中所述。\n",
    "\n",
    "![将预训练GloVe送入基于注意力和MLP的自然语言推断架构](../img/nlp-map-nli-attention.svg)\n",
    ":label:`fig_nlp-map-nli-attention`\n",
    "\n",
    "## 模型\n",
    "\n",
    "与保留前提和假设中词元的顺序相比，我们可以将一个文本序列中的词元与另一个文本序列中的每个词元对齐，然后比较和聚合这些信息，以预测前提和假设之间的逻辑关系。与机器翻译中源句和目标句之间的词元对齐类似，前提和假设之间的词元对齐可以通过注意力机制灵活地完成。\n",
    "\n",
    "![利用注意力机制进行自然语言推断](../img/nli-attention.svg)\n",
    ":label:`fig_nli_attention`\n",
    "\n",
    " :numref:`fig_nli_attention`描述了使用注意力机制的自然语言推断方法。从高层次上讲，它由三个联合训练的步骤组成：对齐、比较和汇总。我们将在下面一步一步地对它们进行说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c711296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dee4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df80a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import paddle\n",
    "from paddle import nn\n",
    "from paddle.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19081359",
   "metadata": {},
   "source": [
    "### 注意（Attending）\n",
    "\n",
    "第一步是将一个文本序列中的词元与另一个序列中的每个词元对齐。假设前提是“我确实需要睡眠”，假设是“我累了”。由于语义上的相似性，我们不妨将假设中的“我”与前提中的“我”对齐，将假设中的“累”与前提中的“睡眠”对齐。同样，我们可能希望将前提中的“我”与假设中的“我”对齐，将前提中的“需要”和“睡眠”与假设中的“累”对齐。请注意，这种对齐是使用加权平均的“软”对齐，其中理想情况下较大的权重与要对齐的词元相关联。为了便于演示， :numref:`fig_nli_attention`以“硬”对齐的方式显示了这种对齐方式。\n",
    "\n",
    "现在，我们更详细地描述使用注意力机制的软对齐。用$\\mathbf{A} = (\\mathbf{a}_1, \\ldots, \\mathbf{a}_m)$和$\\mathbf{B} = (\\mathbf{b}_1, \\ldots, \\mathbf{b}_n)$表示前提和假设，其词元数量分别为$m$和$n$，其中$\\mathbf{a}_i, \\mathbf{b}_j \\in \\mathbb{R}^{d}$（$i = 1, \\ldots, m, j = 1, \\ldots, n$）是$d$维的词向量。对于软对齐，我们将注意力权重$e_{ij} \\in \\mathbb{R}$计算为：\n",
    "\n",
    "$$e_{ij} = f(\\mathbf{a}_i)^\\top f(\\mathbf{b}_j),$$\n",
    ":eqlabel:`eq_nli_e`\n",
    "\n",
    "其中函数$f$是在下面的`mlp`函数中定义的多层感知机。输出维度$f$由`mlp`的`num_hiddens`参数指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d946fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(num_hiddens, flatten):\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dropout(0.2))\n",
    "    net.add(nn.Dense(num_hiddens, activation='relu', flatten=flatten))\n",
    "    net.add(nn.Dropout(0.2))\n",
    "    net.add(nn.Dense(num_hiddens, activation='relu', flatten=flatten))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe68133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def mlp(num_inputs, num_hiddens, flatten):\n",
    "    net = []\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(num_inputs, num_hiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(num_hiddens, num_hiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_dim=1))\n",
    "    return nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def mlp(num_inputs, num_hiddens, flatten):\n",
    "    net = []\n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(num_inputs, num_hiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_axis=1)) \n",
    "    net.append(nn.Dropout(0.2))\n",
    "    net.append(nn.Linear(num_hiddens, num_hiddens))\n",
    "    net.append(nn.ReLU())\n",
    "    if flatten:\n",
    "        net.append(nn.Flatten(start_axis=1))\n",
    "    return nn.Sequential(*net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e22798a",
   "metadata": {},
   "source": [
    "值得注意的是，在 :eqref:`eq_nli_e`中，$f$分别输入$\\mathbf{a}_i$和$\\mathbf{b}_j$，而不是将它们一对放在一起作为输入。这种*分解*技巧导致$f$只有$m + n$个次计算（线性复杂度），而不是$mn$次计算（二次复杂度）\n",
    "\n",
    "对 :eqref:`eq_nli_e`中的注意力权重进行规范化，我们计算假设中所有词元向量的加权平均值，以获得假设的表示，该假设与前提中索引$i$的词元进行软对齐：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta}_i = \\sum_{j=1}^{n}\\frac{\\exp(e_{ij})}{ \\sum_{k=1}^{n} \\exp(e_{ik})} \\mathbf{b}_j.\n",
    "$$\n",
    "\n",
    "同样，我们计算假设中索引为$j$的每个词元与前提词元的软对齐：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\alpha}_j = \\sum_{i=1}^{m}\\frac{\\exp(e_{ij})}{ \\sum_{k=1}^{m} \\exp(e_{kj})} \\mathbf{a}_i.\n",
    "$$\n",
    "\n",
    "下面，我们定义`Attend`类来计算假设（`beta`）与输入前提`A`的软对齐以及前提（`alpha`）与输入假设`B`的软对齐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attend(nn.Block):\n",
    "    def __init__(self, num_hiddens, **kwargs):\n",
    "        super(Attend, self).__init__(**kwargs)\n",
    "        self.f = mlp(num_hiddens=num_hiddens, flatten=False)\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        # A/B的形状：（批量大小，序列A/B的词元数，embed_size）\n",
    "        # f_A/f_B的形状：（批量大小，序列A/B的词元数，num_hiddens）\n",
    "        f_A = self.f(A)\n",
    "        f_B = self.f(B)\n",
    "        # e的形状：（批量大小，序列A的词元数，序列B的词元数）\n",
    "        e = npx.batch_dot(f_A, f_B, transpose_b=True)\n",
    "        # beta的形状：（批量大小，序列A的词元数，embed_size），\n",
    "        # 意味着序列B被软对齐到序列A的每个词元(beta的第1个维度)\n",
    "        beta = npx.batch_dot(npx.softmax(e), B)\n",
    "        # alpha的形状：（批量大小，序列B的词元数，embed_size），\n",
    "        # 意味着序列A被软对齐到序列B的每个词元(alpha的第1个维度)\n",
    "        alpha = npx.batch_dot(npx.softmax(e.transpose(0, 2, 1)), A)\n",
    "        return beta, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8668b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class Attend(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
    "        super(Attend, self).__init__(**kwargs)\n",
    "        self.f = mlp(num_inputs, num_hiddens, flatten=False)\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        # A/B的形状：（批量大小，序列A/B的词元数，embed_size）\n",
    "        # f_A/f_B的形状：（批量大小，序列A/B的词元数，num_hiddens）\n",
    "        f_A = self.f(A)\n",
    "        f_B = self.f(B)\n",
    "        # e的形状：（批量大小，序列A的词元数，序列B的词元数）\n",
    "        e = torch.bmm(f_A, f_B.permute(0, 2, 1))\n",
    "        # beta的形状：（批量大小，序列A的词元数，embed_size），\n",
    "        # 意味着序列B被软对齐到序列A的每个词元(beta的第1个维度)\n",
    "        beta = torch.bmm(F.softmax(e, dim=-1), B)\n",
    "        # beta的形状：（批量大小，序列B的词元数，embed_size），\n",
    "        # 意味着序列A被软对齐到序列B的每个词元(alpha的第1个维度)\n",
    "        alpha = torch.bmm(F.softmax(e.permute(0, 2, 1), dim=-1), A)\n",
    "        return beta, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3bde4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class Attend(nn.Layer):\n",
    "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
    "        super(Attend, self).__init__(**kwargs)\n",
    "        self.f = mlp(num_inputs, num_hiddens, flatten=False)\n",
    "\n",
    "    def forward(self, A, B):\n",
    "        # A/B的形状：（批量大小，序列A/B的词元数，embed_size）\n",
    "        # f_A/f_B的形状：（批量大小，序列A/B的词元数，num_hiddens）\n",
    "        f_A = self.f(A)\n",
    "        f_B = self.f(B)\n",
    "        # e的形状：（批量大小，序列A的词元数，序列B的词元数）\n",
    "        e = paddle.bmm(f_A, f_B.transpose([0, 2, 1]))\n",
    "        # beta的形状：（批量大小，序列A的词元数，embed_size），\n",
    "        # 意味着序列B被软对齐到序列A的每个词元(beta的第1个维度)\n",
    "        beta = paddle.bmm(F.softmax(e, axis=-1), B)\n",
    "        # beta的形状：（批量大小，序列B的词元数，embed_size），\n",
    "        # 意味着序列A被软对齐到序列B的每个词元(alpha的第1个维度)\n",
    "        alpha = paddle.bmm(F.softmax(e.transpose([0, 2, 1]), axis=-1), A)\n",
    "        return beta, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58429e17",
   "metadata": {},
   "source": [
    "### 比较\n",
    "\n",
    "在下一步中，我们将一个序列中的词元与与该词元软对齐的另一个序列进行比较。请注意，在软对齐中，一个序列中的所有词元（尽管可能具有不同的注意力权重）将与另一个序列中的词元进行比较。为便于演示， :numref:`fig_nli_attention`对词元以*硬*的方式对齐。例如，上述的*注意*（attending）步骤确定前提中的“need”和“sleep”都与假设中的“tired”对齐，则将对“疲倦-需要睡眠”进行比较。\n",
    "\n",
    "在比较步骤中，我们将来自一个序列的词元的连结（运算符$[\\cdot, \\cdot]$）和来自另一序列的对齐的词元送入函数$g$（一个多层感知机）：\n",
    "\n",
    "$$\\mathbf{v}_{A,i} = g([\\mathbf{a}_i, \\boldsymbol{\\beta}_i]), i = 1, \\ldots, m\\\\ \\mathbf{v}_{B,j} = g([\\mathbf{b}_j, \\boldsymbol{\\alpha}_j]), j = 1, \\ldots, n.$$\n",
    "\n",
    ":eqlabel:`eq_nli_v_ab`\n",
    "\n",
    "在 :eqref:`eq_nli_v_ab`中，$\\mathbf{v}_{A,i}$是指，所有假设中的词元与前提中词元$i$软对齐，再与词元$i$的比较；而$\\mathbf{v}_{B,j}$是指，所有前提中的词元与假设中词元$i$软对齐，再与词元$i$的比较。下面的`Compare`个类定义了比较步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compare(nn.Block):\n",
    "    def __init__(self, num_hiddens, **kwargs):\n",
    "        super(Compare, self).__init__(**kwargs)\n",
    "        self.g = mlp(num_hiddens=num_hiddens, flatten=False)\n",
    "\n",
    "    def forward(self, A, B, beta, alpha):\n",
    "        V_A = self.g(np.concatenate([A, beta], axis=2))\n",
    "        V_B = self.g(np.concatenate([B, alpha], axis=2))\n",
    "        return V_A, V_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a488e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class Compare(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
    "        super(Compare, self).__init__(**kwargs)\n",
    "        self.g = mlp(num_inputs, num_hiddens, flatten=False)\n",
    "\n",
    "    def forward(self, A, B, beta, alpha):\n",
    "        V_A = self.g(torch.cat([A, beta], dim=2))\n",
    "        V_B = self.g(torch.cat([B, alpha], dim=2))\n",
    "        return V_A, V_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class Compare(nn.Layer):\n",
    "    def __init__(self, num_inputs, num_hiddens, **kwargs):\n",
    "        super(Compare, self).__init__(**kwargs)\n",
    "        self.g = mlp(num_inputs, num_hiddens, flatten=False)\n",
    "\n",
    "    def forward(self, A, B, beta, alpha):\n",
    "        V_A = self.g(paddle.concat([A, beta], axis=2))\n",
    "        V_B = self.g(paddle.concat([B, alpha], axis=2))\n",
    "        return V_A, V_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6058bf2",
   "metadata": {},
   "source": [
    "### 聚合\n",
    "\n",
    "现在我们有两组比较向量$\\mathbf{v}_{A,i}$（$i = 1, \\ldots, m$）和$\\mathbf{v}_{B,j}$（$j = 1, \\ldots, n$）。在最后一步中，我们将聚合这些信息以推断逻辑关系。我们首先求和这两组比较向量：\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_A = \\sum_{i=1}^{m} \\mathbf{v}_{A,i}, \\quad \\mathbf{v}_B = \\sum_{j=1}^{n}\\mathbf{v}_{B,j}.\n",
    "$$\n",
    "\n",
    "接下来，我们将两个求和结果的连结提供给函数$h$（一个多层感知机），以获得逻辑关系的分类结果：\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = h([\\mathbf{v}_A, \\mathbf{v}_B]).\n",
    "$$\n",
    "\n",
    "聚合步骤在以下`Aggregate`类中定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc2d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Aggregate(nn.Block):\n",
    "    def __init__(self, num_hiddens, num_outputs, **kwargs):\n",
    "        super(Aggregate, self).__init__(**kwargs)\n",
    "        self.h = mlp(num_hiddens=num_hiddens, flatten=True)\n",
    "        self.h.add(nn.Dense(num_outputs))\n",
    "\n",
    "    def forward(self, V_A, V_B):\n",
    "        # 对两组比较向量分别求和\n",
    "        V_A = V_A.sum(axis=1)\n",
    "        V_B = V_B.sum(axis=1)\n",
    "        # 将两个求和结果的连结送到多层感知机中\n",
    "        Y_hat = self.h(np.concatenate([V_A, V_B], axis=1))\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f366ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class Aggregate(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):\n",
    "        super(Aggregate, self).__init__(**kwargs)\n",
    "        self.h = mlp(num_inputs, num_hiddens, flatten=True)\n",
    "        self.linear = nn.Linear(num_hiddens, num_outputs)\n",
    "\n",
    "    def forward(self, V_A, V_B):\n",
    "        # 对两组比较向量分别求和\n",
    "        V_A = V_A.sum(dim=1)\n",
    "        V_B = V_B.sum(dim=1)\n",
    "        # 将两个求和结果的连结送到多层感知机中\n",
    "        Y_hat = self.linear(self.h(torch.cat([V_A, V_B], dim=1)))\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class Aggregate(nn.Layer):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_outputs, **kwargs):\n",
    "        super(Aggregate, self).__init__(**kwargs)\n",
    "        self.h = mlp(num_inputs, num_hiddens, flatten=True)\n",
    "        self.linear = nn.Linear(num_hiddens, num_outputs)\n",
    "\n",
    "    def forward(self, V_A, V_B):\n",
    "        # 对两组比较向量分别求和\n",
    "        V_A = V_A.sum(axis=1)\n",
    "        V_B = V_B.sum(axis=1)\n",
    "        # 将两个求和结果的连结送到多层感知机中\n",
    "        Y_hat = self.linear(self.h(paddle.concat([V_A, V_B], axis=1)))\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4fd707",
   "metadata": {},
   "source": [
    "### 整合代码\n",
    "\n",
    "通过将注意步骤、比较步骤和聚合步骤组合在一起，我们定义了可分解注意力模型来联合训练这三个步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7188db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposableAttention(nn.Block):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, **kwargs):\n",
    "        super(DecomposableAttention, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.attend = Attend(num_hiddens)\n",
    "        self.compare = Compare(num_hiddens)\n",
    "        # 有3种可能的输出：蕴涵、矛盾和中性\n",
    "        self.aggregate = Aggregate(num_hiddens, 3)\n",
    "\n",
    "    def forward(self, X):\n",
    "        premises, hypotheses = X\n",
    "        A = self.embedding(premises)\n",
    "        B = self.embedding(hypotheses)\n",
    "        beta, alpha = self.attend(A, B)\n",
    "        V_A, V_B = self.compare(A, B, beta, alpha)\n",
    "        Y_hat = self.aggregate(V_A, V_B)\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class DecomposableAttention(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_inputs_attend=100,\n",
    "                 num_inputs_compare=200, num_inputs_agg=400, **kwargs):\n",
    "        super(DecomposableAttention, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.attend = Attend(num_inputs_attend, num_hiddens)\n",
    "        self.compare = Compare(num_inputs_compare, num_hiddens)\n",
    "        # 有3种可能的输出：蕴涵、矛盾和中性\n",
    "        self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)\n",
    "\n",
    "    def forward(self, X):\n",
    "        premises, hypotheses = X\n",
    "        A = self.embedding(premises)\n",
    "        B = self.embedding(hypotheses)\n",
    "        beta, alpha = self.attend(A, B)\n",
    "        V_A, V_B = self.compare(A, B, beta, alpha)\n",
    "        Y_hat = self.aggregate(V_A, V_B)\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f667b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class DecomposableAttention(nn.Layer):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_inputs_attend=100,\n",
    "                 num_inputs_compare=200, num_inputs_agg=400, **kwargs):\n",
    "        super(DecomposableAttention, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.attend = Attend(num_inputs_attend, num_hiddens)\n",
    "        self.compare = Compare(num_inputs_compare, num_hiddens)\n",
    "        # 有3种可能的输出：蕴涵、矛盾和中性\n",
    "        self.aggregate = Aggregate(num_inputs_agg, num_hiddens, num_outputs=3)\n",
    "\n",
    "    def forward(self, X):\n",
    "        premises, hypotheses = X\n",
    "        A = self.embedding(premises)\n",
    "        B = self.embedding(hypotheses)\n",
    "        beta, alpha = self.attend(A, B)\n",
    "        V_A, V_B = self.compare(A, B, beta, alpha)\n",
    "        Y_hat = self.aggregate(V_A, V_B)\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ac811b",
   "metadata": {},
   "source": [
    "## 训练和评估模型\n",
    "\n",
    "现在，我们将在SNLI数据集上对定义好的可分解注意力模型进行训练和评估。我们从读取数据集开始。\n",
    "\n",
    "### 读取数据集\n",
    "\n",
    "我们使用 :numref:`sec_natural-language-inference-and-dataset`中定义的函数下载并读取SNLI数据集。批量大小和序列长度分别设置为$256$和$50$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f1872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "batch_size, num_steps = 256, 50\n",
    "train_iter, test_iter, vocab = d2l.load_data_snli(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebcdd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def load_data_snli(batch_size, num_steps=50):\n",
    "    \"\"\"下载SNLI数据集并返回数据迭代器和词表\n",
    "\n",
    "    Defined in :numref:`sec_natural-language-inference-and-dataset`\"\"\"\n",
    "    data_dir = d2l.download_extract('SNLI')\n",
    "    train_data = d2l.read_snli(data_dir, True)\n",
    "    test_data = d2l.read_snli(data_dir, False)\n",
    "    train_set = d2l.SNLIDataset(train_data, num_steps)\n",
    "    test_set = d2l.SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    train_iter = paddle.io.DataLoader(train_set,batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=0,\n",
    "                                             return_list=True)\n",
    "\n",
    "    test_iter = paddle.io.DataLoader(test_set, batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=0,\n",
    "                                            return_list=True)\n",
    "    return train_iter, test_iter, train_set.vocab\n",
    "\n",
    "batch_size, num_steps = 256, 50\n",
    "train_iter, test_iter, vocab = load_data_snli(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b3ab2",
   "metadata": {},
   "source": [
    "### 创建模型\n",
    "\n",
    "我们使用预训练好的100维GloVe嵌入来表示输入词元。我们将向量$\\mathbf{a}_i$和$\\mathbf{b}_j$在 :eqref:`eq_nli_e`中的维数预定义为100。 :eqref:`eq_nli_e`中的函数$f$和 :eqref:`eq_nli_v_ab`中的函数$g$的输出维度被设置为200.然后我们创建一个模型实例，初始化它的参数，并加载GloVe嵌入来初始化输入词元的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cae3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()\n",
    "net = DecomposableAttention(vocab, embed_size, num_hiddens)\n",
    "net.initialize(init.Xavier(), ctx=devices)\n",
    "glove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "net.embedding.weight.set_data(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "embed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()\n",
    "net = DecomposableAttention(vocab, embed_size, num_hiddens)\n",
    "glove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "net.embedding.weight.data.copy_(embeds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "embed_size, num_hiddens, devices = 100, 200, d2l.try_all_gpus()\n",
    "net = DecomposableAttention(vocab, embed_size, num_hiddens)\n",
    "glove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "net.embedding.weight.set_value(embeds);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8c40c",
   "metadata": {},
   "source": [
    "### 训练和评估模型\n",
    "\n",
    "与 :numref:`sec_multi_gpu`中接受单一输入（如文本序列或图像）的`split_batch`函数不同，我们定义了一个`split_batch_multi_inputs`函数以小批量接受多个输入，如前提和假设。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def split_batch_multi_inputs(X, y, devices):\n",
    "    \"\"\"将多输入'X'和'y'拆分到多个设备\"\"\"\n",
    "    X = list(zip(*[gluon.utils.split_and_load(\n",
    "        feature, devices, even_split=False) for feature in X]))\n",
    "    return (X, gluon.utils.split_and_load(y, devices, even_split=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317dabc0",
   "metadata": {},
   "source": [
    "现在我们可以在SNLI数据集上训练和评估模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af6e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.001, 4\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, \n",
    "    devices, split_batch_multi_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e748b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "lr, num_epochs = 0.001, 4\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, \n",
    "    devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9442ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "lr, num_epochs = 0.001, 4\n",
    "trainer = paddle.optimizer.Adam(learning_rate=lr, parameters=net.parameters())\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "    devices[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae86cc7",
   "metadata": {},
   "source": [
    "### 使用模型\n",
    "\n",
    "最后，定义预测函数，输出一对前提和假设之间的逻辑关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc831b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def predict_snli(net, vocab, premise, hypothesis):\n",
    "    \"\"\"预测前提和假设之间的逻辑关系\"\"\"\n",
    "    premise = np.array(vocab[premise], ctx=d2l.try_gpu())\n",
    "    hypothesis = np.array(vocab[hypothesis], ctx=d2l.try_gpu())\n",
    "    label = np.argmax(net([premise.reshape((1, -1)),\n",
    "                           hypothesis.reshape((1, -1))]), axis=1)\n",
    "    return 'entailment' if label == 0 else 'contradiction' if label == 1 \\\n",
    "            else 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27077df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def predict_snli(net, vocab, premise, hypothesis):\n",
    "    \"\"\"预测前提和假设之间的逻辑关系\"\"\"\n",
    "    net.eval()\n",
    "    premise = torch.tensor(vocab[premise], device=d2l.try_gpu())\n",
    "    hypothesis = torch.tensor(vocab[hypothesis], device=d2l.try_gpu())\n",
    "    label = torch.argmax(net([premise.reshape((1, -1)),\n",
    "                           hypothesis.reshape((1, -1))]), dim=1)\n",
    "    return 'entailment' if label == 0 else 'contradiction' if label == 1 \\\n",
    "            else 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77266c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def predict_snli(net, vocab, premise, hypothesis):\n",
    "    \"\"\"预测前提和假设之间的逻辑关系\"\"\"\n",
    "    net.eval()\n",
    "    premise = paddle.to_tensor(vocab[premise], place=d2l.try_gpu())\n",
    "    hypothesis = paddle.to_tensor(vocab[hypothesis], place=d2l.try_gpu())\n",
    "    label = paddle.argmax(net([premise.reshape((1, -1)),\n",
    "                           hypothesis.reshape((1, -1))]), axis=1)\n",
    "                           \n",
    "    return 'entailment' if label == 0 else 'contradiction' if label == 1 \\\n",
    "            else 'neutral'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df44cf21",
   "metadata": {},
   "source": [
    "我们可以使用训练好的模型来获得对示例句子的自然语言推断结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7977fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "predict_snli(net, vocab, ['he', 'is', 'good', '.'], ['he', 'is', 'bad', '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe5fe1",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 可分解注意模型包括三个步骤来预测前提和假设之间的逻辑关系：注意、比较和聚合。\n",
    "* 通过注意力机制，我们可以将一个文本序列中的词元与另一个文本序列中的每个词元对齐，反之亦然。这种对齐是使用加权平均的软对齐，其中理想情况下较大的权重与要对齐的词元相关联。\n",
    "* 在计算注意力权重时，分解技巧会带来比二次复杂度更理想的线性复杂度。\n",
    "* 我们可以使用预训练好的词向量作为下游自然语言处理任务（如自然语言推断）的输入表示。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 使用其他超参数组合训练模型，能在测试集上获得更高的准确度吗？\n",
    "1. 自然语言推断的可分解注意模型的主要缺点是什么？\n",
    "1. 假设我们想要获得任何一对句子的语义相似级别（例如，0～1之间的连续值）。我们应该如何收集和标注数据集？请尝试设计一个有注意力机制的模型。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/5727)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/5728)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11829)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba5a1b",
   "metadata": {},
   "source": [
    "# 针对序列级和词元级应用微调BERT\n",
    ":label:`sec_finetuning-bert`\n",
    "\n",
    "在本章的前几节中，我们为自然语言处理应用设计了不同的模型，例如基于循环神经网络、卷积神经网络、注意力和多层感知机。这些模型在有空间或时间限制的情况下是有帮助的，但是，为每个自然语言处理任务精心设计一个特定的模型实际上是不可行的。在 :numref:`sec_bert`中，我们介绍了一个名为BERT的预训练模型，该模型可以对广泛的自然语言处理任务进行最少的架构更改。一方面，在提出时，BERT改进了各种自然语言处理任务的技术水平。另一方面，正如在 :numref:`sec_bert-pretraining`中指出的那样，原始BERT模型的两个版本分别带有1.1亿和3.4亿个参数。因此，当有足够的计算资源时，我们可以考虑为下游自然语言处理应用微调BERT。\n",
    "\n",
    "下面，我们将自然语言处理应用的子集概括为序列级和词元级。在序列层次上，介绍了在单文本分类任务和文本对分类（或回归）任务中，如何将文本输入的BERT表示转换为输出标签。在词元级别，我们将简要介绍新的应用，如文本标注和问答，并说明BERT如何表示它们的输入并转换为输出标签。在微调期间，不同应用之间的BERT所需的“最小架构更改”是额外的全连接层。在下游应用的监督学习期间，额外层的参数是从零开始学习的，而预训练BERT模型中的所有参数都是微调的。\n",
    "\n",
    "## 单文本分类\n",
    "\n",
    "*单文本分类*将单个文本序列作为输入，并输出其分类结果。\n",
    "除了我们在这一章中探讨的情感分析之外，语言可接受性语料库（Corpus of Linguistic Acceptability，COLA）也是一个单文本分类的数据集，它的要求判断给定的句子在语法上是否可以接受。 :cite:`Warstadt.Singh.Bowman.2019`。例如，“I should study.”是可以接受的，但是“I should studying.”不是可以接受的。\n",
    "\n",
    "![微调BERT用于单文本分类应用，如情感分析和测试语言可接受性（这里假设输入的单个文本有六个词元）](../img/bert-one-seq.svg)\n",
    ":label:`fig_bert-one-seq`\n",
    "\n",
    " :numref:`sec_bert`描述了BERT的输入表示。BERT输入序列明确地表示单个文本和文本对，其中特殊分类标记“&lt;cls&gt;”用于序列分类，而特殊分类标记“&lt;sep&gt;”标记单个文本的结束或分隔成对文本。如 :numref:`fig_bert-one-seq`所示，在单文本分类应用中，特殊分类标记“&lt;cls&gt;”的BERT表示对整个输入文本序列的信息进行编码。作为输入单个文本的表示，它将被送入到由全连接（稠密）层组成的小多层感知机中，以输出所有离散标签值的分布。\n",
    "\n",
    "## 文本对分类或回归\n",
    "\n",
    "在本章中，我们还研究了自然语言推断。它属于*文本对分类*，这是一种对文本进行分类的应用类型。\n",
    "\n",
    "以一对文本作为输入但输出连续值，*语义文本相似度*是一个流行的“文本对回归”任务。\n",
    "这项任务评估句子的语义相似度。例如，在语义文本相似度基准数据集（Semantic Textual Similarity Benchmark）中，句子对的相似度得分是从0（无语义重叠）到5（语义等价）的分数区间 :cite:`Cer.Diab.Agirre.ea.2017`。我们的目标是预测这些分数。来自语义文本相似性基准数据集的样本包括（句子1，句子2，相似性得分）：\n",
    "\n",
    "* \"A plane is taking off.\"（“一架飞机正在起飞。”），\"An air plane is taking off.\"（“一架飞机正在起飞。”），5.000分;\n",
    "* \"A woman is eating something.\"（“一个女人在吃东西。”），\"A woman is eating meat.\"（“一个女人在吃肉。”），3.000分;\n",
    "* \"A woman is dancing.\"（一个女人在跳舞。），\"A man is talking.\"（“一个人在说话。”），0.000分。\n",
    "\n",
    "![文本对分类或回归应用的BERT微调，如自然语言推断和语义文本相似性（假设输入文本对分别有两个词元和三个词元）](../img/bert-two-seqs.svg)\n",
    ":label:`fig_bert-two-seqs`\n",
    "\n",
    "与 :numref:`fig_bert-one-seq`中的单文本分类相比， :numref:`fig_bert-two-seqs`中的文本对分类的BERT微调在输入表示上有所不同。对于文本对回归任务（如语义文本相似性），可以应用细微的更改，例如输出连续的标签值和使用均方损失：它们在回归中很常见。\n",
    "\n",
    "## 文本标注\n",
    "\n",
    "现在让我们考虑词元级任务，比如*文本标注*（text tagging），其中每个词元都被分配了一个标签。在文本标注任务中，*词性标注*为每个单词分配词性标记（例如，形容词和限定词）。\n",
    "根据单词在句子中的作用。如，在Penn树库II标注集中，句子“John Smith‘s car is new”应该被标记为“NNP（名词，专有单数）NNP POS（所有格结尾）NN（名词，单数或质量）VB（动词，基本形式）JJ（形容词）”。\n",
    "\n",
    "![文本标记应用的BERT微调，如词性标记。假设输入的单个文本有六个词元。](../img/bert-tagging.svg)\n",
    ":label:`fig_bert-tagging`\n",
    "\n",
    " :numref:`fig_bert-tagging`中说明了文本标记应用的BERT微调。与 :numref:`fig_bert-one-seq`相比，唯一的区别在于，在文本标注中，输入文本的*每个词元*的BERT表示被送到相同的额外全连接层中，以输出词元的标签，例如词性标签。\n",
    "\n",
    "## 问答\n",
    "\n",
    "作为另一个词元级应用，*问答*反映阅读理解能力。\n",
    "例如，斯坦福问答数据集（Stanford Question Answering Dataset，SQuAD v1.1）由阅读段落和问题组成，其中每个问题的答案只是段落中的一段文本（文本片段） :cite:`Rajpurkar.Zhang.Lopyrev.ea.2016`。举个例子，考虑一段话：“Some experts report that a mask's efficacy is inconclusive.However,mask makers insist that their products,such as N95 respirator masks,can guard against the virus.”（“一些专家报告说面罩的功效是不确定的。然而，口罩制造商坚持他们的产品，如N95口罩，可以预防病毒。”）还有一个问题“Who say that N95 respirator masks can guard against the virus?”（“谁说N95口罩可以预防病毒？”）。答案应该是文章中的文本片段“mask makers”（“口罩制造商”）。因此，SQuAD v1.1的目标是在给定问题和段落的情况下预测段落中文本片段的开始和结束。\n",
    "\n",
    "![对问答进行BERT微调（假设输入文本对分别有两个和三个词元）](../img/bert-qa.svg)\n",
    ":label:`fig_bert-qa`\n",
    "\n",
    "为了微调BERT进行问答，在BERT的输入中，将问题和段落分别作为第一个和第二个文本序列。为了预测文本片段开始的位置，相同的额外的全连接层将把来自位置$i$的任何词元的BERT表示转换成标量分数$s_i$。文章中所有词元的分数还通过softmax转换成概率分布，从而为文章中的每个词元位置$i$分配作为文本片段开始的概率$p_i$。预测文本片段的结束与上面相同，只是其额外的全连接层中的参数与用于预测开始位置的参数无关。当预测结束时，位置$i$的词元由相同的全连接层变换成标量分数$e_i$。 :numref:`fig_bert-qa`描述了用于问答的微调BERT。\n",
    "\n",
    "对于问答，监督学习的训练目标就像最大化真实值的开始和结束位置的对数似然一样简单。当预测片段时，我们可以计算从位置$i$到位置$j$的有效片段的分数$s_i + e_j$（$i \\leq j$），并输出分数最高的跨度。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 对于序列级和词元级自然语言处理应用，BERT只需要最小的架构改变（额外的全连接层），如单个文本分类（例如，情感分析和测试语言可接受性）、文本对分类或回归（例如，自然语言推断和语义文本相似性）、文本标记（例如，词性标记）和问答。\n",
    "* 在下游应用的监督学习期间，额外层的参数是从零开始学习的，而预训练BERT模型中的所有参数都是微调的。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 让我们为新闻文章设计一个搜索引擎算法。当系统接收到查询（例如，“冠状病毒爆发期间的石油行业”）时，它应该返回与该查询最相关的新闻文章的排序列表。假设我们有一个巨大的新闻文章池和大量的查询。为了简化问题，假设为每个查询标记了最相关的文章。如何在算法设计中应用负采样（见 :numref:`subsec_negative-sampling`）和BERT？\n",
    "1. 我们如何利用BERT来训练语言模型？\n",
    "1. 我们能在机器翻译中利用BERT吗？\n",
    "\n",
    "[Discussions](https://discuss.d2l.ai/t/5729)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b72557",
   "metadata": {},
   "source": [
    "# 自然语言推断：微调BERT\n",
    ":label:`sec_natural-language-inference-bert`\n",
    "\n",
    "在本章的前面几节中，我们已经为SNLI数据集（ :numref:`sec_natural-language-inference-and-dataset`）上的自然语言推断任务设计了一个基于注意力的结构（ :numref:`sec_natural-language-inference-attention`）。现在，我们通过微调BERT来重新审视这项任务。正如在 :numref:`sec_finetuning-bert`中讨论的那样，自然语言推断是一个序列级别的文本对分类问题，而微调BERT只需要一个额外的基于多层感知机的架构，如 :numref:`fig_nlp-map-nli-bert`中所示。\n",
    "\n",
    "![将预训练BERT提供给基于多层感知机的自然语言推断架构](../img/nlp-map-nli-bert.svg)\n",
    ":label:`fig_nlp-map-nli-bert`\n",
    "\n",
    "本节将下载一个预训练好的小版本的BERT，然后对其进行微调，以便在SNLI数据集上进行自然语言推断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac1074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import mxnet as d2l\n",
    "import json\n",
    "import multiprocessing\n",
    "from mxnet import gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "import os\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab38dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import json\n",
    "import multiprocessing\n",
    "import torch\n",
    "from torch import nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c19ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "from d2l import paddle as d2l\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import json\n",
    "import multiprocessing\n",
    "import paddle\n",
    "from paddle import nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ede7b",
   "metadata": {},
   "source": [
    "## [**加载预训练的BERT**]\n",
    "\n",
    "我们已经在 :numref:`sec_bert-dataset`和 :numref:`sec_bert-pretraining`WikiText-2数据集上预训练BERT（请注意，原始的BERT模型是在更大的语料库上预训练的）。正如在 :numref:`sec_bert-pretraining`中所讨论的，原始的BERT模型有数以亿计的参数。在下面，我们提供了两个版本的预训练的BERT：“bert.base”与原始的BERT基础模型一样大，需要大量的计算资源才能进行微调，而“bert.small”是一个小版本，以便于演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.zip',\n",
    "                             '7b3820b35da691042e5d34c0971ac3edbd80d3f4')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.zip',\n",
    "                              'a4e718a47137ccd1809c9107ab4f5edd317bae2c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6300c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "d2l.DATA_HUB['bert_small'] = ('https://paddlenlp.bj.bcebos.com/models/bert.small.paddle.zip', '9fcde07509c7e87ec61c640c1b277509c7e87ec6153d9041758e4')\n",
    "\n",
    "d2l.DATA_HUB['bert_base'] = ('https://paddlenlp.bj.bcebos.com/models/bert.base.paddle.zip', '9fcde07509c7e87ec61c640c1b27509c7e87ec61753d9041758e4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d55343",
   "metadata": {},
   "source": [
    "两个预训练好的BERT模型都包含一个定义词表的“vocab.json”文件和一个预训练参数的“pretrained.params”文件。我们实现了以下`load_pretrained_model`函数来[**加载预先训练好的BERT参数**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ff966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
    "                          num_heads, num_layers, dropout, max_len, devices):\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    # 定义空词表以加载预定义词表\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir,\n",
    "         'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n",
    "        vocab.idx_to_token)}\n",
    "    bert = d2l.BERTModel(len(vocab), num_hiddens, ffn_num_hiddens, \n",
    "                         num_heads, num_layers, dropout, max_len)\n",
    "    # 加载预训练BERT参数\n",
    "    bert.load_parameters(os.path.join(data_dir, 'pretrained.params'),\n",
    "                         ctx=devices)\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
    "                          num_heads, num_layers, dropout, max_len, devices):\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    # 定义空词表以加载预定义词表\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir, \n",
    "        'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n",
    "        vocab.idx_to_token)}\n",
    "    bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256],\n",
    "                         ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,\n",
    "                         num_heads=4, num_layers=2, dropout=0.2,\n",
    "                         max_len=max_len, key_size=256, query_size=256,\n",
    "                         value_size=256, hid_in_features=256,\n",
    "                         mlm_in_features=256, nsp_in_features=256)\n",
    "    # 加载预训练BERT参数\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir,\n",
    "                                                 'pretrained.params')))\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
    "                          num_heads, num_layers, dropout, max_len, devices):\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    # 定义空词表以加载预定义词表\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir,\n",
    "        'vocab.json')))\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n",
    "        vocab.idx_to_token)}\n",
    "    bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256],\n",
    "                         ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,\n",
    "                         num_heads=4, num_layers=2, dropout=0.2,\n",
    "                         max_len=max_len, key_size=256, query_size=256,\n",
    "                         value_size=256, hid_in_features=256,\n",
    "                         mlm_in_features=256, nsp_in_features=256)\n",
    "    # 加载预训练BERT参数\n",
    "    bert.set_state_dict(paddle.load(os.path.join(data_dir,\n",
    "                                                 'pretrained.pdparams')))\n",
    "\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a230ea",
   "metadata": {},
   "source": [
    "为了便于在大多数机器上演示，我们将在本节中加载和微调经过预训练BERT的小版本（“bert.small”）。在练习中，我们将展示如何微调大得多的“bert.base”以显著提高测试精度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet, pytorch\n",
    "devices = d2l.try_all_gpus()\n",
    "bert, vocab = load_pretrained_model(\n",
    "    'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n",
    "    num_layers=2, dropout=0.1, max_len=512, devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40963e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "devices = d2l.try_all_gpus()\n",
    "bert, vocab = load_pretrained_model(\n",
    "    'bert_small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n",
    "    num_layers=2, dropout=0.1, max_len=512, devices=devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1030c4",
   "metadata": {},
   "source": [
    "## [**微调BERT的数据集**]\n",
    "\n",
    "对于SNLI数据集的下游任务自然语言推断，我们定义了一个定制的数据集类`SNLIBERTDataset`。在每个样本中，前提和假设形成一对文本序列，并被打包成一个BERT输入序列，如 :numref:`fig_bert-two-seqs`所示。回想 :numref:`subsec_bert_input_rep`，片段索引用于区分BERT输入序列中的前提和假设。利用预定义的BERT输入序列的最大长度（`max_len`），持续移除输入文本对中较长文本的最后一个标记，直到满足`max_len`。为了加速生成用于微调BERT的SNLI数据集，我们使用4个工作进程并行生成训练或测试样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba0fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIBERTDataset(gluon.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [[\n",
    "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "        \n",
    "        self.labels = np.array(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        pool = multiprocessing.Pool(4)  # 使用4个进程\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        all_token_ids = [\n",
    "            token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (np.array(all_token_ids, dtype='int32'),\n",
    "                np.array(all_segments, dtype='int32'), \n",
    "                np.array(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                             * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # 为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b0cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [[\n",
    "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "        \n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        pool = multiprocessing.Pool(4)  # 使用4个进程\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        all_token_ids = [\n",
    "            token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long), \n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                             * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # 为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class SNLIBERTDataset(paddle.io.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [[\n",
    "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "\n",
    "        self.labels = paddle.to_tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        # pool = multiprocessing.Pool(1)  # 使用4个进程\n",
    "        # out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        out = []\n",
    "        for i in all_premise_hypothesis_tokens:\n",
    "            tempOut = self._mp_worker(i)\n",
    "            out.append(tempOut)\n",
    "        \n",
    "        all_token_ids = [\n",
    "            token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (paddle.to_tensor(all_token_ids, dtype='int64'),\n",
    "                paddle.to_tensor(all_segments, dtype='int64'),\n",
    "                paddle.to_tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                             * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # 为BERT输入中的'<CLS>'、'<SEP>'和'<SEP>'词元保留位置\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ee283",
   "metadata": {},
   "source": [
    "下载完SNLI数据集后，我们通过实例化`SNLIBERTDataset`类来[**生成训练和测试样本**]。这些样本将在自然语言推断的训练和测试期间进行小批量读取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512\n",
    "batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n",
    "data_dir = d2l.download_extract('SNLI')\n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,\n",
    "                                   num_workers=num_workers)\n",
    "test_iter = gluon.data.DataLoader(test_set, batch_size,\n",
    "                                  num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545afccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "# 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512\n",
    "batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n",
    "data_dir = d2l.download_extract('SNLI')\n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n",
    "                                   num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                  num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce73e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "# 如果出现显存不足错误，请减少“batch_size”。在原始的BERT模型中，max_len=512\n",
    "batch_size, max_len, num_workers = 512, 128, d2l.get_dataloader_workers()\n",
    "data_dir = d2l.download_extract('SNLI')\n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "train_iter = paddle.io.DataLoader(train_set, batch_size=batch_size, shuffle=True, return_list=True)\n",
    "test_iter = paddle.io.DataLoader(test_set, batch_size=batch_size, return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316c7a7",
   "metadata": {},
   "source": [
    "## 微调BERT\n",
    "\n",
    "如 :numref:`fig_bert-two-seqs`所示，用于自然语言推断的微调BERT只需要一个额外的多层感知机，该多层感知机由两个全连接层组成（请参见下面`BERTClassifier`类中的`self.hidden`和`self.output`）。[**这个多层感知机将特殊的“&lt;cls&gt;”词元**]的BERT表示进行了转换，该词元同时编码前提和假设的信息(**为自然语言推断的三个输出**)：蕴涵、矛盾和中性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8672ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Block):\n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.Dense(3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef876968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class BERTClassifier(nn.Layer):\n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x.squeeze(1))\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f27bd",
   "metadata": {},
   "source": [
    "在下文中，预训练的BERT模型`bert`被送到用于下游应用的`BERTClassifier`实例`net`中。在BERT微调的常见实现中，只有额外的多层感知机（`net.output`）的输出层的参数将从零开始学习。预训练BERT编码器（`net.encoder`）和额外的多层感知机的隐藏层（`net.hidden`）的所有参数都将进行微调。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BERTClassifier(bert)\n",
    "net.output.initialize(ctx=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13442bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "net = BERTClassifier(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6ebed",
   "metadata": {},
   "source": [
    "回想一下，在 :numref:`sec_bert`中，`MaskLM`类和`NextSentencePred`类在其使用的多层感知机中都有一些参数。这些参数是预训练BERT模型`bert`中参数的一部分，因此是`net`中的参数的一部分。然而，这些参数仅用于计算预训练过程中的遮蔽语言模型损失和下一句预测损失。这两个损失函数与微调下游应用无关，因此当BERT微调时，`MaskLM`和`NextSentencePred`中采用的多层感知机的参数不会更新（陈旧的，staled）。\n",
    "\n",
    "为了允许具有陈旧梯度的参数，标志`ignore_stale_grad=True`在`step`函数`d2l.train_batch_ch13`中被设置。我们通过该函数使用SNLI的训练集（`train_iter`）和测试集（`test_iter`）对`net`模型进行训练和评估。由于计算资源有限，[**训练**]和测试精度可以进一步提高：我们把对它的讨论留在练习中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d025113",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 1e-4, 5\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, \n",
    "    devices, d2l.split_batch_multi_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "lr, num_epochs = 1e-4, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, \n",
    "    devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedde23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "lr, num_epochs = 1e-4, 5\n",
    "trainer = paddle.optimizer.Adam(learning_rate=lr, parameters=net.parameters())\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "    devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a09cda",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们可以针对下游应用对预训练的BERT模型进行微调，例如在SNLI数据集上进行自然语言推断。\n",
    "* 在微调过程中，BERT模型成为下游应用模型的一部分。仅与训练前损失相关的参数在微调期间不会更新。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果您的计算资源允许，请微调一个更大的预训练BERT模型，该模型与原始的BERT基础模型一样大。修改`load_pretrained_model`函数中的参数设置：将“bert.small”替换为“bert.base”，将`num_hiddens=256`、`ffn_num_hiddens=512`、`num_heads=4`和`num_layers=2`的值分别增加到768、3072、12和12。通过增加微调迭代轮数（可能还会调优其他超参数），你可以获得高于0.86的测试精度吗？\n",
    "1. 如何根据一对序列的长度比值截断它们？将此对截断方法与`SNLIBERTDataset`类中使用的方法进行比较。它们的利弊是什么？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/5715)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/5718)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11831)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

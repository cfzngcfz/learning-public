{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7b2734",
   "metadata": {},
   "source": [
    "# 深度学习计算\n",
    ":label:`chap_computation`\n",
    "\n",
    "除了庞大的数据集和强大的硬件，\n",
    "优秀的软件工具在深度学习的快速发展中发挥了不可或缺的作用。\n",
    "从2007年发布的开创性的Theano库开始，\n",
    "灵活的开源工具使研究人员能够快速开发模型原型，\n",
    "避免了我们使用标准组件时的重复工作，\n",
    "同时仍然保持了我们进行底层修改的能力。\n",
    "随着时间的推移，深度学习库已经演变成提供越来越粗糙的抽象。\n",
    "就像半导体设计师从指定晶体管到逻辑电路再到编写代码一样，\n",
    "神经网络研究人员已经从考虑单个人工神经元的行为转变为从层的角度构思网络，\n",
    "通常在设计架构时考虑的是更粗糙的块（block）。\n",
    "\n",
    "之前我们已经介绍了一些基本的机器学习概念，\n",
    "并慢慢介绍了功能齐全的深度学习模型。\n",
    "在上一章中，我们从零开始实现了多层感知机的每个组件，\n",
    "然后展示了如何利用高级API轻松地实现相同的模型。\n",
    "为了易于学习，我们调用了深度学习库，但是跳过了它们工作的细节。\n",
    "在本章中，我们将深入探索深度学习计算的关键组件，\n",
    "即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘，\n",
    "以及利用GPU实现显著的加速。\n",
    "这些知识将使读者从深度学习“基础用户”变为“高级用户”。\n",
    "虽然本章不介绍任何新的模型或数据集，\n",
    "但后面的高级模型章节在很大程度上依赖于本章的知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8602a1",
   "metadata": {
    "attributes": {
     "classes": [
      "toc"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    ":maxdepth: 2\n",
    "\n",
    "model-construction\n",
    "parameters\n",
    "deferred-init\n",
    "custom-layer\n",
    "read-write\n",
    "use-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc9b28d",
   "metadata": {},
   "source": [
    "# 层和块\n",
    ":label:`sec_model_construction`\n",
    "\n",
    "之前首次介绍神经网络时，我们关注的是具有单一输出的线性模型。\n",
    "在这里，整个模型只有一个输出。\n",
    "注意，单个神经网络\n",
    "（1）接受一些输入；\n",
    "（2）生成相应的标量输出；\n",
    "（3）具有一组相关 *参数*（parameters），更新这些参数可以优化某目标函数。\n",
    "\n",
    "然后，当考虑具有多个输出的网络时，\n",
    "我们利用矢量化算法来描述整层神经元。\n",
    "像单个神经元一样，层（1）接受一组输入，\n",
    "（2）生成相应的输出，\n",
    "（3）由一组可调整参数描述。\n",
    "当我们使用softmax回归时，一个单层本身就是模型。\n",
    "然而，即使我们随后引入了多层感知机，我们仍然可以认为该模型保留了上面所说的基本架构。\n",
    "\n",
    "对于多层感知机而言，整个模型及其组成层都是这种架构。\n",
    "整个模型接受原始输入（特征），生成输出（预测），\n",
    "并包含一些参数（所有组成层的参数集合）。\n",
    "同样，每个单独的层接收输入（由前一层提供），\n",
    "生成输出（到下一层的输入），并且具有一组可调参数，\n",
    "这些参数根据从下一层反向传播的信号进行更新。\n",
    "\n",
    "事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。\n",
    "例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层，\n",
    "这些层是由*层组*（groups of layers）的重复模式组成。\n",
    "这个ResNet架构赢得了2015年ImageNet和COCO计算机视觉比赛\n",
    "的识别和检测任务 :cite:`He.Zhang.Ren.ea.2016`。\n",
    "目前ResNet架构仍然是许多视觉任务的首选架构。\n",
    "在其他的领域，如自然语言处理和语音，\n",
    "层组以各种重复模式排列的类似架构现在也是普遍存在。\n",
    "\n",
    "为了实现这些复杂的网络，我们引入了神经网络*块*的概念。\n",
    "*块*（block）可以描述单个层、由多个层组成的组件或整个模型本身。\n",
    "使用块进行抽象的一个好处是可以将一些块组合成更大的组件，\n",
    "这一过程通常是递归的，如 :numref:`fig_blocks`所示。\n",
    "通过定义代码来按需生成任意复杂度的块，\n",
    "我们可以通过简洁的代码实现复杂的神经网络。\n",
    "\n",
    "![多个层被组合成块，形成更大的模型](../img/blocks.svg)\n",
    ":label:`fig_blocks`\n",
    "\n",
    "从编程的角度来看，块由*类*（class）表示。\n",
    "它的任何子类都必须定义一个将其输入转换为输出的前向传播函数，\n",
    "并且必须存储任何必需的参数。\n",
    "注意，有些块不需要任何参数。\n",
    "最后，为了计算梯度，块必须具有反向传播函数。\n",
    "在定义我们自己的块时，由于自动微分（在 :numref:`sec_autograd` 中引入）\n",
    "提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。\n",
    "\n",
    "在构造自定义块之前，(**我们先回顾一下多层感知机**)\n",
    "（ :numref:`sec_mlp_concise` ）的代码。\n",
    "下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层，\n",
    "然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0278012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()\n",
    "\n",
    "X = np.random.uniform(size=(2, 20))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b95fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4be0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10),\n",
    "])\n",
    "\n",
    "X = tf.random.uniform((2, 20))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import paddle\n",
    "from paddle import nn\n",
    "from paddle.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = paddle.rand([2, 20])\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a41f365",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "在这个例子中，我们通过实例化`nn.Sequential`来构建我们的模型，\n",
    "返回的对象赋给`net`变量。\n",
    "接下来，我们反复调用`net`变量的`add`函数，按照想要执行的顺序添加层。\n",
    "简而言之，`nn.Sequential`定义了一种特殊类型的`Block`，\n",
    "即在Gluon中表示块的类，它维护`Block`的有序列表。\n",
    "`add`函数方便将每个连续的`Block`添加到列表中。\n",
    "请注意，每层都是`Dense`类的一个实例，`Dense`类本身就是`Block`的子类。\n",
    "到目前为止，我们一直在通过`net(X)`调用我们的模型来获得模型的输出。\n",
    "这实际上是`net.forward(X)`的简写，\n",
    "这是通过`Block`类的`__call__`函数实现的一个Python技巧。\n",
    "前向传播（`forward`）函数非常简单：它将列表中的每个`Block`连接在一起，\n",
    "将每个`Block`的输出作为输入传递给下一层。\n",
    "\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "在这个例子中，我们通过实例化`nn.Sequential`来构建我们的模型，\n",
    "层的执行顺序是作为参数传递的。\n",
    "简而言之，(**`nn.Sequential`定义了一种特殊的`Module`**)，\n",
    "即在PyTorch中表示一个块的类，\n",
    "它维护了一个由`Module`组成的有序列表。\n",
    "注意，两个全连接层都是`Linear`类的实例，\n",
    "`Linear`类本身就是`Module`的子类。\n",
    "另外，到目前为止，我们一直在通过`net(X)`调用我们的模型来获得模型的输出。\n",
    "这实际上是`net.__call__(X)`的简写。\n",
    "这个前向传播函数非常简单：\n",
    "它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。\n",
    "\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "在这个例子中，我们通过实例化`keras.models.Sequential`来构建我们的模型，\n",
    "层的执行顺序是作为参数传递的。\n",
    "简而言之，`Sequential`定义了一种特殊的`keras.Model`，\n",
    "即在Keras中表示一个块的类。\n",
    "它维护了一个由`Model`组成的有序列表，\n",
    "注意两个全连接层都是`Model`类的实例，\n",
    "这个类本身就是`Model`的子类。\n",
    "前向传播（`call`）函数也非常简单：\n",
    "它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。\n",
    "注意，到目前为止，我们一直在通过`net(X)`调用我们的模型来获得模型的输出。\n",
    "这实际上是`net.call(X)`的简写，\n",
    "这是通过Block类的`__call__`函数实现的一个Python技巧。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "在这个例子中，我们通过实例化`nn.Sequential`来构建我们的模型，\n",
    "层的执行顺序是作为参数传递的。\n",
    "简而言之，(**`nn.Sequential`定义了一种特殊的`Layer`**)，\n",
    "即在PaddlePaddle中表示一个块的类，\n",
    "它维护了一个由`Layer`组成的有序列表。\n",
    "注意，两个全连接层都是`Linear`类的实例，\n",
    "`Linear`类本身就是`Layer`的子类。\n",
    "另外，到目前为止，我们一直在通过`net(X)`调用我们的模型来获得模型的输出。\n",
    "这实际上是`net.__call__(X)`的简写。\n",
    "这个前向传播函数非常简单：\n",
    "它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。\n",
    ":end_tab:\n",
    "\n",
    "## [**自定义块**]\n",
    "\n",
    "要想直观地了解块是如何工作的，最简单的方法就是自己实现一个。\n",
    "在实现我们自定义块之前，我们简要总结一下每个块必须提供的基本功能。\n",
    "\n",
    ":begin_tab:`mxnet, tensorflow`\n",
    "1. 将输入数据作为其前向传播函数的参数。\n",
    "1. 通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收任意维的输入，但是返回一个维度256的输出。\n",
    "1. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\n",
    "1. 存储和访问前向传播计算所需的参数。\n",
    "1. 根据需要初始化模型参数。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch, paddle`\n",
    "1. 将输入数据作为其前向传播函数的参数。\n",
    "1. 通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。\n",
    "1. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\n",
    "1. 存储和访问前向传播计算所需的参数。\n",
    "1. 根据需要初始化模型参数。\n",
    ":end_tab:\n",
    "\n",
    "\n",
    "在下面的代码片段中，我们从零开始编写一个块。\n",
    "它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。\n",
    "注意，下面的`MLP`类继承了表示块的类。\n",
    "我们的实现只需要提供我们自己的构造函数（Python中的`__init__`函数）和前向传播函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Block):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self, **kwargs):\n",
    "        # 调用MLP的父类Block的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = nn.Dense(256, activation='relu')  # 隐藏层\n",
    "        self.out = nn.Dense(10)  # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        return self.out(self.hidden(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20542a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "class MLP(tf.keras.Model):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Model的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        # Hiddenlayer\n",
    "        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(units=10)  # Outputlayer\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def call(self, X):\n",
    "        return self.out(self.hidden((X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class MLP(nn.Layer):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用`MLP`的父类Layer的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数`params`（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的正向传播，即如何根据输入`X`返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f5c2a",
   "metadata": {},
   "source": [
    "我们首先看一下前向传播函数，它以`X`作为输入，\n",
    "计算带有激活函数的隐藏表示，并输出其未规范化的输出值。\n",
    "在这个`MLP`实现中，两个层都是实例变量。\n",
    "要了解这为什么是合理的，可以想象实例化两个多层感知机（`net1`和`net2`），\n",
    "并根据不同的数据对它们进行训练。\n",
    "当然，我们希望它们学到两种不同的模型。\n",
    "\n",
    "接着我们[**实例化多层感知机的层，然后在每次调用前向传播函数时调用这些层**]。\n",
    "注意一些关键细节：\n",
    "首先，我们定制的`__init__`函数通过`super().__init__()`\n",
    "调用父类的`__init__`函数，\n",
    "省去了重复编写模版代码的痛苦。\n",
    "然后，我们实例化两个全连接层，\n",
    "分别为`self.hidden`和`self.out`。\n",
    "注意，除非我们实现一个新的运算符，\n",
    "否则我们不必担心反向传播函数或参数初始化，\n",
    "系统将自动生成这些。\n",
    "\n",
    "我们来试一下这个函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe3932",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP()\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e72825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694378cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a5a5e",
   "metadata": {},
   "source": [
    "块的一个主要优点是它的多功能性。\n",
    "我们可以子类化块以创建层（如全连接层的类）、\n",
    "整个模型（如上面的`MLP`类）或具有中等复杂度的各种组件。\n",
    "我们在接下来的章节中充分利用了这种多功能性，\n",
    "比如在处理卷积神经网络时。\n",
    "\n",
    "## [**顺序块**]\n",
    "\n",
    "现在我们可以更仔细地看看`Sequential`类是如何工作的，\n",
    "回想一下`Sequential`的设计是为了把其他模块串起来。\n",
    "为了构建我们自己的简化的`MySequential`，\n",
    "我们只需要定义两个关键函数：\n",
    "\n",
    "1. 一种将块逐个追加到列表中的函数；\n",
    "1. 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。\n",
    "\n",
    "下面的`MySequential`类提供了与默认`Sequential`类相同的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665e5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Block):\n",
    "    def add(self, block):\n",
    "    # 这里，block是Block子类的一个实例，我们假设它有一个唯一的名称。我们把它\n",
    "    # 保存在'Block'类的成员变量_children中。block的类型是OrderedDict。\n",
    "    # 当MySequential实例调用initialize函数时，系统会自动初始化_children\n",
    "    # 的所有成员\n",
    "        self._children[block.name] = block\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._children.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbaa0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "class MySequential(tf.keras.Model):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.modules = []\n",
    "        for block in args:\n",
    "            # 这里，block是tf.keras.layers.Layer子类的一个实例\n",
    "            self.modules.append(block)\n",
    "\n",
    "    def call(self, X):\n",
    "        for module in self.modules:\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d94cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class MySequential(nn.Layer):\n",
    "    def __init__(self, *layers):\n",
    "        super(MySequential, self).__init__()\n",
    "        # 如果传入的是一个tuple\n",
    "        if len(layers) > 0 and isinstance(layers[0], tuple): \n",
    "            for name, layer in layers:\n",
    "                # add_sublayer方法会将layer添加到self._sub_layers(一个tuple)\n",
    "                self.add_sublayer(name, layer)  \n",
    "        else:\n",
    "            for idx, layer in enumerate(layers):\n",
    "                self.add_sublayer(str(idx), layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for layer in self._sub_layers.values():\n",
    "            X = layer(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65286c04",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "`add`函数向有序字典`_children`添加一个块。\n",
    "读者可能会好奇为什么每个Gluon中的`Block`都有一个`_children`属性？\n",
    "以及为什么我们使用它而不是自己定义一个Python列表？\n",
    "简而言之，`_children`的主要优点是：\n",
    "在块的参数初始化过程中，\n",
    "Gluon知道在`_children`字典中查找需要初始化参数的子块。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "`__init__`函数将每个模块逐个添加到有序字典`_modules`中。\n",
    "读者可能会好奇为什么每个`Module`都有一个`_modules`属性？\n",
    "以及为什么我们使用它而不是自己定义一个Python列表？\n",
    "简而言之，`_modules`的主要优点是：\n",
    "在模块的参数初始化过程中，\n",
    "系统知道在`_modules`字典中查找需要初始化参数的子块。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "`__init__`函数将每个模块逐个添加到有序字典`_sub_layers`中。\n",
    "你可能会好奇为什么每个`Layer`都有一个`_sub_layers`属性？\n",
    "以及为什么我们使用它而不是自己定义一个Python列表？\n",
    "简而言之，`_sub_layers`的主要优点是：\n",
    "在模块的参数初始化过程中，\n",
    "系统知道在`_sub_layers`字典中查找需要初始化参数的子块。\n",
    ":end_tab:\n",
    "\n",
    "当`MySequential`的前向传播函数被调用时，\n",
    "每个添加的块都按照它们被添加的顺序执行。\n",
    "现在可以使用我们的`MySequential`类重新实现多层感知机。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d101bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MySequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b729bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7684ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "net = MySequential(\n",
    "    tf.keras.layers.Dense(units=256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c478cc",
   "metadata": {},
   "source": [
    "请注意，`MySequential`的用法与之前为`Sequential`类编写的代码相同\n",
    "（如 :numref:`sec_mlp_concise` 中所述）。\n",
    "\n",
    "## [**在前向传播函数中执行代码**]\n",
    "\n",
    "`Sequential`类使模型构造变得简单，\n",
    "允许我们组合新的架构，而不必定义自己的类。\n",
    "然而，并不是所有的架构都是简单的顺序架构。\n",
    "当需要更强的灵活性时，我们需要定义自己的块。\n",
    "例如，我们可能希望在前向传播函数中执行Python的控制流。\n",
    "此外，我们可能希望执行任意的数学运算，\n",
    "而不是简单地依赖预定义的神经网络层。\n",
    "\n",
    "到目前为止，\n",
    "我们网络中的所有操作都对网络的激活值及网络的参数起作用。\n",
    "然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项，\n",
    "我们称之为*常数参数*（constant parameter）。\n",
    "例如，我们需要一个计算函数\n",
    "$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$的层，\n",
    "其中$\\mathbf{x}$是输入，\n",
    "$\\mathbf{w}$是参数，\n",
    "$c$是某个在优化过程中没有更新的指定常量。\n",
    "因此我们实现了一个`FixedHiddenMLP`类，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a258212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 使用get_constant函数创建的随机权重参数在训练期间不会更新（即为常量参数）\n",
    "        self.rand_weight = self.params.get_constant(\n",
    "            'rand_weight', np.random.uniform(size=(20, 20)))\n",
    "        self.dense = nn.Dense(20, activation='relu')\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense(X)\n",
    "        # 使用创建的常量参数以及relu和dot函数\n",
    "        X = npx.relu(np.dot(X, self.rand_weight.data()) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.dense(X)\n",
    "        # 控制流\n",
    "        while np.abs(X).sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd008f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "class FixedHiddenMLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # 使用tf.constant函数创建的随机权重参数在训练期间不会更新（即为常量参数）\n",
    "        self.rand_weight = tf.constant(tf.random.uniform((20, 20)))\n",
    "        self.dense = tf.keras.layers.Dense(20, activation=tf.nn.relu)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = self.flatten(inputs)\n",
    "        # 使用创建的常量参数以及relu和matmul函数\n",
    "        X = tf.nn.relu(tf.matmul(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数。\n",
    "        X = self.dense(X)\n",
    "        # 控制流\n",
    "        while tf.reduce_sum(tf.math.abs(X)) > 1:\n",
    "            X /= 2\n",
    "        return tf.reduce_sum(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b763af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class FixedHiddenMLP(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变。\n",
    "        self.rand_weight = paddle.rand([20, 20])\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数。\n",
    "        X = F.relu(paddle.tensor.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数。\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a770b3e",
   "metadata": {},
   "source": [
    "在这个`FixedHiddenMLP`模型中，我们实现了一个隐藏层，\n",
    "其权重（`self.rand_weight`）在实例化时被随机初始化，之后为常量。\n",
    "这个权重不是一个模型参数，因此它永远不会被反向传播更新。\n",
    "然后，神经网络将这个固定层的输出通过一个全连接层。\n",
    "\n",
    "注意，在返回输出之前，模型做了一些不寻常的事情：\n",
    "它运行了一个while循环，在$L_1$范数大于$1$的条件下，\n",
    "将输出向量除以$2$，直到它满足条件为止。\n",
    "最后，模型返回了`X`中所有项的和。\n",
    "注意，此操作可能不会常用于在任何实际任务中，\n",
    "我们只展示如何将任意代码集成到神经网络计算的流程中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ebfcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57da805",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, tensorflow, paddle\n",
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca462e7",
   "metadata": {},
   "source": [
    "我们可以[**混合搭配各种组合块的方法**]。\n",
    "在下面的例子中，我们以一些想到的方法嵌套块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c18b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "        self.net.add(nn.Dense(64, activation='relu'),\n",
    "                     nn.Dense(32, activation='relu'))\n",
    "        self.dense = nn.Dense(16, activation='relu')\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential()\n",
    "chimera.add(NestMLP(), nn.Dense(20), FixedHiddenMLP())\n",
    "chimera.initialize()\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be98233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "class NestMLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential()\n",
    "        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
    "        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))\n",
    "        self.dense = tf.keras.layers.Dense(16, activation=tf.nn.relu)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(self.net(inputs))\n",
    "\n",
    "chimera = tf.keras.Sequential()\n",
    "chimera.add(NestMLP())\n",
    "chimera.add(tf.keras.layers.Dense(20))\n",
    "chimera.add(FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class NestMLP(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4ae3d",
   "metadata": {},
   "source": [
    "## 效率\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "读者可能会开始担心操作效率的问题。\n",
    "毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、\n",
    "代码执行和许多其他的Python代码。\n",
    "Python的问题[全局解释器锁](https://wiki.python.org/moin/GlobalInterpreterLock)\n",
    "是众所周知的。\n",
    "在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。\n",
    "\n",
    "提高Python速度的最好方法是完全避免使用Python。\n",
    "Gluon这样做的一个方法是允许*混合式编程*（hybridization），这将在后面描述。\n",
    "Python解释器在第一次调用块时执行它。\n",
    "Gluon运行时记录正在发生的事情，以及下一次它将对Python调用加速。\n",
    "在某些情况下，这可以大大加快运行速度，\n",
    "但当控制流（如上所述）在不同的网络通路上引导不同的分支时，需要格外小心。\n",
    "我们建议感兴趣的读者在读完本章后，阅读混合式编程部分（ :numref:`sec_hybridize` ）来了解编译。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "读者可能会开始担心操作效率的问题。\n",
    "毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、\n",
    "代码执行和许多其他的Python代码。\n",
    "Python的问题[全局解释器锁](https://wiki.python.org/moin/GlobalInterpreterLock)\n",
    "是众所周知的。\n",
    "在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "读者可能会开始担心操作效率的问题。\n",
    "毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、\n",
    "代码执行和许多其他的Python代码。\n",
    "Python的问题[全局解释器锁](https://wiki.python.org/moin/GlobalInterpreterLock)\n",
    "是众所周知的。\n",
    "在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "你可能会开始担心操作效率的问题。\n",
    "毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、\n",
    "代码执行和许多其他的Python代码。\n",
    "Python的问题[全局解释器锁](https://wiki.python.org/moin/GlobalInterpreterLock)\n",
    "是众所周知的。\n",
    "在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。\n",
    ":end_tab:\n",
    "\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 一个块可以由许多层组成；一个块可以由许多块组成。\n",
    "* 块可以包含代码。\n",
    "* 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "* 层和块的顺序连接由`Sequential`块处理。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果将`MySequential`中存储块的方式更改为Python列表，会出现什么样的问题？\n",
    "1. 实现一个块，它以两个块为参数，例如`net1`和`net2`，并返回前向传播中两个网络的串联输出。这也被称为平行块。\n",
    "1. 假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/1828)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1827)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1826)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11777)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc5fe9b",
   "metadata": {},
   "source": [
    "# 参数管理\n",
    "\n",
    "在选择了架构并设置了超参数后，我们就进入了训练阶段。\n",
    "此时，我们的目标是找到使损失函数最小化的模型参数值。\n",
    "经过训练后，我们将需要使用这些参数来做出未来的预测。\n",
    "此外，有时我们希望提取参数，以便在其他环境中复用它们，\n",
    "将模型保存下来，以便它可以在其他软件中执行，\n",
    "或者为了获得科学的理解而进行检查。\n",
    "\n",
    "之前的介绍中，我们只依靠深度学习框架来完成训练的工作，\n",
    "而忽略了操作参数的具体细节。\n",
    "本节，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化；\n",
    "* 参数初始化；\n",
    "* 在不同模型组件间共享参数。\n",
    "\n",
    "(**我们首先看一下具有单隐藏层的多层感知机。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(8, activation='relu'))\n",
    "net.add(nn.Dense(1))\n",
    "net.initialize()  # 使用默认初始化方法\n",
    "\n",
    "X = np.random.uniform(size=(2, 4))\n",
    "net(X)  # 正向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ffcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5266a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "X = tf.random.uniform((2, 4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e065b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import paddle\n",
    "from paddle import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))\n",
    "X = paddle.rand([2, 4])\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a4691a",
   "metadata": {},
   "source": [
    "## [**参数访问**]\n",
    "\n",
    "我们从已有模型中访问参数。\n",
    "当通过`Sequential`类定义模型时，\n",
    "我们可以通过索引来访问模型的任意层。\n",
    "这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "如下所示，我们可以检查第二个全连接层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5473b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net[1].params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "print(net.layers[2].weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d84ed",
   "metadata": {},
   "source": [
    "输出的结果告诉我们一些重要的事情：\n",
    "首先，这个全连接层包含两个参数，分别是该层的权重和偏置。\n",
    "两者都存储为单精度浮点数（float32）。\n",
    "注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。\n",
    "\n",
    "### [**目标参数**]\n",
    "\n",
    "注意，每个参数都表示为参数类的一个实例。\n",
    "要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。\n",
    "下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，\n",
    "提取后返回的是一个参数类实例，并进一步访问该参数的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ce770",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(net[1].bias))\n",
    "print(net[1].bias)\n",
    "print(net[1].bias.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a77a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90adf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "print(type(net.layers[2].weights[1]))\n",
    "print(net.layers[2].weights[1])\n",
    "print(tf.convert_to_tensor(net.layers[2].weights[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6923f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b48a9a",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet,pytorch,paddle`\n",
    "参数是复合的对象，包含值、梯度和额外信息。\n",
    "这就是我们需要显式参数值的原因。\n",
    "除了值之外，我们还可以访问每个参数的梯度。\n",
    "在上面这个网络中，由于我们还没有调用反向传播，所以参数的梯度处于初始状态。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[1].weight.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf30174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51191d2b",
   "metadata": {},
   "source": [
    "### [**一次性访问所有参数**]\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。\n",
    "当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，\n",
    "因为我们需要递归整个树来提取每个子块的参数。\n",
    "下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c25b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net[0].collect_params())\n",
    "print(net.collect_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c73fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5139b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "print(net.layers[1].weights)\n",
    "print(net.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3316dc",
   "metadata": {},
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591283dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.collect_params()['dense1_bias'].data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b097296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "net.get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8236c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "net.state_dict()['2.bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5df895f",
   "metadata": {},
   "source": [
    "### [**从嵌套块收集参数**]\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。\n",
    "我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45980656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block1():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(32, activation='relu'))\n",
    "    net.add(nn.Dense(16, activation='relu'))\n",
    "    return net\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for _ in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add(block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential()\n",
    "rgnet.add(block2())\n",
    "rgnet.add(nn.Dense(10))\n",
    "rgnet.initialize()\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8e5b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5863c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def block1(name):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4, activation=tf.nn.relu)],\n",
    "        name=name)\n",
    "\n",
    "def block2():\n",
    "    net = tf.keras.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add(block1(name=f'block-{i}'))\n",
    "    return net\n",
    "\n",
    "rgnet = tf.keras.Sequential()\n",
    "rgnet.add(block2())\n",
    "rgnet.add(tf.keras.layers.Dense(1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5818fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), \n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_sublayer(f'block {i}', block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97045375",
   "metadata": {},
   "source": [
    "[**设计了网络后，我们看看它是如何工作的。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec95048",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rgnet.collect_params)\n",
    "print(rgnet.collect_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11ab210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe80a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "print(rgnet.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a3c6c",
   "metadata": {},
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。\n",
    "下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgnet[0][1][0].bias.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d71ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "rgnet.layers[0].layers[1].layers[1].weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "print(rgnet[0].state_dict()['block 0.0.bias'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5577f",
   "metadata": {},
   "source": [
    "## 参数初始化\n",
    "\n",
    "知道了如何访问参数后，现在我们看看如何正确地初始化参数。\n",
    "我们在 :numref:`sec_numerical_stability`中讨论了良好初始化的必要性。\n",
    "深度学习框架提供默认随机初始化，\n",
    "也允许我们创建自定义初始化方法，\n",
    "满足我们通过其他规则实现初始化权重。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "默认情况下，MXNet通过初始化权重参数的方法是\n",
    "从均匀分布$U(-0.07, 0.07)$中随机采样权重，并将偏置参数设置为0。\n",
    "MXNet的`init`模块提供了多种预置初始化方法。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵，\n",
    "这个范围是根据输入和输出维度计算出的。\n",
    "PyTorch的`nn.init`模块提供了多种预置初始化方法。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "默认情况下，Keras会根据一个范围均匀地初始化权重矩阵，\n",
    "这个范围是根据输入和输出维度计算出的。\n",
    "偏置参数设置为0。\n",
    "TensorFlow在根模块和`keras.initializers`模块中提供了各种初始化方法。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "默认情况下，PaddlePaddle会使用Xavier初始化权重矩阵，\n",
    "偏置参数设置为0。\n",
    "PaddlePaddle的`nn.initializer`模块提供了多种预置初始化方法。\n",
    ":end_tab:\n",
    "\n",
    "### [**内置初始化**]\n",
    "\n",
    "让我们首先调用内置的初始化器。\n",
    "下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，\n",
    "且将偏置参数设置为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5f6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里的force_reinit确保参数会被重新初始化，不论之前是否已经被初始化\n",
    "net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae88c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "        bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac9ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        paddle.nn.initializer.Normal(mean=0.0, std=0.01)\n",
    "        paddle.zeros(m.bias)    \n",
    "net.apply(init_normal)\n",
    "net[0].weight[0],net[0].state_dict()['bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f1dc73",
   "metadata": {},
   "source": [
    "我们还可以将所有参数初始化为给定的常数，比如初始化为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(init=init.Constant(1), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db8731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.keras.initializers.Constant(1),\n",
    "        bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8e3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        paddle.nn.initializer.Constant(value = 1)\n",
    "        paddle.zeros(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight[0],net[0].state_dict()['bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85631f7b",
   "metadata": {},
   "source": [
    "我们还可以[**对某些块应用不同的初始化方法**]。\n",
    "例如，下面我们使用Xavier初始化方法初始化第一个神经网络层，\n",
    "然后将第三个神经网络层初始化为常量值42。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].weight.initialize(init=init.Xavier(), force_reinit=True)\n",
    "net[1].initialize(init=init.Constant(42), force_reinit=True)\n",
    "print(net[0].weight.data()[0])\n",
    "print(net[1].weight.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04239be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dbe48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4,\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.keras.initializers.GlorotUniform()),\n",
    "    tf.keras.layers.Dense(\n",
    "        1, kernel_initializer=tf.keras.initializers.Constant(1)),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "print(net.layers[1].weights[0])\n",
    "print(net.layers[2].weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6acdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        paddle.nn.initializer.XavierUniform(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        paddle.nn.initializer.Constant(42)\n",
    "        \n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight[0])\n",
    "print(net[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2242b95",
   "metadata": {},
   "source": [
    "### [**自定义初始化**]\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。\n",
    "在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "在这里，我们定义了`Initializer`类的子类。\n",
    "通常，我们只需要实现`_init_weight`函数，\n",
    "该函数接受张量参数（`data`）并为其分配所需的初始化值。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "同样，我们实现了一个`my_init`函数来应用到`net`。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "在这里，我们定义了一个`Initializer`的子类，\n",
    "并实现了`__call__`函数。\n",
    "该函数返回给定形状和数据类型的所需张量。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "同样，我们实现了一个`my_init`函数来应用到`net`。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyInit(init.Initializer):\n",
    "    def _init_weight(self, name, data):\n",
    "        print('Init', name, data.shape)\n",
    "        data[:] = np.random.uniform(-10, 10, data.shape)\n",
    "        data *= np.abs(data) >= 5\n",
    "\n",
    "net.initialize(MyInit(), force_reinit=True)\n",
    "net[0].weight.data()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c39f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "class MyInit(tf.keras.initializers.Initializer):\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        data=tf.random.uniform(shape, -10, 10, dtype=dtype)\n",
    "        factor=(tf.abs(data) >= 5)\n",
    "        factor=tf.cast(factor, tf.float32)\n",
    "        return data * factor\n",
    "\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4,\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_initializer=MyInit()),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "print(net.layers[1].weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c707de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape) \n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        paddle.nn.initializer.XavierUniform(m.weight, -10, 10)\n",
    "        h = paddle.abs(m.weight) >= 5\n",
    "        h = paddle.to_tensor(h)\n",
    "        m = paddle.to_tensor(m.weight)\n",
    "        m *= h       \n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50097ed",
   "metadata": {},
   "source": [
    "注意，我们始终可以直接设置参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af27900",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].weight.data()[:] += 1\n",
    "net[0].weight.data()[0, 0] = 42\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf12e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0c07e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "net.layers[1].weights[0][:].assign(net.layers[1].weights[0] + 1)\n",
    "net.layers[1].weights[0][0, 0].assign(42)\n",
    "net.layers[1].weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "net[0].weight.set_value(net[0].weight.numpy() + 1)\n",
    "val = net[0].weight.numpy()\n",
    "val[0, 0] = 42\n",
    "net[0].weight.set_value(val)\n",
    "net[0].weight[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6d23a8",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "高级用户请注意：如果要在`autograd`范围内调整参数，\n",
    "则需要使用`set_data`，以避免误导自动微分机制。\n",
    ":end_tab:\n",
    "\n",
    "## [**参数绑定**]\n",
    "\n",
    "有时我们希望在多个层间共享参数：\n",
    "我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc0a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Dense(8, activation='relu')\n",
    "net.add(nn.Dense(8, activation='relu'),\n",
    "        shared,\n",
    "        nn.Dense(8, activation='relu', params=shared.params),\n",
    "        nn.Dense(10))\n",
    "net.initialize()\n",
    "\n",
    "X = np.random.uniform(size=(2, 20))\n",
    "net(X)\n",
    "\n",
    "# 检查参数是否相同\n",
    "print(net[1].weight.data()[0] == net[2].weight.data()[0])\n",
    "net[1].weight.data()[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[1].weight.data()[0] == net[2].weight.data()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a727e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c8e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "# tf.keras的表现有点不同。它会自动删除重复层\n",
    "shared = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    shared,\n",
    "    shared,\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "# 检查参数是否不同\n",
    "print(len(net.layers) == 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45287310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "# 我们需要给共享层一个名称，以便可以引用它的参数。\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight[0] == net[4].weight[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346eb6d6",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "这个例子表明第二层和第三层的参数是绑定的。\n",
    "它们不仅值相等，而且由相同的张量表示。\n",
    "因此，如果我们改变其中一个参数，另一个参数也会改变。\n",
    "这里有一个问题：当参数绑定时，梯度会发生什么情况？\n",
    "答案是由于模型参数包含梯度，\n",
    "因此在反向传播期间第二个隐藏层和第三个隐藏层的梯度会加在一起。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "这个例子表明第三个和第五个神经网络层的参数是绑定的。\n",
    "它们不仅值相等，而且由相同的张量表示。\n",
    "因此，如果我们改变其中一个参数，另一个参数也会改变。\n",
    "这里有一个问题：当参数绑定时，梯度会发生什么情况？\n",
    "答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层\n",
    "（即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。\n",
    ":end_tab:\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 使用 :numref:`sec_model_construction` 中定义的`FancyMLP`模型，访问各个层的参数。\n",
    "1. 查看初始化模块文档以了解不同的初始化方法。\n",
    "1. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "1. 为什么共享参数是个好主意？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/1831)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1829)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1830)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11778)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a23bc0",
   "metadata": {},
   "source": [
    "# 延后初始化\n",
    ":label:`sec_deferred_init`\n",
    "\n",
    "到目前为止，我们忽略了建立网络时需要做的以下这些事情：\n",
    "\n",
    "* 我们定义了网络架构，但没有指定输入维度。\n",
    "* 我们添加层时没有指定前一层的输出维度。\n",
    "* 我们在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。\n",
    "\n",
    "有些读者可能会对我们的代码能运行感到惊讶。\n",
    "毕竟，深度学习框架无法判断网络的输入维度是什么。\n",
    "这里的诀窍是框架的*延后初始化*（defers initialization），\n",
    "即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。\n",
    "\n",
    "在以后，当使用卷积神经网络时，\n",
    "由于输入维度（即图像的分辨率）将影响每个后续层的维数，\n",
    "有了该技术将更加方便。\n",
    "现在我们在编写代码时无须知道维度是什么就可以设置参数，\n",
    "这种能力可以大大简化定义和修改模型的任务。\n",
    "接下来，我们将更深入地研究初始化机制。\n",
    "\n",
    "## 实例化网络\n",
    "\n",
    "首先，让我们实例化一个多层感知机。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112407eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "def get_net():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(256, activation='relu'))\n",
    "    net.add(nn.Dense(10))\n",
    "    return net\n",
    "\n",
    "net = get_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea2846",
   "metadata": {},
   "source": [
    "此时，因为输入维数是未知的，所以网络不可能知道输入层权重的维数。\n",
    "因此，框架尚未初始化任何参数，我们通过尝试访问以下参数进行确认。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net.collect_params)\n",
    "print(net.collect_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23171185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "[net.layers[i].get_weights() for i in range(len(net.layers))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513871bd",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "注意，当参数对象存在时，每个层的输入维度为-1。\n",
    "MXNet使用特殊值-1表示参数维度仍然未知。\n",
    "此时，尝试访问`net[0].weight.data()`将触发运行时错误，\n",
    "提示必须先初始化网络，然后才能访问参数。\n",
    "现在让我们看看当我们试图通过`initialize`函数初始化参数时会发生什么。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "请注意，每个层对象都存在，但权重为空。\n",
    "使用`net.get_weights()`将抛出一个错误，因为权重尚未初始化。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43080911",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize()\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b85255",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "如我们所见，一切都没有改变。\n",
    "当输入维度未知时，调用`initialize`不会真正初始化参数。\n",
    "而是会在MXNet内部声明希望初始化参数，并且可以选择初始化分布。\n",
    ":end_tab:\n",
    "\n",
    "接下来让我们将数据通过网络，最终使框架初始化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a6533",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(size=(2, 20))\n",
    "net(X)\n",
    "\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "X = tf.random.uniform((2, 20))\n",
    "net(X)\n",
    "[w.shape for w in net.get_weights()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e0bce",
   "metadata": {},
   "source": [
    "一旦我们知道输入维数是20，框架可以通过代入值20来识别第一层权重矩阵的形状。\n",
    "识别出第一层的形状后，框架处理第二层，依此类推，直到所有形状都已知为止。\n",
    "注意，在这种情况下，只有第一层需要延迟初始化，但是框架仍是按顺序初始化的。\n",
    "等到知道了所有的参数形状，框架就可以初始化参数。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 延后初始化使框架能够自动推断参数形状，使修改模型架构变得容易，避免了一些常见的错误。\n",
    "* 我们可以通过模型传递数据，使框架最终初始化参数。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果指定了第一层的输入尺寸，但没有指定后续层的尺寸，会发生什么？是否立即进行初始化？\n",
    "1. 如果指定了不匹配的维度会发生什么？\n",
    "1. 如果输入具有不同的维度，需要做什么？提示：查看参数绑定的相关内容。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/5770)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/5770)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1833)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11779)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd6c4c",
   "metadata": {},
   "source": [
    "# 自定义层\n",
    "\n",
    "深度学习成功背后的一个因素是神经网络的灵活性：\n",
    "我们可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。\n",
    "例如，研究人员发明了专门用于处理图像、文本、序列数据和执行动态规划的层。\n",
    "有时我们会遇到或要自己发明一个现在在深度学习框架中还不存在的层。\n",
    "在这些情况下，必须构建自定义层。本节将展示如何构建自定义层。\n",
    "\n",
    "## 不带参数的层\n",
    "\n",
    "首先，我们(**构造一个没有任何参数的自定义层**)。\n",
    "回忆一下在 :numref:`sec_model_construction`对块的介绍，\n",
    "这应该看起来很眼熟。\n",
    "下面的`CenteredLayer`类要从其输入中减去均值。\n",
    "要构建它，我们只需继承基础层类并实现前向传播功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "class CenteredLayer(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "class CenteredLayer(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs - tf.reduce_mean(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import paddle\n",
    "from paddle import nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "class CenteredLayer(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da10833",
   "metadata": {},
   "source": [
    "让我们向该层提供一些数据，验证它是否能按预期工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f6c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(np.array([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f71d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "layer = CenteredLayer()\n",
    "layer(tf.constant([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "layer = CenteredLayer()\n",
    "layer(paddle.to_tensor([1, 2, 3, 4, 5], dtype='float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5506e",
   "metadata": {},
   "source": [
    "现在，我们可以[**将层作为组件合并到更复杂的模型中**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d79ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(128), CenteredLayer())\n",
    "net.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28602416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "net = tf.keras.Sequential([tf.keras.layers.Dense(128), CenteredLayer()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf1c31",
   "metadata": {},
   "source": [
    "作为额外的健全性检查，我们可以在向该网络发送随机数据后，检查均值是否为0。\n",
    "由于我们处理的是浮点数，因为存储精度的原因，我们仍然可能会看到一个非常小的非零数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b3df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = net(np.random.uniform(size=(4, 8)))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73adf66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "Y = net(tf.random.uniform((4, 8)))\n",
    "tf.reduce_mean(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d1304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "Y = net(paddle.rand([4, 8]))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcba8358",
   "metadata": {},
   "source": [
    "## [**带参数的层**]\n",
    "\n",
    "以上我们知道了如何定义简单的层，下面我们继续定义具有参数的层，\n",
    "这些参数可以通过训练进行调整。\n",
    "我们可以使用内置函数来创建参数，这些函数提供一些基本的管理功能。\n",
    "比如管理访问、初始化、共享、保存和加载模型参数。\n",
    "这样做的好处之一是：我们不需要为每个自定义层编写自定义的序列化程序。\n",
    "\n",
    "现在，让我们实现自定义版本的全连接层。\n",
    "回想一下，该层需要两个参数，一个用于表示权重，另一个用于表示偏置项。\n",
    "在此实现中，我们使用修正线性单元作为激活函数。\n",
    "该层需要输入参数：`in_units`和`units`，分别表示输入数和输出数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(nn.Block):\n",
    "    def __init__(self, units, in_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape=(in_units, units))\n",
    "        self.bias = self.params.get('bias', shape=(units,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = np.dot(x, self.weight.data(ctx=x.ctx)) + self.bias.data(\n",
    "            ctx=x.ctx)\n",
    "        return npx.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3cb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "class MyDense(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, X_shape):\n",
    "        self.weight = self.add_weight(name='weight',\n",
    "            shape=[X_shape[-1], self.units],\n",
    "            initializer=tf.random_normal_initializer())\n",
    "        self.bias = self.add_weight(\n",
    "            name='bias', shape=[self.units],\n",
    "            initializer=tf.zeros_initializer())\n",
    "\n",
    "    def call(self, X):\n",
    "        linear = tf.matmul(X, self.weight) + self.bias\n",
    "        return tf.nn.relu(linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b91f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class MyLinear(nn.Layer):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = paddle.create_parameter(shape=(in_units, units), dtype='float32')\n",
    "        self.bias = paddle.create_parameter(shape=(units,), dtype='float32')\n",
    "        \n",
    "    def forward(self, X):\n",
    "        linear = paddle.matmul(X, self.weight) + self.bias\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb33ed4",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet, tensorflow`\n",
    "接下来，我们实例化`MyDense`类并访问其模型参数。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "接下来，我们实例化`MyLinear`类并访问其模型参数。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "接下来，我们实例化`MyLinear`类并访问其模型参数。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc09f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = MyDense(units=3, in_units=5)\n",
    "dense.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c4fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "dense = MyDense(3)\n",
    "dense(tf.random.uniform((2, 5)))\n",
    "dense.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e6e5f",
   "metadata": {},
   "source": [
    "我们可以[**使用自定义层直接执行前向传播计算**]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense.initialize()\n",
    "dense(np.random.uniform(size=(2, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70564467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94294b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "dense(tf.random.uniform((2, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f581ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "linear(paddle.randn([2, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e641a0",
   "metadata": {},
   "source": [
    "我们还可以(**使用自定义层构建模型**)，就像使用内置的全连接层一样使用自定义层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(MyDense(8, in_units=64),\n",
    "        MyDense(1, in_units=8))\n",
    "net.initialize()\n",
    "net(np.random.uniform(size=(2, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28a34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "net = tf.keras.models.Sequential([MyDense(8), MyDense(1)])\n",
    "net(tf.random.uniform((2, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a14098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(paddle.rand([2, 64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc9b9ec",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 我们可以通过基本层类设计自定义层。这允许我们定义灵活的新层，其行为与深度学习框架中的任何现有层不同。\n",
    "* 在自定义层定义完成后，我们就可以在任意环境和网络架构中调用该自定义层。\n",
    "* 层可以有局部参数，这些参数可以通过内置函数创建。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 设计一个接受输入并计算张量降维的层，它返回$y_k = \\sum_{i, j} W_{ijk} x_i x_j$。\n",
    "1. 设计一个返回输入数据的傅立叶系数前半部分的层。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/1837)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1835)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1836)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11780)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6fa62",
   "metadata": {},
   "source": [
    "# 读写文件\n",
    "\n",
    "到目前为止，我们讨论了如何处理数据，\n",
    "以及如何构建、训练和测试深度学习模型。\n",
    "然而，有时我们希望保存训练的模型，\n",
    "以备将来在各种环境中使用（比如在部署中进行预测）。\n",
    "此外，当运行一个耗时较长的训练过程时，\n",
    "最佳的做法是定期保存中间结果，\n",
    "以确保在服务器电源被不小心断掉时，我们不会损失几天的计算结果。\n",
    "因此，现在是时候学习如何加载和存储权重向量和整个模型了。\n",
    "\n",
    "## (**加载和保存张量**)\n",
    "\n",
    "对于单个张量，我们可以直接调用`load`和`save`函数分别读写它们。\n",
    "这两个函数都要求我们提供一个名称，`save`要求将要保存的变量作为输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aeb106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "x = np.arange(4)\n",
    "npx.save('x-file', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x = tf.range(4)\n",
    "np.save('x-file.npy', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c813936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import paddle\n",
    "from paddle import nn \n",
    "from paddle.nn import functional as F\n",
    "\n",
    "x = paddle.arange(4)  \n",
    "paddle.save(x, 'x-file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842394b",
   "metadata": {},
   "source": [
    "我们现在可以将存储在文件中的数据读回内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626c51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = npx.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730790e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "x2 = np.load('x-file.npy', allow_pickle=True)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6cae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x2 = paddle.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425f2dc",
   "metadata": {},
   "source": [
    "我们可以[**存储一个张量列表，然后把它们读回内存。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19bdc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros(4)\n",
    "npx.save('x-files', [x, y])\n",
    "x2, y2 = npx.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87513b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "y = tf.zeros(4)\n",
    "np.save('xy-files.npy', [x, y])\n",
    "x2, y2 = np.load('xy-files.npy', allow_pickle=True)\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "y = paddle.zeros([4])\n",
    "paddle.save([x,y], 'x-file')\n",
    "x2, y2 = paddle.load('x-file')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199efd4f",
   "metadata": {},
   "source": [
    "我们甚至可以(**写入或读取从字符串映射到张量的字典**)。\n",
    "当我们要读取或写入模型中的所有权重时，这很方便。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {'x': x, 'y': y}\n",
    "npx.save('mydict', mydict)\n",
    "mydict2 = npx.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a601480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a555d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "mydict = {'x': x, 'y': y}\n",
    "np.save('mydict.npy', mydict)\n",
    "mydict2 = np.load('mydict.npy', allow_pickle=True)\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "mydict = {'x': x, 'y': y}\n",
    "paddle.save(mydict, 'mydict')\n",
    "mydict2 = paddle.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f10cb4f",
   "metadata": {},
   "source": [
    "## [**加载和保存模型参数**]\n",
    "\n",
    "保存单个权重向量（或其他张量）确实有用，\n",
    "但是如果我们想保存整个模型，并在以后加载它们，\n",
    "单独保存每个向量则会变得很麻烦。\n",
    "毕竟，我们可能有数百个参数散布在各处。\n",
    "因此，深度学习框架提供了内置函数来保存和加载整个网络。\n",
    "需要注意的一个重要细节是，这将保存模型的参数而不是保存整个模型。\n",
    "例如，如果我们有一个3层多层感知机，我们需要单独指定架构。\n",
    "因为模型本身可以包含任意代码，所以模型本身难以序列化。\n",
    "因此，为了恢复模型，我们需要用代码生成架构，\n",
    "然后从磁盘加载参数。\n",
    "让我们从熟悉的多层感知机开始尝试一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Dense(256, activation='relu')\n",
    "        self.output = nn.Dense(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(self.hidden(x))\n",
    "\n",
    "net = MLP()\n",
    "net.initialize()\n",
    "X = np.random.uniform(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41758406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.hidden(x)\n",
    "        return self.out(x)\n",
    "\n",
    "net = MLP()\n",
    "X = tf.random.uniform((2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "class MLP(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = paddle.randn(shape=[2, 20])\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba906e4",
   "metadata": {},
   "source": [
    "接下来，我们[**将模型的参数存储在一个叫做“mlp.params”的文件中。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422cb6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters('mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "net.save_weights('mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d615e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.save(net.state_dict(), 'mlp.pdparams')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41f08a",
   "metadata": {},
   "source": [
    "为了恢复模型，我们[**实例化了原始多层感知机模型的一个备份。**]\n",
    "这里我们不需要随机初始化模型参数，而是(**直接读取文件中存储的参数。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a73fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clone = MLP()\n",
    "clone.load_parameters('mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a46506",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "clone = MLP()\n",
    "clone.load_weights('mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "clone = MLP()\n",
    "clone.set_state_dict(paddle.load('mlp.pdparams'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b49f60",
   "metadata": {},
   "source": [
    "由于两个实例具有相同的模型参数，在输入相同的`X`时，\n",
    "两个实例的计算结果应该相同。\n",
    "让我们来验证一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7adbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad755e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b645e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d65de9",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* `save`和`load`函数可用于张量对象的文件读写。\n",
    "* 我们可以通过参数字典保存和加载网络的全部参数。\n",
    "* 保存架构必须在代码中完成，而不是在参数中完成。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 即使不需要将经过训练的模型部署到不同的设备上，存储模型参数还有什么实际的好处？\n",
    "1. 假设我们只想复用网络的一部分，以将其合并到不同的网络架构中。比如想在一个新的网络中使用之前网络的前两层，该怎么做？\n",
    "1. 如何同时保存网络架构和参数？需要对架构加上什么限制？\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/1840)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1839)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1838)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11781)\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b36c6e",
   "metadata": {},
   "source": [
    "# GPU\n",
    ":label:`sec_use_gpu`\n",
    "\n",
    "在 :numref:`tab_intro_decade`中，\n",
    "我们回顾了过去20年计算能力的快速增长。\n",
    "简而言之，自2000年以来，GPU性能每十年增长1000倍。\n",
    "\n",
    "本节，我们将讨论如何利用这种计算性能进行研究。\n",
    "首先是如何使用单个GPU，然后是如何使用多个GPU和多个服务器（具有多个GPU）。\n",
    "\n",
    "我们先看看如何使用单个NVIDIA GPU进行计算。\n",
    "首先，确保至少安装了一个NVIDIA GPU。\n",
    "然后，下载[NVIDIA驱动和CUDA](https://developer.nvidia.com/cuda-downloads)\n",
    "并按照提示设置适当的路径。\n",
    "当这些准备工作完成，就可以使用`nvidia-smi`命令来(**查看显卡信息。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e9bf4",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "读者可能已经注意到MXNet张量看起来与NumPy的`ndarray`几乎相同。\n",
    "但有一些关键区别，其中之一是MXNet支持不同的硬件设备。\n",
    "\n",
    "在MXNet中，每个数组都有一个环境（context）。\n",
    "默认情况下，所有变量和相关的计算都分配给CPU。\n",
    "有时环境可能是GPU。\n",
    "当我们跨多个服务器部署作业时，事情会变得更加棘手。\n",
    "通过智能地将数组分配给环境，\n",
    "我们可以最大限度地减少在设备之间传输数据的时间。\n",
    "例如，当在带有GPU的服务器上训练神经网络时，\n",
    "我们通常希望模型的参数在GPU上。\n",
    "\n",
    "接下来，我们需要确认是否安装了MXNet的GPU版本。\n",
    "如果已经安装了MXNet的CPU版本，我们需要先卸载它。\n",
    "例如，使用`pip uninstall mxnet`命令，\n",
    "然后根据CUDA版本安装相应的MXNet的GPU版本。\n",
    "例如，假设已经安装了CUDA10.0，可以通过`pip install mxnet-cu100`安装支持CUDA10.0的MXNet版本。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "在PyTorch中，每个数组都有一个设备（device），\n",
    "我们通常将其称为环境（context）。\n",
    "默认情况下，所有变量和相关的计算都分配给CPU。\n",
    "有时环境可能是GPU。\n",
    "当我们跨多个服务器部署作业时，事情会变得更加棘手。\n",
    "通过智能地将数组分配给环境，\n",
    "我们可以最大限度地减少在设备之间传输数据的时间。\n",
    "例如，当在带有GPU的服务器上训练神经网络时，\n",
    "我们通常希望模型的参数在GPU上。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "在PaddlePaddle中，每个张量都有一个设备（device），\n",
    "我们通常将其称为上下文（context）。\n",
    "默认情况下，所有变量和相关的计算都分配给CPU。\n",
    "有时上下文可能是GPU。\n",
    "当我们跨多个服务器部署作业时，事情会变得更加棘手。\n",
    "通过智能地将数组分配给上下文，\n",
    "我们可以最大限度地减少在设备之间传输数据的时间。\n",
    "例如，当在带有GPU的服务器上训练神经网络时，\n",
    "我们通常希望模型的参数在GPU上。\n",
    "\n",
    "接下来，我们需要确认安装了PaddlePaddle的GPU版本。\n",
    "如果已经安装了PaddlePaddle的CPU版本，我们需要先卸载它。\n",
    "然后根据你的CUDA版本安装相应的PaddlePaddle的GPU版本。\n",
    "例如，假设你安装了CUDA10.1，你可以通过`conda install paddlepaddle-gpu==2.2.2 cudatoolkit=10.1 --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/Paddle/`安装支持CUDA10.1的PaddlePaddle版本。\n",
    ":end_tab:\n",
    "\n",
    "要运行此部分中的程序，至少需要两个GPU。\n",
    "注意，对大多数桌面计算机来说，这可能是奢侈的，但在云中很容易获得。\n",
    "例如可以使用AWS EC2的多GPU实例。\n",
    "本书的其他章节大都不需要多个GPU，\n",
    "而本节只是为了展示数据如何在不同的设备之间传递。\n",
    "\n",
    "## [**计算设备**]\n",
    "\n",
    "我们可以指定用于存储和计算的设备，如CPU和GPU。\n",
    "默认情况下，张量是在内存中创建的，然后使用CPU计算它。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "在MXNet中，CPU和GPU可以用`cpu()`和`gpu()`表示。\n",
    "需要注意的是，`cpu()`（或括号中的任意整数）表示所有物理CPU和内存，\n",
    "这意味着MXNet的计算将尝试使用所有CPU核心。\n",
    "然而，`gpu()`只代表一个卡和相应的显存。\n",
    "如果有多个GPU，我们使用`gpu(i)`表示第$i$块GPU（$i$从0开始）。\n",
    "另外，`gpu(0)`和`gpu()`是等价的。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "在PyTorch中，CPU和GPU可以用`torch.device('cpu')`\n",
    "和`torch.device('cuda')`表示。\n",
    "应该注意的是，`cpu`设备意味着所有物理CPU和内存，\n",
    "这意味着PyTorch的计算将尝试使用所有CPU核心。\n",
    "然而，`gpu`设备只代表一个卡和相应的显存。\n",
    "如果有多个GPU，我们使用`torch.device(f'cuda:{i}')`\n",
    "来表示第$i$块GPU（$i$从0开始）。\n",
    "另外，`cuda:0`和`cuda`是等价的。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "在飞桨中，CPU和GPU可以用`paddle.device.set_device('cpu')` \n",
    "和`paddle.device.set_device('gpu')`表示。 \n",
    "应该注意的是，`cpu`设备意味着所有物理CPU和内存,\n",
    "这意味着飞桨的计算将尝试使用所有CPU核心。 \n",
    "然而，`gpu`设备只代表一个卡和相应的显存。 \n",
    "如果有多个GPU，我们使用`paddle.device.get_device()`\n",
    "其中输出的数字是表示的是卡号（比如`gpu:3`，表示的是卡3，注意GPU的卡号是从0开始的）。 \n",
    "另外，`gpu:0`和`gpu`是等价的。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99334187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "npx.cpu(), npx.gpu(), npx.gpu(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798c998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.device('cpu'), torch.device('cuda'), torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64691b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.device('/CPU:0'), tf.device('/GPU:0'), tf.device('/GPU:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400fd3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "import paddle\n",
    "from paddle import nn\n",
    "\n",
    "paddle.device.set_device(\"cpu\"), paddle.CUDAPlace(0), paddle.CUDAPlace(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32b2c40",
   "metadata": {},
   "source": [
    "我们可以(**查询可用gpu的数量。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98911746",
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.num_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52752e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "len(tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee14f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "paddle.device.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8db1c",
   "metadata": {},
   "source": [
    "现在我们定义了两个方便的函数，\n",
    "[**这两个函数允许我们在不存在所需所有GPU的情况下运行代码。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    return npx.gpu(i) if npx.num_gpus() >= i + 1 else npx.cpu()\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu()]\"\"\"\n",
    "    devices = [npx.gpu(i) for i in range(npx.num_gpus())]\n",
    "    return devices if devices else [npx.cpu()]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf32451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]\"\"\"\n",
    "    devices = [torch.device(f'cuda:{i}')\n",
    "             for i in range(torch.cuda.device_count())]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ec779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if len(tf.config.experimental.list_physical_devices('GPU')) >= i + 1:\n",
    "        return tf.device(f'/GPU:{i}')\n",
    "    return tf.device('/CPU:0')\n",
    "\n",
    "def try_all_gpus():  #@save\n",
    "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]\"\"\"\n",
    "    num_gpus = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "    devices = [tf.device(f'/GPU:{i}') for i in range(num_gpus)]\n",
    "    return devices if devices else [tf.device('/CPU:0')]\n",
    "\n",
    "try_gpu(), try_gpu(10), try_all_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3500f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "#@save\n",
    "def try_gpu(i=0):  \n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()。\"\"\"\n",
    "    if paddle.device.cuda.device_count() >= i + 1:\n",
    "        return paddle.CUDAPlace(i)\n",
    "    return paddle.CPUPlace()\n",
    "\n",
    "#@save\n",
    "def try_all_gpus():  \n",
    "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]。\"\"\"\n",
    "    devices = [paddle.CUDAPlace(i)\n",
    "               for i in range(paddle.device.cuda.device_count())]\n",
    "    return devices if devices else paddle.CPUPlace()\n",
    "\n",
    "try_gpu(),try_gpu(10),try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69563087",
   "metadata": {},
   "source": [
    "## 张量与GPU\n",
    "\n",
    "我们可以[**查询张量所在的设备。**]\n",
    "默认情况下，张量是在CPU上创建的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5adfb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "x.ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4070671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "x = torch.tensor([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78040ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "x = tf.constant([1, 2, 3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a542538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "x = paddle.to_tensor([1, 2, 3])\n",
    "x.place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465b684b",
   "metadata": {},
   "source": [
    "需要注意的是，无论何时我们要对多个项进行操作，\n",
    "它们都必须在同一个设备上。\n",
    "例如，如果我们对两个张量求和，\n",
    "我们需要确保两个张量都位于同一个设备上，\n",
    "否则框架将不知道在哪里存储结果，甚至不知道在哪里执行计算。\n",
    "\n",
    "### [**存储在GPU上**]\n",
    "\n",
    "有几种方法可以在GPU上存储张量。\n",
    "例如，我们可以在创建张量时指定存储设备。接\n",
    "下来，我们在第一个`gpu`上创建张量变量`X`。\n",
    "在GPU上创建的张量只消耗这个GPU的显存。\n",
    "我们可以使用`nvidia-smi`命令查看显存使用情况。\n",
    "一般来说，我们需要确保不创建超过GPU显存限制的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones((2, 3), ctx=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "X = torch.ones(2, 3, device=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "with try_gpu():\n",
    "    X = tf.ones((2, 3))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2f4e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "X = paddle.to_tensor(paddle.ones(shape=[2, 3]), place=try_gpu())\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e95b4",
   "metadata": {},
   "source": [
    "假设我们至少有两个GPU，下面的代码将在(**第二个GPU上创建一个随机张量。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.random.uniform(size=(2, 3), ctx=try_gpu(1))\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf9ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "Y = torch.rand(2, 3, device=try_gpu(1))\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3591af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "with try_gpu(1):\n",
    "    Y = tf.random.uniform((2, 3))\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1114e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "Y = paddle.to_tensor(paddle.rand([2, 3]), place=try_gpu(1))\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a598a4",
   "metadata": {},
   "source": [
    "### 复制\n",
    "\n",
    "如果我们[**要计算`X + Y`，我们需要决定在哪里执行这个操作**]。\n",
    "例如，如 :numref:`fig_copyto`所示，\n",
    "我们可以将`X`传输到第二个GPU并在那里执行操作。\n",
    "*不要*简单地`X`加上`Y`，因为这会导致异常，\n",
    "运行时引擎不知道该怎么做：它在同一设备上找不到数据会导致失败。\n",
    "由于`Y`位于第二个GPU上，所以我们需要将`X`移到那里，\n",
    "然后才能执行相加运算。\n",
    "\n",
    "![复制数据以在同一设备上执行操作](../img/copyto.svg)\n",
    ":label:`fig_copyto`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X.copyto(try_gpu(1))\n",
    "print(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a51ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "Z = X.cuda(1)\n",
    "print(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "with try_gpu(1):\n",
    "    Z = X\n",
    "print(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca4e2d",
   "metadata": {},
   "source": [
    "[**现在数据在同一个GPU上（`Z`和`Y`都在），我们可以将它们相加。**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def3e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "Y + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772c939",
   "metadata": {},
   "source": [
    ":begin_tab:`mxnet`\n",
    "假设变量`Z`已经存在于第二个GPU上。\n",
    "如果现在我们还是调用`Z.copyto(gpu(1))`会发生什么？\n",
    "即使该变量已经存在于目标设备（第二个GPU）上，\n",
    "它仍将被复制并保存在新分配的显存中。\n",
    "有时，我们只想在变量存在于不同设备中时进行复制。\n",
    "在这种情况下，我们可以调用`as_in_ctx`。\n",
    "如果变量已经存在于指定的设备中，则这不会进行任何操作。\n",
    "除非我们特别想创建一个复制，否则选择`as_in_ctx`方法。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "假设变量`Z`已经存在于第二个GPU上。\n",
    "如果我们还是调用`Z.cuda(1)`会发生什么？\n",
    "它将返回`Z`，而不会复制并分配新内存。\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "假设变量`Z`已经存在于第二个GPU上。\n",
    "如果我们仍然在同一个设备作用域下调用`Z2 = Z`会发生什么？\n",
    "它将返回`Z`，而不会复制并分配新内存。\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe18ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.as_in_ctx(try_gpu(1)) is Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch, paddle\n",
    "Z.cuda(1) is Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a562da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "with try_gpu(1):\n",
    "    Z2 = Z\n",
    "Z2 is Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee627a8c",
   "metadata": {},
   "source": [
    "### 旁注\n",
    "\n",
    "人们使用GPU来进行机器学习，因为单个GPU相对运行速度快。\n",
    "但是在设备（CPU、GPU和其他机器）之间传输数据比计算慢得多。\n",
    "这也使得并行化变得更加困难，因为我们必须等待数据被发送（或者接收），\n",
    "然后才能继续进行更多的操作。\n",
    "这就是为什么拷贝操作要格外小心。\n",
    "根据经验，多个小操作比一个大操作糟糕得多。\n",
    "此外，一次执行几个操作比代码中散布的许多单个操作要好得多。\n",
    "如果一个设备必须等待另一个设备才能执行其他操作，\n",
    "那么这样的操作可能会阻塞。\n",
    "这有点像排队订购咖啡，而不像通过电话预先订购：\n",
    "当客人到店的时候，咖啡已经准备好了。\n",
    "\n",
    "最后，当我们打印张量或将张量转换为NumPy格式时，\n",
    "如果数据不在内存中，框架会首先将其复制到内存中，\n",
    "这会导致额外的传输开销。\n",
    "更糟糕的是，它现在受制于全局解释器锁，使得一切都得等待Python完成。\n",
    "\n",
    "## [**神经网络与GPU**]\n",
    "\n",
    "类似地，神经网络模型可以指定设备。\n",
    "下面的代码将模型参数放在GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9034097",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(1))\n",
    "net.initialize(ctx=try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac503a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "net = nn.Sequential(nn.Linear(3, 1))\n",
    "net = net.to(device=try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    net = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105cb785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "net = nn.Sequential(nn.Linear(3, 1))\n",
    "net=net.to(try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130e44bd",
   "metadata": {},
   "source": [
    "在接下来的几章中，\n",
    "我们将看到更多关于如何在GPU上运行模型的例子，\n",
    "因为它们将变得更加计算密集。\n",
    "\n",
    "当输入为GPU上的张量时，模型将在同一GPU上计算结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62964b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69020ff6",
   "metadata": {},
   "source": [
    "让我们(**确认模型参数存储在同一个GPU上。**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c84723",
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].weight.data().ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467d54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab tensorflow\n",
    "net.layers[0].weights[0].device, net.layers[0].weights[1].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f11e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab paddle\n",
    "net[0].weight.place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847fb87",
   "metadata": {},
   "source": [
    "总之，只要所有的数据和参数都在同一个设备上，\n",
    "我们就可以有效地学习模型。\n",
    "在下面的章节中，我们将看到几个这样的例子。\n",
    "\n",
    "## 小结\n",
    "\n",
    "* 我们可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主内存中创建，然后使用CPU进行计算。\n",
    "* 深度学习框架要求计算的所有输入数据都在同一设备上，无论是CPU还是GPU。\n",
    "* 不经意地移动数据可能会显著降低性能。一个典型的错误如下：计算GPU上每个小批量的损失，并在命令行中将其报告给用户（或将其记录在NumPy `ndarray`中）时，将触发全局解释器锁，从而使所有GPU阻塞。最好是为GPU内部的日志分配内存，并且只移动较大的日志。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 尝试一个计算量更大的任务，比如大矩阵的乘法，看看CPU和GPU之间的速度差异。再试一个计算量很小的任务呢？\n",
    "1. 我们应该如何在GPU上读写模型参数？\n",
    "1. 测量计算1000个$100 \\times 100$矩阵的矩阵乘法所需的时间，并记录输出矩阵的Frobenius范数，一次记录一个结果，而不是在GPU上保存日志并仅传输最终结果。\n",
    "1. 测量同时在两个GPU上执行两个矩阵乘法与在一个GPU上按顺序执行两个矩阵乘法所需的时间。提示：应该看到近乎线性的缩放。\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/1843)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1841)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/1842)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`paddle`\n",
    "[Discussions](https://discuss.d2l.ai/t/11782)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

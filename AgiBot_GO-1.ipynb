{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3dadd9-2407-455f-ac00-9ad7992eb405",
   "metadata": {},
   "source": [
    "[AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems](https://agibot-world.com/blog/go1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb10728-626b-4047-ae8b-5981b8b39224",
   "metadata": {},
   "source": [
    "# AGIBOTWORLD: MODEL\n",
    "<!-- To effectively utilize our high-quality AgiBot World dataset and enhance the policy’s generalizability, we propose a hierarchical Vision-Language-Latent-Action (ViLLA) framework with three training stages, as depicted in Fig. 4. Compared to Vision-Language-Action (VLA) model where action is vision-language conditioned, the ViLLA model predicts latent action tokens, conditioned on the generation of subsequent robot control actions. -->\n",
    "\n",
    "为了有效利用我们高质量的 AgiBot World 数据集并增强策略的通用性, 我们提出了一个具有三个训练阶段的分层 Vision-Language-Latent-Action (ViLLA) 框架, 如图 4 所示。与视觉-语言-动作 (VLA) 模型(其中动作以视觉语言为条件)相比, ViLLA 模型以后续机器人控制动作的生成为条件来预测 latent action tokens。\n",
    "\n",
    "<!-- In Stage 1, we project consecutive images into a latent action space by training an encoder-decoder latent action model (LAM) on internet-scale heterogeneous data. This allows the latent action to serve as an intermediate representation, bridging the gap between general image-text inputs and robotic actions. In Stage 2, these latent actions act as pseudo-labels for the latent planner, facilitating embodiment-agnostic long-horizon planning and leveraging the generalizability of the pre-trained VLM. Finally, in Stage 3, we introduce the action expert and jointly train it with the latent planner to support the learning of dexterous manipulation. -->\n",
    "\n",
    "在第 1 阶段, 我们通过在互联网规模的异构数据上训练编码器-解码器 latent action model (LAM), 将连续图像投射到 latent action 空间。这使得 latent action 可以作为中间表示, 弥合通用图像文本输入和机器人动作之间的差距。在第 2 阶段, 这些 latent actions 充当 latent 规划器的伪标签, 便于与具身无关的长期规划并利用预训练 VLM 的通用性。最后, 在第 3 阶段, 我们引入动作专家并将其与 latent 规划器联合训练, 以支持灵巧操作的学习。\n",
    "\n",
    "## Latent Action Model\n",
    "<!-- Despite considerable advancements in gathering diverse robot demonstrations, the volume of action-labeled robot data remains limited relative to web-scale datasets. To broaden the data pool by incorporating internet-scale human videos lacking action labels and cross-embodiment robot data, we employ latent actions [30] in Stage 1 to model the inverse dynamics of consecutive frames. This approach enables the transfer of real-world dynamics from heterogeneous data sources into universal manipulation knowledge. -->\n",
    "\n",
    "尽管在收集多样化的机器人演示方面取得了显著进展, 但相对于互联网规模的数据集, 带有动作标签的机器人数据量仍然有限。为了通过整合缺乏动作标签的互联网规模的人类视频和跨实体机器人数据来扩大数据池, 我们在第 1 阶段采用 latent actions [30] 来建模连续帧的逆动力学。这种方法可以将现实世界的动态从异构数据源转移到通用操纵知识中。\n",
    "\n",
    "<!-- To extract latent actions from video frames $\\{I_t,I_{t+H} \\}$, the latent action model is constructed around an inverse dynamics model-based encoder $\\mathbf{I}(z_t|I_t,I_{t+H})$ and a forward dynamics model-based decoder $\\mathbf{F}(I_{t+H}|I_t,z_t)$. The encoder employs a spatial-temporal transformer [31] with casual temporal masks following Bruce et al. [30], while the decoder is a spatial transformer that takes the initial frame and discretized latent action tokens $z_t = [z^0_t, \\dots ,z^{k−1}_t]$ as input, with $k$ set to 4. The latent action tokens are quantized using a VQ-VAE objective [32], with a codebook of size $|C|$. -->\n",
    "\n",
    "为了从视频帧 $\\{I_t,I_{t+H} \\}$ 中提取 latent actions, latent action 模型由一个基于逆动力学模型的编码器 $\\mathbf{I}(z_t|I_t,I_{t+H})$ 和一个基于前向动力学模型的解码器 $\\mathbf{F}(I_{t+H}|I_t,z_t)$ 构建。编码器采用时空 transformer [31], 并遵循 Bruce et al. [30] 的因果时间掩码, 而解码器是一个空间 transformer, 以初始帧和离散的 latent action tokens $z_t = [z^0_t, \\dots ,z^{k−1}_t]$ 作为输入, $k$ 设置为 4。使用 VQ-VAE 目标 [32] 量化 latent action tokens, 代码本大小为 $|C|$。$。\n",
    "                                   \n",
    "## Latent Planner\n",
    "With the aim of establishing a solid foundation for sce e\n",
    "and object understanding and general reasoning ability, t e\n",
    "ViLLA model harnesses a VLM pre-trained on web-sca vision-language data and incorporates a latent planner for \n",
    "embodiment-agnostic planning within the latent action space \r\n",
    "We use InternVL2.5-2B [33] as the VLM backbone d e\r\n",
    "to its strong transfer learning capabilities. The two-bill on\r\n",
    "parameter scale has proven effective for robotic tasks in our\r\n",
    "preliminary experiments, as well as in prior studies  10],\r\n",
    "[26]. Multiview image observations are first encoded using\r\n",
    "InternViT before being projected into the language space.\r\n",
    "The latent planner consists of 24 transformer layer , which\r\n",
    "enable layer-by-layer conditioning from the VLM backbone\r\n",
    "with full bidirectional \n",
    "attention.\r\n",
    "Specifically, given multiview i $(p^u_ i m^a_es \r",
    "^\n",
    "_h)$ t,Ilt,Ir\r\n",
    "t\u0001\r\n",
    "(typ-\r\n",
    "ically from the head, left wrist, and right w $i$st) at timestep\r\n",
    "t, along with a lan$g$ uage instruction ldcribing the ongo-\r\n",
    "ing task, the latent planner predicts la $\\mathbf{e}(t_ ac^t_on ^t_ken^s_\r\n",
    "P)$zt|Ih\r\n",
    "t,Il\r\n",
    "t,Ir\r\n",
    "t,l\u0001\r\n",
    ", with superv sion produced by the LAM\r\n",
    "encode$r_  ba \\mathbf{s}ed^ _n t^h_{ he}a$d view: zt:=I(Ih\r\n",
    "t,Ih\r\n",
    "t+H). Since\r\n",
    "the latent action space is ord rs of magnitude smaller than\r\n",
    "the discretized low-level act ons used in OpenVLA [4], this\r\n",
    "approach also facilitates the eficient adaptation of general-\r\n",
    "p\n",
    "\n",
    "为了建立场景和物体理解以及一般推理能力的坚实基础，ViLLA模型利用了在网络规模的视觉-语言数据上预训练的视觉语言模型（VLM），并结合了用于潜在动作空间内与具体实现无关的规划的潜在规划器。我们选择 InternVL2.5-2B [33] 作为 VLM 的骨干网络，因为它具有强大的迁移学习能力。在我们的初步实验以及先前的研究 [10]、[26] 中，两亿参数的规模已被证明对机器人任务是有效的。多视角图像观测首先使用 InternViT 编码，然后投影到语言空间。潜在规划器由 24 层变换器组成，使得可以逐层从 VLM 骨干网络进行全双向注意力的条件化。\n",
    "\n",
    "具体来说，给定时间步 $t$ 的多视角输入图像 $(I^h_t, I^l_t, I^r_t)$（通常来自头部、左手腕和右手腕），以及描述当前任务的语言指令 $l$，潜在规划器预测潜在动作标记：$\\mathbf{P}(z_t|I^h_t,I^l_t,I^r_t,l)$，该预测由基于头部视角的 LAM 编码器生成监督：$z_t := \\mathbf{I}(I^h_t,I^h_{t+H})$。由于潜在动作空间的规模比 OpenVLA [4] 中使用的离散低级动作小几个数量级，这种方法也有助于将通用 VLM 高效适应到机器人策略中。\n",
    "u##ose VLMs into robot policies.\r\n",
    "C. Action Expert\r\n",
    "To achieve high-frequency and dexterous manipulation,\r\n",
    "Stage 3 integrates an action expert that utilizes a diffusion\r\n",
    "objective to model the continuous distribution of low-level\r\n",
    "actions [34]. Although the action expert shares the same\r\n",
    "architectural framework as the latent planner, their objectives\r\n",
    "diverge: the latent planner generates discretized latent action\r\n",
    "tokens through masked language modeling, while the action\r\n",
    "expert regresses low-level actions via an iterative denoising\r\n",
    "process. Both expert modules are conditioned hierarchically\r\n",
    "on preceding modules, including the action expert itself,\r\n",
    "ensuring coherent integration and information flow within\r\n",
    "the dual-expert system.\r\n",
    "The action expert$ _decod_es low-level acti$ on ch$unks$, de-noted  At= [at,at+1, ...$,_a$ t+H]with H=30, using$ $ pro-\r\n",
    "prio $\\mathbf{e}(t_ive^ _tat^e_pto^v_r a_n i)$erval of Htimesteps:\r\n",
    "A\r\n",
    "At|Ih\r\n",
    "t,Il\r\n",
    "t,\r\n",
    "t,pt,l\u0001\r\n",
    ". During inference, the VLM, latent plan-\r\n",
    "ner, and action expert are synergistically combined within$\r",
    "$ \n",
    "the g neralist policy GO-1, which initially predicts klatent\r",
    " action tokens and subsequently conditions the denoising\r\n",
    "process to produce the final control signals.le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac6f3db-b4b3-4b34-b3b0-10843fdc22fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LargeVision-languageModel…InternViTTokenizerLatent PlannerAction Expert\n",
      "Task InstructionsMultiviewImagesLAMEncoderLAMDecoderLatentAction Tokens\n",
      "𝐼!\"#𝐼!𝑧$𝑧%𝑧&𝑧'(%\n",
      "𝐼#!\"#\n",
      "“Hang the T-shirt”𝑎$𝑎%𝑎&𝑎#(%ActionChunk\n",
      "…Latent ActionRobotAction…Reconstructed\n",
      "HumanVideos\n",
      "Web-scaleVision-languageData\n",
      "Cross-EmbodimentRobotDataGO-1𝑝𝑎)𝑧$𝑧%𝑧&𝑧'(%Latent PlanningStage-1\n",
      "Stage-2Stage-3Image& Text\n",
      "AgiBotWorld\n",
      "……𝑧)…𝑎*…Pre-trainingData\n",
      "𝑧)Fig. 4: We propose GO-1 , a generalist policy featuring general reasoning and long-horizon planning capabilities. The latent\n",
      "action model (LAM) learns universal action representations from web-scale video data ( i.e.,human videos from Ego4D),\n",
      "and quantizes them into discrete latent action tokens. The latent planner conducts temporal reasoning through latent action\n",
      "prediction, bridging the gap between image-text inputs and robot actions generated by the action expert.\n",
      "vision-language data and incorporates a latent planner for\n",
      "embodiment-agnostic planning within the latent action space.\n",
      "We use InternVL2.5-2B [33] as the VLM backbone due\n",
      "to its strong transfer learning capabilities. The two-billion\n",
      "parameter scale has proven effective for robotic tasks in our\n",
      "preliminary experiments, as well as in prior studies [10],\n",
      "[26]. Multiview image observations are first encoded using\n",
      "InternViT before being projected into the language space.\n",
      "The latent planner consists of 24 transformer layers, which\n",
      "enable layer-by-layer conditioning from the VLM backbone\n",
      "with full bidirectional attention.\n",
      "Specifically, given multiview input images\u0000\n",
      "Ih\n",
      "t,Il\n",
      "t,Ir\n",
      "t\u0001\n",
      "(typ-\n",
      "ically from the head, left wrist, and right wrist) at timestep\n",
      "t, along with a language instruction ldescribing the ongo-\n",
      "ing task, the latent planner predicts latent action tokens:\n",
      "P\u0000\n",
      "zt|Ih\n",
      "t,Il\n",
      "t,Ir\n",
      "t,l\u0001\n",
      ", with supervision produced by the LAM\n",
      "encoder based on the head view: zt:=I(Ih\n",
      "t,Ih\n",
      "t+H). Since\n",
      "the latent action space is orders of magnitude smaller than\n",
      "the discretized low-level actions used in OpenVLA [4], this\n",
      "approach also facilitates the efficient adaptation of general-\n",
      "purpose VLMs into robot policies.\n",
      "C. Action Expert\n",
      "To achieve high-frequency and dexterous manipulation,\n",
      "Stage 3 integrates an action expert that utilizes a diffusion\n",
      "objective to model the continuous distribution of low-level\n",
      "actions [34]. Although the action expert shares the same\n",
      "architectural framework as the latent planner, their objectives\n",
      "diverge: the latent planner generates discretized latent action\n",
      "tokens through masked language modeling, while the action\n",
      "expert regresses low-level actions via an iterative denoising\n",
      "process. Both expert modules are conditioned hierarchically\n",
      "on preceding modules, including the action expert itself,\n",
      "ensuring coherent integration and information flow within\n",
      "the dual-expert system.\n",
      "The action expert decodes low-level action chunks, de-noted by At= [at,at+1, ...,at+H]with H=30, using pro-\n",
      "prioceptive state ptover an interval of Htimesteps:\n",
      "A\u0000\n",
      "At|Ih\n",
      "t,Il\n",
      "t,Ir\n",
      "t,pt,l\u0001\n",
      ". During inference, the VLM, latent plan-\n",
      "ner, and action expert are synergistically combined within\n",
      "the generalist policy GO-1, which initially predicts klatent\n",
      "action tokens and subsequently conditions the denoising\n",
      "process to produce the final control signals.\n",
      "V. E XPERIMENT AND ANALYSIS\n",
      "We evaluate the real-world performance of policies pre-\n",
      "trained on different data sources including the AgiBot World\n",
      "dataset, demonstrating the effectiveness credited from the\n",
      "GO-1 model in policy learning.\n",
      "A. Experiment Setup\n",
      "1) Evaluation Tasks\n",
      "Here we choose a comprehensive set of tasks that span var-\n",
      "ious dimensions of policy capabilities from AgiBot World for\n",
      "evaluation, including tool-usage (Wipe Table), deformable\n",
      "objects manipulation (Fold Shorts), human-robot interac-\n",
      "tion (Handover Bottle), language-following (Restock Bev-\n",
      "erage), etc. Moreover, we design 2 unseen scenarios for each\n",
      "task, covering position generalization, visual distractors, and\n",
      "language generalization, delivering thorough generalization\n",
      "evaluations for policies. The evaluated tasks, also partially\n",
      "shown in Fig. 5, are: 1) “Restock Bag”: Pick up the snack\n",
      "from the cart and place it on the supermarket shelf; 2) “Table\n",
      "Bussing”: Clear tabletop debris into the trash can; 3) “Pour\n",
      "Water”: Grasp the kettle handle, lift the kettle and pour water\n",
      "into the cup; 4) “Restock Beverage”: Pick up the bottled\n",
      "beverage from the cart and place it on the supermarket shelf;\n",
      "5) “Fold Shorts”: Fold the shorts laid flat on the table in half\n",
      "twice; 6) “Wipe Table”: Clean water spills using the sponge.\n",
      "Scoring rubrics. The evaluation metric employs a normal-\n",
      "ized score, computed as the average across 10 rollouts per\n",
      "task, scenario, and method. Each episode scores 1.0 for full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cfz/miniconda3/envs/learning/lib/python3.10/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "pdf_path = \"/mnt/y/paper/AgiBot_GO-1.pdf\"\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[5]\n",
    "text = page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcc6da-54fd-4728-abfd-f40f47280e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

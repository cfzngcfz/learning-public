{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3dadd9-2407-455f-ac00-9ad7992eb405",
   "metadata": {},
   "source": [
    "[AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems](https://agibot-world.com/blog/go1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e2651-bbd0-4b06-872d-128f070f629b",
   "metadata": {},
   "source": [
    "# AGIBOTWORLD: MODEL\n",
    "<!-- To effectively utilize our high-quality AgiBot World dataset and enhance the policyâ€™s generalizability, we propose a hierarchical Vision-Language-Latent-Action (ViLLA) framework with three training stages, as depicted in Fig. 4. Compared to Vision-Language-Action (VLA) model where action is vision-language conditioned, the ViLLA model predicts latent action tokens, conditioned on the generation of subsequent robot control actions. -->\n",
    "\n",
    "ä¸ºäº†æœ‰æ•ˆåˆ©ç”¨æˆ‘ä»¬é«˜è´¨é‡çš„ AgiBot World æ•°æ®é›†å¹¶å¢å¼ºç­–ç•¥çš„é€šç”¨æ€§, æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªè®­ç»ƒé˜¶æ®µçš„åˆ†å±‚ Vision-Language-Latent-Action (ViLLA) æ¡†æ¶, å¦‚å›¾ 4 æ‰€ç¤ºã€‚ä¸è§†è§‰-è¯­è¨€-åŠ¨ä½œ (VLA) æ¨¡å‹(å…¶ä¸­åŠ¨ä½œä»¥è§†è§‰è¯­è¨€ä¸ºæ¡ä»¶)ç›¸æ¯”, ViLLA æ¨¡å‹ä»¥åç»­æœºå™¨äººæ§åˆ¶åŠ¨ä½œçš„ç”Ÿæˆä¸ºæ¡ä»¶æ¥é¢„æµ‹ latent action tokensã€‚\n",
    "\n",
    "<!-- In Stage 1, we project consecutive images into a latent action space by training an encoder-decoder latent action model (LAM) on internet-scale heterogeneous data. This allows the latent action to serve as an intermediate representation, bridging the gap between general image-text inputs and robotic actions. In Stage 2, these latent actions act as pseudo-labels for the latent planner, facilitating embodiment-agnostic long-horizon planning and leveraging the generalizability of the pre-trained VLM. Finally, in Stage 3, we introduce the action expert and jointly train it with the latent planner to support the learning of dexterous manipulation. -->\n",
    "\n",
    "åœ¨ç¬¬ 1 é˜¶æ®µ, æˆ‘ä»¬é€šè¿‡åœ¨äº’è”ç½‘è§„æ¨¡çš„å¼‚æ„æ•°æ®ä¸Šè®­ç»ƒç¼–ç å™¨-è§£ç å™¨ latent action model (LAM), å°†è¿ç»­å›¾åƒæŠ•å°„åˆ° latent action ç©ºé—´ã€‚è¿™ä½¿å¾— latent action å¯ä»¥ä½œä¸ºä¸­é—´è¡¨ç¤º, å¼¥åˆé€šç”¨å›¾åƒæ–‡æœ¬è¾“å…¥å’Œæœºå™¨äººåŠ¨ä½œä¹‹é—´çš„å·®è·ã€‚åœ¨ç¬¬ 2 é˜¶æ®µ, è¿™äº› latent actions å……å½“ latent è§„åˆ’å™¨çš„ä¼ªæ ‡ç­¾, ä¾¿äºä¸å…·èº«æ— å…³çš„é•¿æœŸè§„åˆ’å¹¶åˆ©ç”¨é¢„è®­ç»ƒ VLM çš„é€šç”¨æ€§ã€‚æœ€å, åœ¨ç¬¬ 3 é˜¶æ®µ, æˆ‘ä»¬å¼•å…¥åŠ¨ä½œä¸“å®¶å¹¶å°†å…¶ä¸ latent è§„åˆ’å™¨è”åˆè®­ç»ƒ, ä»¥æ”¯æŒçµå·§æ“ä½œçš„å­¦ä¹ ã€‚\n",
    "\n",
    "## Latent Action Model\n",
    "<!-- Despite considerable advancements in gathering diverse robot demonstrations, the volume of action-labeled robot data remains limited relative to web-scale datasets. To broaden the data pool by incorporating internet-scale human videos lacking action labels and cross-embodiment robot data, we employ latent actions [30] in Stage 1 to model the inverse dynamics of consecutive frames. This approach enables the transfer of real-world dynamics from heterogeneous data sources into universal manipulation knowledge. -->\n",
    "\n",
    "å°½ç®¡åœ¨æ”¶é›†å¤šæ ·åŒ–çš„æœºå™¨äººæ¼”ç¤ºæ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•, ä½†ç›¸å¯¹äºäº’è”ç½‘è§„æ¨¡çš„æ•°æ®é›†, å¸¦æœ‰åŠ¨ä½œæ ‡ç­¾çš„æœºå™¨äººæ•°æ®é‡ä»ç„¶æœ‰é™ã€‚ä¸ºäº†é€šè¿‡æ•´åˆç¼ºä¹åŠ¨ä½œæ ‡ç­¾çš„äº’è”ç½‘è§„æ¨¡çš„äººç±»è§†é¢‘å’Œè·¨å®ä½“æœºå™¨äººæ•°æ®æ¥æ‰©å¤§æ•°æ®æ± , æˆ‘ä»¬åœ¨ç¬¬ 1 é˜¶æ®µé‡‡ç”¨ latent actions [30] æ¥å»ºæ¨¡è¿ç»­å¸§çš„é€†åŠ¨åŠ›å­¦ã€‚è¿™ç§æ–¹æ³•å¯ä»¥å°†ç°å®ä¸–ç•Œçš„åŠ¨æ€ä»å¼‚æ„æ•°æ®æºè½¬ç§»åˆ°é€šç”¨æ“çºµçŸ¥è¯†ä¸­ã€‚\n",
    "\n",
    "<!-- To extract latent actions from video frames $\\{I_t,I_{t+H} \\}$, the latent action model is constructed around an inverse dynamics model-based encoder $\\mathbf{I}(z_t|I_t,I_{t+H})$ and a forward dynamics model-based decoder $\\mathbf{F}(I_{t+H}|I_t,z_t)$. The encoder employs a spatial-temporal transformer [31] with casual temporal masks following Bruce et al. [30], while the decoder is a spatial transformer that takes the initial frame and discretized latent action tokens $z_t = [z^0_t, \\dots ,z^{kâˆ’1}_t]$ as input, with $k$ set to 4. The latent action tokens are quantized using a VQ-VAE objective [32], with a codebook of size $|C|$. -->\n",
    "\n",
    "ä¸ºäº†ä»è§†é¢‘å¸§ $\\{I_t,I_{t+H} \\}$ ä¸­æå– latent actions, latent action æ¨¡å‹ç”±ä¸€ä¸ªåŸºäºé€†åŠ¨åŠ›å­¦æ¨¡å‹çš„ç¼–ç å™¨ $\\mathbf{I}(z_t|I_t,I_{t+H})$ å’Œä¸€ä¸ªåŸºäºå‰å‘åŠ¨åŠ›å­¦æ¨¡å‹çš„è§£ç å™¨ $\\mathbf{F}(I_{t+H}|I_t,z_t)$ æ„å»ºã€‚ç¼–ç å™¨é‡‡ç”¨æ—¶ç©º transformer [31], å¹¶éµå¾ª Bruce et al. [30] çš„å› æœæ—¶é—´æ©ç , è€Œè§£ç å™¨æ˜¯ä¸€ä¸ªç©ºé—´ transformer, ä»¥åˆå§‹å¸§å’Œç¦»æ•£çš„ latent action tokens $z_t = [z^0_t, \\dots ,z^{kâˆ’1}_t]$ ä½œä¸ºè¾“å…¥, $k$ è®¾ç½®ä¸º 4ã€‚ä½¿ç”¨ VQ-VAE ç›®æ ‡ [32] é‡åŒ– latent action tokens, ä»£ç æœ¬å¤§å°ä¸º $|C|$ã€‚\n",
    "                                   \n",
    "## Latent Planner\n",
    "<!-- With the aim of establishing a solid foundation for scene and object understanding and general reasoning ability, the ViLLA model harnesses a VLM pre-trained on web-scale vision-language data and incorporates a latent planner for embodiment-agnostic planning within the latent action space. We use InternVL2.5-2B [33] as the VLM backbone due to its strong transfer learning capabilities. The two-billion parameter scale has proven effective for robotic tasks in our preliminary experiments, as well as in prior studies [10], [26]. Multiview image observations are first encoded using InternViT before being projected into the language space. The latent planner consists of 24 transformer layers, which enable layer-by-layer conditioning from the VLM backbone with full bidirectional attention. -->\n",
    "\n",
    "ä¸ºäº†å¥ å®šåœºæ™¯å’Œç‰©ä½“ç†è§£ä»¥åŠä¸€èˆ¬æ¨ç†èƒ½åŠ›çš„åšå®åŸºç¡€, ViLLA æ¨¡å‹åˆ©ç”¨åœ¨äº’è”ç½‘è§„æ¨¡çš„è§†è§‰è¯­è¨€æ•°æ®ä¸Šé¢„è®­ç»ƒçš„ VLM, å¹¶è€¦åˆä¸€ä¸ª latent è§„åˆ’å™¨, ç”¨äº latent action ç©ºé—´ä¸Šçš„å…·èº«æ— å…³çš„è§„åˆ’ã€‚æˆ‘ä»¬ä½¿ç”¨ InternVL2.5-2B [33] ä½œä¸º VLM éª¨å¹², å› ä¸ºå…¶å¼ºå¤§çš„è¿ç§»å­¦ä¹ èƒ½åŠ›ã€‚åœ¨æˆ‘ä»¬çš„åˆæ­¥å®éªŒä»¥åŠå…ˆå‰çš„ç ”ç©¶ [10]ã€[26] ä¸­, 20 äº¿å‚æ•°è§„æ¨¡å·²è¢«è¯æ˜å¯¹æœºå™¨äººä»»åŠ¡æ˜¯æœ‰æ•ˆçš„ã€‚é¦–å…ˆä½¿ç”¨ InternViT å¯¹å¤šè§†è§’å›¾åƒè§‚æµ‹è¿›è¡Œç¼–ç , ç„¶åå°†å…¶æŠ•å½±åˆ°è¯­è¨€ç©ºé—´ä¸­ã€‚latent è§„åˆ’å™¨ç”± 24 ä¸ª transformer å±‚ç»„æˆ, å®ƒä»¬èƒ½å¤Ÿä» VLM éª¨å¹²è¿›è¡Œé€å±‚è°ƒèŠ‚, å¹¶å…·æœ‰å®Œå…¨åŒå‘æ³¨æ„åŠ›ã€‚\n",
    "\n",
    "<!-- Specifically, given multiview input images $(I^h_t, I^l_t, I^r_t)$ (typically from the head, left wrist, and right wrist) at timestep $t$, along with a language instruction $l$ describing the ongoing task, the latent planner predicts latent action tokens: $\\mathbf{P}(z_t|I^h_t,I^l_t,I^r_t,l)$, with supervision produced by the LAM encoder based on the head view: $z_t := \\mathbf{I}(I^h_t,I^h_{t+H})$. Since the latent action space is orders of magnitude smaller than the discretized low-level actions used in OpenVLA [4], this approach also facilitates the efficient adaptation of general-purpose VLMs into robot policies. -->\n",
    "\n",
    "å…·ä½“è€Œè¨€, ç»™å®šæ—¶é—´æ­¥é•¿ $t$ çš„å¤šè§†è§’è¾“å…¥å›¾åƒ $(I^h_t, I^l_t, I^r_t)$ (é€šå¸¸æ¥è‡ªå¤´éƒ¨ã€å·¦æ‰‹è…•å’Œå³æ‰‹è…•), ä»¥åŠæè¿°å½“å‰ä»»åŠ¡çš„è¯­è¨€æŒ‡ä»¤ $l$, latent è§„åˆ’å™¨é¢„æµ‹ latent action tokens: $\\mathbf{P}(z_t|I^h_t,I^l_t,I^r_t,l)$ï¼Œå…¶ä¸­ LAM ç¼–ç å™¨åŸºäºå¤´éƒ¨è§†è§’ç”Ÿæˆç›‘ç£: $z_t := \\mathbf{I}(I^h_t,I^h_{t+H})$ã€‚ç”±äº latent action ç©ºé—´æ¯” OpenVLA [4] ä¸­ä½¿ç”¨çš„ç¦»æ•£åŒ–ä¸‹å±‚åŠ¨ä½œå°å‡ ä¸ªæ•°é‡çº§, è¿™ç§æ–¹æ³•ä¹Ÿæœ‰åŠ©äºå°†é€šç”¨ VLM é«˜æ•ˆé€‚é…åˆ°æœºå™¨äººç­–ç•¥ã€‚\n",
    "\n",
    "## Action Expert\n",
    "<!-- To achieve high-frequency and dexterous manipulation, Stage 3 integrates an action expert that utilizes a diffusion objective to model the continuous distribution of low-level actions [34]. Although the action expert shares the same architectural framework as the latent planner, their objectives diverge: the latent planner generates discretized latent action tokens through masked language modeling, while the action expert regresses low-level actions via an iterative denoising process. Both expert modules are conditioned hierarchically on preceding modules, including the action expert itself, ensuring coherent integration and information flow within the dual-expert system. -->\n",
    "\n",
    "ä¸ºäº†å®ç°é«˜é¢‘å’Œçµå·§çš„æ“ä½œ, ç¬¬ 3 é˜¶æ®µé›†æˆä¸€ä¸ªåŠ¨ä½œä¸“å®¶, è¯¥ä¸“å®¶åˆ©ç”¨æ‰©æ•£ç›®æ ‡æ¥å»ºæ¨¡ä¸‹å±‚åŠ¨ä½œçš„è¿ç»­åˆ†å¸ƒ [34]ã€‚å°½ç®¡åŠ¨ä½œä¸“å®¶ä¸ latent è§„åˆ’å™¨å…±äº«ç›¸åŒçš„æ¶æ„æ¡†æ¶, ä½†å®ƒä»¬çš„ç›®æ ‡æœ‰æ‰€ä¸åŒ: latent è§„åˆ’å™¨é€šè¿‡æ©ç è¯­è¨€å»ºæ¨¡ç”Ÿæˆç¦»æ•£åŒ–çš„ latent action tokens, è€ŒåŠ¨ä½œä¸“å®¶é€šè¿‡è¿­ä»£å»å™ªè¿‡ç¨‹å›å½’ä¸‹å±‚åŠ¨ä½œã€‚è¿™ä¸¤ä¸ªä¸“å®¶æ¨¡å—åœ¨å±‚æ¬¡ä¸Šéƒ½ä»¥å…ˆå‰çš„æ¨¡å—(åŒ…æ‹¬åŠ¨ä½œä¸“å®¶è‡ªèº«)ä¸ºæ¡ä»¶, ä»è€Œç¡®ä¿åŒä¸“å®¶ç³»ç»Ÿå†…çš„è¿è´¯é›†æˆå’Œä¿¡æ¯æµã€‚\n",
    "\n",
    "The action expert decodes low-level action chunks, de-noted by $A_t= [a_t,a_{t+1}, \\dots,a_{t+H}]$ with $H=30$, using proprioceptive state $p_t$ over an interval of $H$ timesteps: $\\mathbf{A}(A_t|I^h_t,I^l_t,I^r_t,p_t,l)$. During inference, the VLM, latent planner, and action expert are synergistically combined within the generalist policy GO-1, which initially predicts $k$ latent action tokens and subsequently conditions the denoising process to produce the final control signals.\n",
    "\n",
    "åŠ¨ä½œä¸“å®¶è§£ç ä½çº§åŠ¨ä½œå—ï¼Œè¡¨ç¤ºä¸º $A_t= [a_t,a_{t+1}, \\dots,a_{t+H}]$ï¼Œå…¶ä¸­ $H=30$ï¼Œä½¿ç”¨æœ¬ä½“æ„Ÿå—çŠ¶æ€ $p_t$ï¼Œæ—¶é—´é—´éš”ä¸º $H$ ä¸ªæ—¶é—´æ­¥é•¿ï¼š$\\mathbf{A}(A_t|I^h_t,I^l_t,I^r_t,p_t,l)$ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒVLMã€æ½œåœ¨è§„åˆ’å™¨å’ŒåŠ¨ä½œä¸“å®¶åœ¨é€šç”¨ç­–ç•¥ GO-1 ä¸­ååŒç»“åˆï¼Œè¯¥ç­–ç•¥æœ€åˆé¢„æµ‹ $k$ ä¸ªæ½œåœ¨åŠ¨ä½œæ ‡è®°ï¼Œéšåè°ƒèŠ‚å»å™ªè¿‡ç¨‹ä»¥äº§ç”Ÿæœ€ç»ˆæ§åˆ¶ä¿¡å·ã€‚\n",
    "\n",
    "åŠ¨ä½œä¸“å®¶ä½¿ç”¨æœ¬ä½“çŠ¶æ€ $p_t$ åœ¨ $H$ ä¸ªæ—¶é—´æ­¥é•¿çš„é—´éš”å†…è§£ç ä½çº§åŠ¨ä½œå—ï¼Œè®°ä¸º $A_t= [a_t,a_{t+1}, \\dots,a_{t+H}]$ï¼Œå…¶ä¸­ $H=30$ï¼š$\\mathbf{A}(A_t|I^h_t,I^l_t,I^r_t,p_t,l)$ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒVLMã€æ½œåœ¨è§„åˆ’å™¨å’ŒåŠ¨ä½œä¸“å®¶åœ¨é€šç”¨ç­–ç•¥ GO-1 ä¸­ååŒç»“åˆï¼ŒGO-1 é¦–å…ˆé¢„æµ‹ $k$ ä¸ªæ½œåœ¨åŠ¨ä½œæ ‡è®°ï¼Œç„¶åæ¡ä»¶åŒ–å»å™ªè¿‡ç¨‹ä»¥ç”Ÿæˆæœ€ç»ˆçš„æ§åˆ¶ä¿¡å·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac6f3db-b4b3-4b34-b3b0-10843fdc22fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LargeVision-languageModelâ€¦InternViTTokenizerLatent PlannerAction Expert\n",
      "Task InstructionsMultiviewImagesLAMEncoderLAMDecoderLatentAction Tokens\n",
      "ğ¼!\"#ğ¼!ğ‘§$ğ‘§%ğ‘§&ğ‘§'(%\n",
      "ğ¼#!\"#\n",
      "â€œHang the T-shirtâ€ğ‘$ğ‘%ğ‘&ğ‘#(%ActionChunk\n",
      "â€¦Latent ActionRobotActionâ€¦Reconstructed\n",
      "HumanVideos\n",
      "Web-scaleVision-languageData\n",
      "Cross-EmbodimentRobotDataGO-1ğ‘ğ‘)ğ‘§$ğ‘§%ğ‘§&ğ‘§'(%Latent PlanningStage-1\n",
      "Stage-2Stage-3Image& Text\n",
      "AgiBotWorld\n",
      "â€¦â€¦ğ‘§)â€¦ğ‘*â€¦Pre-trainingData\n",
      "ğ‘§)Fig. 4: We propose GO-1 , a generalist policy featuring general reasoning and long-horizon planning capabilities. The latent\n",
      "action model (LAM) learns universal action representations from web-scale video data ( i.e.,human videos from Ego4D),\n",
      "and quantizes them into discrete latent action tokens. The latent planner conducts temporal reasoning through latent action\n",
      "prediction, bridging the gap between image-text inputs and robot actions generated by the action expert.\n",
      "vision-language data and incorporates a latent planner for\n",
      "embodiment-agnostic planning within the latent action space.\n",
      "We use InternVL2.5-2B [33] as the VLM backbone due\n",
      "to its strong transfer learning capabilities. The two-billion\n",
      "parameter scale has proven effective for robotic tasks in our\n",
      "preliminary experiments, as well as in prior studies [10],\n",
      "[26]. Multiview image observations are first encoded using\n",
      "InternViT before being projected into the language space.\n",
      "The latent planner consists of 24 transformer layers, which\n",
      "enable layer-by-layer conditioning from the VLM backbone\n",
      "with full bidirectional attention.\n",
      "Specifically, given multiview input images\u0000\n",
      "Ih\n",
      "t,Il\n",
      "t,Ir\n",
      "t\u0001\n",
      "(typ-\n",
      "ically from the head, left wrist, and right wrist) at timestep\n",
      "t, along with a language instruction ldescribing the ongo-\n",
      "ing task, the latent planner predicts latent action tokens:\n",
      "P\u0000\n",
      "zt|Ih\n",
      "t,Il\n",
      "t,Ir\n",
      "t,l\u0001\n",
      ", with supervision produced by the LAM\n",
      "encoder based on the head view: zt:=I(Ih\n",
      "t,Ih\n",
      "t+H). Since\n",
      "the latent action space is orders of magnitude smaller than\n",
      "the discretized low-level actions used in OpenVLA [4], this\n",
      "approach also facilitates the efficient adaptation of general-\n",
      "purpose VLMs into robot policies.\n",
      "C. Action Expert\n",
      "To achieve high-frequency and dexterous manipulation,\n",
      "Stage 3 integrates an action expert that utilizes a diffusion\n",
      "objective to model the continuous distribution of low-level\n",
      "actions [34]. Although the action expert shares the same\n",
      "architectural framework as the latent planner, their objectives\n",
      "diverge: the latent planner generates discretized latent action\n",
      "tokens through masked language modeling, while the action\n",
      "expert regresses low-level actions via an iterative denoising\n",
      "process. Both expert modules are conditioned hierarchically\n",
      "on preceding modules, including the action expert itself,\n",
      "ensuring coherent integration and information flow within\n",
      "the dual-expert system.\n",
      "The action expert decodes low-level action chunks, de-noted by At= [at,at+1, ...,at+H]with H=30, using pro-\n",
      "prioceptive state ptover an interval of Htimesteps:\n",
      "A\u0000\n",
      "At|Ih\n",
      "t,Il\n",
      "t,Ir\n",
      "t,pt,l\u0001\n",
      ". During inference, the VLM, latent plan-\n",
      "ner, and action expert are synergistically combined within\n",
      "the generalist policy GO-1, which initially predicts klatent\n",
      "action tokens and subsequently conditions the denoising\n",
      "process to produce the final control signals.\n",
      "V. E XPERIMENT AND ANALYSIS\n",
      "We evaluate the real-world performance of policies pre-\n",
      "trained on different data sources including the AgiBot World\n",
      "dataset, demonstrating the effectiveness credited from the\n",
      "GO-1 model in policy learning.\n",
      "A. Experiment Setup\n",
      "1) Evaluation Tasks\n",
      "Here we choose a comprehensive set of tasks that span var-\n",
      "ious dimensions of policy capabilities from AgiBot World for\n",
      "evaluation, including tool-usage (Wipe Table), deformable\n",
      "objects manipulation (Fold Shorts), human-robot interac-\n",
      "tion (Handover Bottle), language-following (Restock Bev-\n",
      "erage), etc. Moreover, we design 2 unseen scenarios for each\n",
      "task, covering position generalization, visual distractors, and\n",
      "language generalization, delivering thorough generalization\n",
      "evaluations for policies. The evaluated tasks, also partially\n",
      "shown in Fig. 5, are: 1) â€œRestock Bagâ€: Pick up the snack\n",
      "from the cart and place it on the supermarket shelf; 2) â€œTable\n",
      "Bussingâ€: Clear tabletop debris into the trash can; 3) â€œPour\n",
      "Waterâ€: Grasp the kettle handle, lift the kettle and pour water\n",
      "into the cup; 4) â€œRestock Beverageâ€: Pick up the bottled\n",
      "beverage from the cart and place it on the supermarket shelf;\n",
      "5) â€œFold Shortsâ€: Fold the shorts laid flat on the table in half\n",
      "twice; 6) â€œWipe Tableâ€: Clean water spills using the sponge.\n",
      "Scoring rubrics. The evaluation metric employs a normal-\n",
      "ized score, computed as the average across 10 rollouts per\n",
      "task, scenario, and method. Each episode scores 1.0 for full\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cfz/miniconda3/envs/learning/lib/python3.10/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "pdf_path = \"/mnt/y/paper/AgiBot_GO-1.pdf\"\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[5]\n",
    "text = page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcc6da-54fd-4728-abfd-f40f47280e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

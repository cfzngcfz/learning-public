{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3dadd9-2407-455f-ac00-9ad7992eb405",
   "metadata": {},
   "source": [
    "[AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems](https://agibot-world.com/blog/go1)\n",
    "\n",
    "[github](https://github.com/OpenDriveLab/Agibot-World)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c8842-666c-4918-97bb-6c844d56c6b2",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "todo ...\n",
    "\n",
    "<!-- Beyond its immediate impact, AgiBot World lays a strong foundation for future research in robotic manipulation. By open-sourcing the dataset, toolchain, and pre-trained models, we aim to foster community-wide innovation, enabling researchers to explore more authentic and diverse applications from household assistant to industrial automation. AgiBot World is more than yet another dataset; it is a step toward scalable, general-purpose robotic intelligence, empowering robots to tackle the complexities of the real world. -->\n",
    "\n",
    "除了其直接影响, AgiBot World 还为机器人操作的未来研究奠定了坚实的基础。通过开源数据集、工具链和预训练模型, 我们旨在促进整个社区的创新, 使研究人员能够探索从家庭助理到工业自动化的更真实和多样化的应用。AgiBot World 不仅仅是另外一个数据集; 它是迈向可扩展的通用机器人智能的一步, 使机器人能够应对现实世界的复杂性。\n",
    "\n",
    "<!-- **Contribution**. 1) We construct AgiBot World dataset, a multifarious robot learning dataset accompanied by open-source tools to advance research on policy learning at scale. As a pioneering initiative, AgiBot World employs an inclusive optimized pipeline, from scene configuration, task design, data collection, to human-in-the-loop verification, which ensures unparalleled data quality. 2) We propose GO-1, a robot foundation policy using latent action representations to unlock web-scale pre-training on heterogeneous data. Empowered by AgiBot World dataset, it outperforms prior generalist policies in generalization and dexterity, with performance scaling predictably with dataset size. -->\n",
    "\n",
    "**贡献**. 1) 我们构建了 AgiBot World 数据集, 一个多样化的机器人学习数据集，并附带开源工具, 以推动大规模策略学习的研究。作为一项开创性的举措，AgiBot World 采用了一个包容的优化流水线, 从场景配置、任务设计、数据收集到人机交互验证, 确保了无与伦比的数据质量。2) 我们提出了 GO-1, 一种使用 latent action 表示的机器人基础策略, 以解锁异构数据上的互联网规模预训练。在 AgiBot World 数据集的支持下, 它在泛化和灵活性方面优于先前的通用策略, 并且性能随着数据集大小可预测地扩展。\n",
    "\n",
    "<!-- **Limitation**. All evaluations are conducted in real-world scenarios. We are currently developing the simulation environment, aligning with the real-world setup and aiming to reflect real-world policy deployment outcome. It would thereby facilitate fast and reproducible evaluation. -->\n",
    "\n",
    "**局限性**. 所有评估均在现实世界场景中进行。我们目前正在开发模拟环境, 与现实世界配置保持一致, 并旨在反映现实世界策略部署结果。这将有助于快速且可重复的评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf2987-1e08-4a76-8c1d-bb4939b23d04",
   "metadata": {},
   "source": [
    "# RELATED WORK\n",
    "<!-- **Data scaling in robotics**. Robot learning datasets from automated scripts or human teleoperation have enabled policy learning, with early efforts like RoboTurk [19] and BridgeData [12] offering small-scale datasets with 2.1k and 7.2k trajectories, respectively. Larger datasets, such as RT-1 [14] (130k trajectories), expand scopes yet remain limited to few environments and skills. Open X-Embodiment [6] aggregates various datasets into a unified format, growing to more than 2.4 million trajectories, as a consequence it suffers from significant variability in embodiments, observation perspectives, and inconsistent data quality, limiting its overall effectiveness. More recently, DROID [7] moves towards scaling up scenes for greater diversity by crowd-sourcing demonstrations yet falls short in data scale and quality control. Prior datasets above generally face limitations in data scale, task practicality, and scenario naturalness, compounded by inadequate quality assurance and hardware restrictions, which impedes generalist policy training. As shown in Tab. I, our dataset addresses these gap adequately. We build a data collection facility spanning five scenarios to reconstruct real-world diversity and authenticity. With over 1 million trajectories gathered by skilled teleoperators through rigorous verification protocols, AgiBot World utilizes humanoid robots equipped with visuo-tactile sensors and dexterous hands to enable multimodal demonstrations, setting it apart from previous efforts. Unlike Pumacay et al.[20], which serves as a simulation benchmark for evaluating generalization, what we propose is a full-stack platform with data, models, benchmarks, and ecosystem. -->\n",
    "\n",
    "**机器人中的数据 scaling**. 来自自动脚本或人类远程操作的机器人学习数据集使得策略学习成为可能, 早期的努力如 RoboTurk [19] 和 BridgeData [12] 分别提供了具有 2.1k 和 7.2k 条轨迹的小规模数据集。更大的数据集, 如 RT-1 [14] (130k 条轨迹), 虽然扩大了范围, 但仍然局限于少数环境和技能。Open X-Embodiment [6] 将各种数据集聚合为统一格式, 增加至超过 240 万条轨迹, 因此其在实施、观察视角和数据质量方面存在很大差异, 限制了其整体有效性。最近, DROID [7] 通过众包演示, 扩大场景以增加多样性, 但在数据规模和质量控制方面仍显不足。上述先前的数据集通常面临数据规模、任务实用性和场景自然性的限制, 再加上质量保证不足和硬件限制, 这阻碍了通用策略训练。如表 I 所示, 我们的数据集充分弥补了这些差距。我们构建了一个涵盖五个场景的数据收集设施, 以重建现实世界的多样性和真实性。通过严格的验证协议, 熟练的远程操作员收集了超过 100 万条轨迹, AgiBot World 利用配备视觉触觉传感器和灵巧手的人形机器人进行多模态演示, 使其有别于之前的努力。与 Pumacay et al.[20] 不同, 后者作为评估泛化的模拟基准, 我们所提出的是一个全栈平台, 包含数据、模型、基准和生态系统。\n",
    "\n",
    "<!-- TABLE I: **Comparison to existing datasets**. AgiBot World features the largest number of trajectories to date . We replicate real-world environment at a 1:1 scale for the industrial and retail scenarios, which are barely present before. Extensive human annotations are offered, including item, scene, skill (sub-task segmented), and task-level annotations. Notably, to expand data applicability and potential, we include imperfect data ( i.e.,failure recovery data with annotated error states) and tasks with dexterous hands. To ensure data quality, we adopt a human-in-the-loop philosophy: the policy learning is performed on collected demonstrations. The deployment results are adopted as feedback to improve the collection protocol. -->\n",
    "\n",
    "表 I. **与现有数据集的比较**。AgiBot World 拥有迄今为止最多的轨迹数量。我们以 1:1 的比例复制了现实世界环境, 用于以前几乎不存在的工业和零售场景。提供了广泛的人工标注, 包括物品、场景、技能(子任务分段)和任务级标注。值得注意的是, 为了扩展数据的适用性和潜力, 我们纳入不完美的数据(即带有标注错误状态的失败恢复数据)和灵巧手任务。为了确保数据质量, 我们采用了人机交互的理念: 在收集到的演示上进行策略学习。部署结果被用作反馈来改进收集协议。\n",
    "\n",
    "<!-- **Policy learning at scale**. Robotic foundation models often co-evolve with the development of dataset scale, equipping robots with escalating general-purpose capabilities through diverse, large-scale training. Several prior arts use web-scale video only to facilitate policy learning given the limited scale of action-labeled robot datasets [21], [22], [23]. Another line of work lies in the use of large, end-to-end models trained on robot trajectories with robotics data scaling up [4], [24], [14], [25]. For instance, RDT [10] employs Diffusion Transformers, initially pre-trained on heterogeneous multirobot datasets and fine-tuned on over 6k dual-arm trajectories, showcasing the benefits of pre-training on diverse sources. $\\pi_0$ [26] uses a pre-trained VLM backbone and a flow-based action expert, advancing dexterous manipulation for complex tasks like laundry. LAPA [27] introduces the use of latent actions as pre-training targets; however, its latent planning capability is not preserved for downstream tasks. Building on a variety of innovative ideas from recent research, we advance the field by transferring web-scale knowledge to robotic control through the adaptation of vision-language models (VLMs) with latent actions, leveraging both human videos and robot data for scalable training. Our work demonstrates how the integration of a latent action planner enhances long-horizon task execution and enables more efficient policy learning, significantly improving upon existing generalist policies. -->\n",
    "\n",
    "**大规模策略学习**. 机器人基础模型通常与数据集规模的发展共同发展, 通过多样化的大规模训练为机器人配备不断升级的通用能力。鉴于带有动作标签的机器人数据集规模有限, 一些先前的研究仅使用互联网规模的视频来促进策略学习 [21], [22], [23]。另一类工作集中于使用在机器人轨迹上训练的大型端到端模型, 伴随机器人数据的不断扩展 [4], [24], [14], [25]。例如, <font color=\"red\">RDT [10] 采用 Diffusion Transformers, 最初在异构多机器人数据集上进行预训练, 并在超过 6k 条双臂轨迹上进行微调, 展示了在不同来源上预训练的好处</font>。$\\pi_0$ [26] 使用预训练的 VLM 骨干和基于流的动作专家, 提高了洗衣等复杂任务的灵巧操作。 LAPA [27] 引入了使用 latent actions 作为预训练目标; 然而, 其 latent 规划能力并未保留用于下游任务。基于近期研究中的各种创新思路, 我们通过融合视觉语言模型 (VLM) 和 latent actions, 将互联网规模的知识迁移到机器人控制中, 推动了该领域的发展, 利用人类视频和机器人数据进行可扩展训练。我们的工作展示了 latent action 规划器的集成如何增强长时域任务执行并实现更高效的策略学习, 显著改进了现有的通用策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48967a2b-3ca2-4195-a9b8-32432626eb9e",
   "metadata": {},
   "source": [
    "# AGIBOTWORLD: PLATFORM AND DATA\n",
    "<!-- AgiBot World is a full-stack and open-source embodied intelligence ecosystem. Based on the hardware platform developed by us, AgiBot G1, we construct AgiBot World —— an open-source robot manipulation dataset collected by more than 100 homogeneous robots, providing high-quality data for challenging tasks spanning a wide spectrum of real-life scenarios. The latest version contains 1,001,552 trajectories, with a total duration of 2976.4 hours, covering 217 specific tasks, 87 skills, and 106 scenes. We go beyond basic tabletop tasks such as pick-and-place in lab environments; instead, concentrate on real-world scenarios involving dual-arm manipulation, dexterous hands, and collaborative tasks. AgiBot World aims to provide an inclusive benchmark to drive the future development of advanced and robust algorithms. -->\n",
    "\n",
    "AgiBot World 是一个全栈开源的具身智能生态系统。基于我们开发的硬件平台 AgiBot G1, 我们构建了 AgiBot World ———— 一个由 100 台同质机器人收集的开源机器人操控数据集, 为涵盖广泛现实场景的挑战性任务提供高质量数据。最新版本包含 1,001,552 条轨迹, 总时长 2976.4 小时, 涵盖 217 个具体任务、87 种技能和 106 个场景。我们不限于实验室环境中的拾取和放置等基本桌面任务, 而是专注于涉及双臂操作、灵巧手和协作任务的现实世界场景。AgiBot World 旨在提供一个包容的基准, 以推动先进且稳健算法的未来发展。\n",
    "\n",
    "<!-- We plan to release all resources to enable the community to build upon AgiBot World. **The dataset is available under the CC BY-NC-SA 4.0 license**, along with the model checkpoints, code for data processing and policy training. -->\n",
    "\n",
    "我们计划发布所有资源, 以实现基于 AgiBot World 构建的社区。数据集, 以及模型检查点、数据处理和策略训练的代码, 在 CC BY-NC-SA 4.0 许可下可用。\n",
    "\n",
    "## Hardware: A Versatile Humanoid Robot\n",
    "<!-- The hardware platform is the cornerstone of AgiBot World, determining the lower limit of its quality. The standardization of hardware is also the key to streamlining distributed data collection and ensuring reproducible results. We meticulously develop a novel hardware platform for AgiBot World, distinguished by visuo-tactile sensors, durable 6-DoF dexterous hands with humanoid configuration. -->\n",
    "\n",
    "硬件平台是 AgiBot World 的基石, 决定了其质量的下限。硬件的标准化也是简化分布式数据收集和确保可重复结果的关键。我们为 AgiBot World 精心开发了一种新型硬件平台, 其特点是视觉-触觉传感器、耐用的 6 自由度灵巧手和人形配置。\n",
    "\n",
    "<!-- As illustrated in Fig. 1, our robotic platform features dual 7-DoF arms, a mobile chassis, and an adjustable waist. The end effectors are modular, allowing for the use of either a standard gripper or a 6-DoF dexterous hand, depending on task requirements. For tasks necessitating tactile feedback, a gripper equipped with visuo-tactile sensors is utilized. The robot is outfitted with eight cameras: an RGB-D camera and three fisheye cameras for the front view, RGB-D or fisheye cameras mounted on each end-effector, and two fisheye cameras positioned at the rear. Image observations and proprioceptive states, including joint and end-effector positions, are recorded at a control frequency of 30 Hz. -->\n",
    "\n",
    "如图 1 所示, 我们的机器人平台具有双 7 自由度臂、移动底盘和可调节腰部。末端执行器是模块化的, 可根据任务需求使用标准夹爪或 6 自由度灵巧手。对于需要触觉反馈的任务, 使用配备视觉-触觉传感器的夹爪。机器人配备了八个摄像头: 一个 RGB-D 摄像头和三个鱼眼摄像头用于前视角, 每个末端执行器上安装 RGB-D 或鱼眼摄像头, 以及两个位于后方的鱼眼摄像头。<font color=\"red\">以 30 Hz 的控p制频率记录图像观察和本体感受状态, 包括 joint and end-effector positions</font>。\n",
    "\n",
    "<!-- We employ two teleoperation systems: VR headset control and whole-body motion capture control. The VR controller maps the hand gesture to the end-effector translation and rotation, which is subsequently converted to joint angles through inverse kinematics. The thumbsticks and buttons on the controller enable robot base and body movement, while the trigger buttons control end-effector actuation. However, the VR controller restricts the dexterous hand to only a few predefined gestures. To extensively unlock our robot’s capabilities, we adapt a motion capture system which records the data of human joints, including the fingers, and maps them to robot posture, enabling more nuanced control, including individual finger movements, torso pose, and head orientation. This system provides posture flexibility and execution precision that are required in achieving more complex manipulation tasks. -->\n",
    "\n",
    "我们采用了两种远程操作系统: VR 头戴式控制和全身动作捕捉控制。VR 控制器将手势映射到末端执行器的平移和旋转, 随后通过逆运动学将其转换为关节角度。控制器上的摇杆和按钮实现机器人底座和身体运动, 而触发按钮控制末端执行器的驱动。然而, VR 控制器将灵巧手限制为几个预定义的手势。为了充分发挥机器人的功能, 我们采用了一种动作捕捉系统, 该系统记录人体关节(包括手指)的数据, 并将其映射到机器人姿态, 实现更细微的控制, 包括单个手指运动、躯干姿势和头部方向。该系统提供姿态灵活性和执行精度, 这是实现更复杂操作任务所需的。\n",
    "\n",
    "## Data Collection: Protocol and Quality\n",
    "<!-- The data collection session, as shown in Fig. 2, can be broadly divided into three phases. (1) Before formally commencing data collection, we first conduct preliminary data acquisition to validate the feasibility of each task and establish corresponding collection standards. (2) After feasibility validation and review of the collection standards, skilled teleoperators arrange the initial scene and formally begin data collection according to the established standards. All data undergoes an initial validity verification locally, such as verifying the absence of missing frames. Once the data is confirmed to be complete, it is uploaded to the cloud for the next phase. (3) During post-processing, the data annotators will verify whether each episode meets the collection standards established in phase 1 and provide language annotations. -->\n",
    "\n",
    "数据采集​​环节如图 2 所示, 大致可分为三个阶段。(1) 在正式开始数据采集之前, 我们首先进行初步数据采集, 验证每个任务的可行性, 并建立相应的采集标准。(2) 在可行性验证和采集标准审核后, 熟练的远程操作员布置初始场景, 并根据既定标准正式开始数据采集。所有数据在本地进行初步有效性验证, 如验证是否存在缺失帧。一旦数据被确认是完整的, 它被上传到云端进入下一阶段。(3) 在后处理过程中, 数据标注员将验证每个 episode 是否符合第一阶段建立的采集标准, 并提供语言标注。\n",
    "\n",
    "<!-- Fig. 2: Data collection pipeline. AgiBot World embraces a human-in-the-loop framework to ensure high quality, enriched with detailed annotations and error recovery behaviors. Human feedback plays a critical role not only in post-collection review but also in actively guiding the data collection process, which is largely overlooked in prior efforts. -->\n",
    "\n",
    "图 2: 数据收集流程。AgiBot World 采用人机交互框架, 以确保高质量, 配有详细标注和错误恢复行为。人工反馈不仅在后期收集审查中发挥关键作用, 而且在积极指导数据采集过程中也发挥关键作用, 这在先前的努力中往往被忽视。\n",
    "\n",
    "<!-- **Failure recovery**. During data collection, teleoperators may occasionally commit errors, such as inadvertently dropping objects while manipulating the robotic arms. However, they are often able to recover from these errors and successfully complete the task without requiring a full reconfiguration of the setup. Rather than discarding such trajectories, we retain them and manually annotate each with corresponding failure reasons and timestamps. These trajectories, referred to asfailure recovery data, constitute approximately one percent of the dataset. We consider them invaluable for achieving policy alignment [28] and failure reflection [29], essential for advancing the next generation of robot foundation models. -->\n",
    "\n",
    "**<font color=\"red\">失败恢复</font>**。在数据收集过程中, 远程操作员可能会偶尔犯错, 例如在操作机械臂时不小心掉落物品。然而, 他们通常能够从这些错误中恢复并成功完成任务, 而无需完全重置配置。我们不会丢弃这些轨迹, 而是保留它们并人工标注每个轨迹, 及相应的失败原因和时间戳。这些轨迹被称为失败恢复数据, 大约占数据集的百分之一。我们认为它们对于实现策略对齐 [28] 和失败反思 [29] 非常有价值, 这对于推进下一代机器人基础模型至关重要。\n",
    "\n",
    "<!-- **Human-in-the-loop**. Concurrent with feedback collection from data annotators, we adopt a human-in-the-loop approach to assess and refine data quality. This process involves an iterative cycle of collecting a small set of demonstrations, training a policy, and deploying the resulting policy to evaluate data availability. Based on the policy’s performance, we iteratively refine the data collection pipeline to address identified gaps or inefficiencies. For instance, during real-world deployment, the model exhibits prolonged pauses at the onset of actions, aligning with data annotator feedback highlighting inconsistent transitions and excessive idle time in the collected data. In response, we revise the data collection protocols and introduce a post-processing step to eliminate idle frames, thereby enhancing the dataset’s overall utility for policy learning. This feedback-driven methodology ensures continuous improvement in data quality. -->\n",
    "\n",
    "**人机协作**。在收集数据标注员反馈的同时, 我们采用人机协作方法来评估和改进数据质量。这个过程涉及一个迭代循环, 包括收集一小部分演示、训练一个策略, 并部署生成的策略以评估数据可用性。根据策略的性能, 我们迭代改进数据收集流水线, 以解决识别出的差距或低效。例如, 在实际部署过程中, 模型在开始执行操作时表现出长时间的停顿, 这与数据标注员反馈指出的收集数据中不一致的转换和过多的空闲时间相一致。作为回应, 我们修改数据采集协议并引入后处理步骤以消除空闲帧, 从而增强数据集在策略学习中的整体效用。这种反馈驱动的方法确保数据质量的持续改进。\n",
    "\n",
    "## Dataset Statistics and Analysis: Beyond Scale\n",
    "<!-- AgiBot World is developed through a large-scale data collection facility, which spans over 4,000 square meters. This extensive environment contains over 3,000 unique objects in a variety of scenes, meticulously designed to reflect real-world settings. The dataset covers a wide range of scenarios and scene setups, ensuring both scale and diversity in the pursuit of generalizable robot policy. -->\n",
    "\n",
    "AgiBot World 是通过一个占地超过 4,000 平方米的大型数据收集设施开发的。这个广阔的环境包含各种场景中的 3,000 多个独特物体, 经过精心设计以反映现实世界配置。数据集涵盖广泛的场景和场景配置, 确保在追求可推广的机器人策略时兼具规模和多样性。\n",
    "\n",
    "<!-- **Reconstructing the diversity of the real world**. Key statistics of our dataset are presented in Fig. 3. AgiBot World provides extensive coverage across five key domains: domestic, retail, industrial, restaurant, and office environments. Within each domain, we further define specific scene categories. For instance, the domestic domain includes detailed environments such as bedrooms, kitchens, living rooms, and balconies, while the retail domain features distinct areas like shelving units and fresh produce sections. Our dataset also features over 3,000 distinct objects, systematically categorized across various scenes. These objects span a wide range of everyday items, including food, furniture, clothing, electronic devices, and more. The distribution of object categories, as illustrated in Fig. 3(a), highlights the relative frequency of different object types within each scene. -->\n",
    "\n",
    "**重建现实世界的多样性**。我们数据集的关键统计信息如图 3 所示。AgiBot World 广泛覆盖五个关键领域: 家庭、零售、工业、餐厅和办公环境。在每个领域中, 我们进一步定义了具体的场景类别。例如, 家庭领域包括卧室、厨房、客厅和阳台等详细环境, 零售领域包括货架单元和新鲜农产品区等独特区域。我们的数据集还包含 3,000 多个不同的物体, 这些物体在各个场景中按系统分类。这些物体涵盖了广泛的日常用品, 包括食物、家具、衣服、电子设备等。如图 3(a) 所示, 物体类别的分布突出显示了每个场景中不同物体类型的相对频率。\n",
    "\n",
    "<!-- Fig. 3: Dataset Statistics. a) AgiBot World dataset covers the vast majority of robotic application scenarios, as well as a wide range of interactive objects. b)Our dataset features long-horizon tasks, with the majority of trajectories ranging from 30s to 60s. In contrast, widely used datasets, such as DROID, primarily consist of trajectories ranging from 5s to 20s, while OXE v1.0 predominantly contains trajectories within 5s. c)AgiBot World dataset focuses on valuable atomic skills, spanning a wide spectrum of skills, each supported by a minimum of 100 trajectories (red dashed line above). -->\n",
    "\n",
    "图 3: 数据集统计. a) AgiBot World 数据集涵盖了绝大多数机器人应用场景, 以及广泛的交互对象。b) 我们的数据集包含长时域任务, 大多数轨迹范围为 30 秒至 60 秒。相比之下, 广泛使用的数据集(例如 DROID)主要由 5 秒至 20 秒的轨迹组成, 而 OXE v1.0 主要包含 5 秒内的轨迹。c) AgiBot World 数据集专注于有价值的原子技能, 涵盖了广泛的技能, 每种技能至少由 100 条轨迹支持(上方的红色虚线)。\n",
    "\n",
    "<!-- **Long-horizon manipulation**. A distinguishing feature of the AgiBot World dataset is its emphasis on long-horizon manipulation. As shown in Fig. 3(b), prior datasets predominantly focus on tasks involving single atomic skills, with most trajectories lasting no more than 5 seconds. In contrast, AgiBot World is built upon continuous and complete tasks composed by multiple atomic skills, like “make a coffee”. Trajectories in our dataset typically span approximately 30 seconds, some of which last over 2 minutes. We also provide key-frame and instruction annotation for each sub-step to facilitate policy learning in such challenging scenarios. -->\n",
    "\n",
    "**长时域操作**. AgiBot World 数据集的一个显著特点是它强调长时域操作。如图 3(b) 所示, 之前的数据集主要聚焦于涉及单个原子技能的任务, 大多数轨迹持续时间不超过 5 秒。相比之下, AgiBot World 建立在由多个原子技能组成的连续完整的任务之上, 例如“制作咖啡”。我们数据集中的轨迹通常持续约 30 秒, 其中一些持续超过 2 分钟。我们还为每个子步骤提供关键帧和指令注释, 以促进在这种具有挑战性场景中的策略学习。\n",
    "\n",
    "<!-- **Comprehensive skill coverage**. In terms of task design, while generic atomic skills, such as “pick-and-place”, dominate the majority of tasks, we have intentionally incorporated tasks that emphasize less frequently used but highly valuable skills, such as “chop” and “plug” (as shown in Fig. 3(c)). This ensures that our dataset adequately represents a broad spectrum of skills, providing sufficient data for each to support robust policy learning. -->\n",
    "\n",
    "**全面的技能覆盖**. 在任务设计方面, 尽管通用原子技能, 如“拾取和放置”, 占据了大多数任务, 但我们有意纳入一些任务, 这个任务注重不常用但非常有价值的技能，如 “chop” 和 “plug”, 如图 3(c) 所示。这确保我们的数据集充分代表了广泛的技能, 为每种技能提供足够的数据, 以支持稳健的策略学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e2651-bbd0-4b06-872d-128f070f629b",
   "metadata": {},
   "source": [
    "# AGIBOTWORLD: MODEL\n",
    "<!-- To effectively utilize our high-quality AgiBot World dataset and enhance the policy’s generalizability, we propose a hierarchical Vision-Language-Latent-Action (ViLLA) framework with three training stages, as depicted in Fig. 4. Compared to Vision-Language-Action (VLA) model where action is vision-language conditioned, the ViLLA model predicts latent action tokens, conditioned on the generation of subsequent robot control actions. -->\n",
    "\n",
    "为了有效利用我们高质量的 AgiBot World 数据集并增强策略的通用性, 我们提出了一个具有三个训练阶段的分层 Vision-Language-Latent-Action (ViLLA) 框架, 如图 4 所示。与视觉-语言-动作 (VLA) 模型(其中动作以视觉语言为条件)相比, ViLLA 模型以后续机器人控制动作的生成为条件来预测 latent action tokens。\n",
    "\n",
    "<!-- In Stage 1, we project consecutive images into a latent action space by training an encoder-decoder latent action model (LAM) on internet-scale heterogeneous data. This allows the latent action to serve as an intermediate representation, bridging the gap between general image-text inputs and robotic actions. In Stage 2, these latent actions act as pseudo-labels for the latent planner, facilitating embodiment-agnostic long-horizon planning and leveraging the generalizability of the pre-trained VLM. Finally, in Stage 3, we introduce the action expert and jointly train it with the latent planner to support the learning of dexterous manipulation. -->\n",
    "\n",
    "在第 1 阶段, 我们通过在互联网规模的异构数据上训练编码器-解码器 latent action model (LAM), 将连续图像投射到 latent action 空间。这使得 latent action 可以作为中间表示, 弥合通用图像文本输入和机器人动作之间的差距。在第 2 阶段, 这些 latent actions 充当 latent 规划器的伪标签, 便于与具身无关的长期规划并利用预训练 VLM 的通用性。最后, 在第 3 阶段, 我们引入动作专家并将其与 latent 规划器联合训练, 以支持灵巧操作的学习。\n",
    "\n",
    "## Latent Action Model\n",
    "<!-- Despite considerable advancements in gathering diverse robot demonstrations, the volume of action-labeled robot data remains limited relative to web-scale datasets. To broaden the data pool by incorporating internet-scale human videos lacking action labels and cross-embodiment robot data, we employ latent actions [30] in Stage 1 to model the inverse dynamics of consecutive frames. This approach enables the transfer of real-world dynamics from heterogeneous data sources into universal manipulation knowledge. -->\n",
    "\n",
    "尽管在收集多样化的机器人演示方面取得了显著进展, 但相对于互联网规模的数据集, 带有动作标签的机器人数据量仍然有限。为了通过整合缺乏动作标签的互联网规模的人类视频和跨实体机器人数据来扩大数据池, 我们在第 1 阶段采用 latent actions [30] 来建模连续帧的逆动力学。这种方法可以将现实世界的动态从异构数据源转移到通用操纵知识中。\n",
    "\n",
    "<!-- To extract latent actions from video frames $\\{I_t,I_{t+H} \\}$, the latent action model is constructed around an inverse dynamics model-based encoder $\\mathbf{I}(z_t|I_t,I_{t+H})$ and a forward dynamics model-based decoder $\\mathbf{F}(I_{t+H}|I_t,z_t)$. The encoder employs a spatial-temporal transformer [31] with casual temporal masks following Bruce et al. [30], while the decoder is a spatial transformer that takes the initial frame and discretized latent action tokens $z_t = [z^0_t, \\dots ,z^{k−1}_t]$ as input, with $k$ set to 4. The latent action tokens are quantized using a VQ-VAE objective [32], with a codebook of size $|C|$. -->\n",
    "\n",
    "为了从视频帧 $\\{I_t,I_{t+H} \\}$ 中提取 latent actions, latent action 模型由一个基于逆动力学模型的编码器 $\\mathbf{I}(z_t|I_t,I_{t+H})$ 和一个基于前向动力学模型的解码器 $\\mathbf{F}(I_{t+H}|I_t,z_t)$ 构建。编码器采用时空 transformer [31], 并遵循 Bruce et al. [30] 的因果时间掩码, 而解码器是一个空间 transformer, 以初始帧和离散的 latent action tokens $z_t = [z^0_t, \\dots ,z^{k−1}_t]$ 作为输入, $k$ 设置为 4。使用 VQ-VAE 目标 [32] 量化 latent action tokens, 代码本大小为 $|C|$。\n",
    "                                   \n",
    "## Latent Planner\n",
    "<!-- With the aim of establishing a solid foundation for scene and object understanding and general reasoning ability, the ViLLA model harnesses a VLM pre-trained on web-scale vision-language data and incorporates a latent planner for embodiment-agnostic planning within the latent action space. We use InternVL2.5-2B [33] as the VLM backbone due to its strong transfer learning capabilities. The two-billion parameter scale has proven effective for robotic tasks in our preliminary experiments, as well as in prior studies [10], [26]. Multiview image observations are first encoded using InternViT before being projected into the language space. The latent planner consists of 24 transformer layers, which enable layer-by-layer conditioning from the VLM backbone with full bidirectional attention. -->\n",
    "\n",
    "为了奠定场景和物体理解以及一般推理能力的坚实基础, ViLLA 模型利用在互联网规模的视觉语言数据上预训练的 VLM, 并耦合一个 latent 规划器, 用于 latent action 空间上的具身无关的规划。我们使用 InternVL2.5-2B [33] 作为 VLM 骨干, 因为其强大的迁移学习能力。在我们的初步实验以及先前的研究 [10]、[26] 中, 20 亿参数规模已被证明对机器人任务是有效的。首先使用 InternViT 对多视角图像观测进行编码, 然后将其投影到语言空间中。latent 规划器由 24 个 transformer 层组成, 它们能够从 VLM 骨干进行逐层调节, 并具有完全双向注意力。\n",
    "\n",
    "<!-- Specifically, given multiview input images $(I^h_t, I^l_t, I^r_t)$ (typically from the head, left wrist, and right wrist) at timestep $t$, along with a language instruction $l$ describing the ongoing task, the latent planner predicts latent action tokens: $\\mathbf{P}(z_t|I^h_t,I^l_t,I^r_t,l)$, with supervision produced by the LAM encoder based on the head view: $z_t := \\mathbf{I}(I^h_t,I^h_{t+H})$. Since the latent action space is orders of magnitude smaller than the discretized low-level actions used in OpenVLA [4], this approach also facilitates the efficient adaptation of general-purpose VLMs into robot policies. -->\n",
    "\n",
    "具体而言, 给定时间步长 $t$ 的多视角输入图像 $(I^h_t, I^l_t, I^r_t)$ (通常来自头部、左手腕和右手腕), 以及描述当前任务的语言指令 $l$, latent 规划器预测 latent action tokens: $\\mathbf{P}(z_t|I^h_t,I^l_t,I^r_t,l)$, 其中 LAM 编码器基于头部视角生成监督: $z_t := \\mathbf{I}(I^h_t,I^h_{t+H})$。由于 latent action 空间比 OpenVLA [4] 中使用的离散化下层动作小几个数量级, 这种方法也有助于将通用 VLM 高效适配到机器人策略。\n",
    "\n",
    "## Action Expert\n",
    "<!-- To achieve high-frequency and dexterous manipulation, Stage 3 integrates an action expert that utilizes a diffusion objective to model the continuous distribution of low-level actions [34]. Although the action expert shares the same architectural framework as the latent planner, their objectives diverge: the latent planner generates discretized latent action tokens through masked language modeling, while the action expert regresses low-level actions via an iterative denoising process. Both expert modules are conditioned hierarchically on preceding modules, including the action expert itself, ensuring coherent integration and information flow within the dual-expert system. -->\n",
    "\n",
    "为了实现高频和灵巧的操作, 第 3 阶段集成一个动作专家, 该专家利用扩散目标来建模下层动作的连续分布 [34]。尽管动作专家与 latent 规划器共享相同的架构框架, 但它们的目标有所不同: latent 规划器通过掩码语言建模生成离散化的 latent action tokens, 而动作专家通过迭代去噪过程回归下层动作。这两个专家模块在层次上都以先前的模块(包括动作专家自身)为条件, 从而确保双专家系统内的连贯集成和信息流。\n",
    "\n",
    "<!-- The action expert decodes low-level action chunks, de-noted by $A_t= [a_t,a_{t+1}, \\dots,a_{t+H}]$ with $H=30$, using proprioceptive state $p_t$ over an interval of $H$ timesteps: $\\mathbf{A}(A_t|I^h_t,I^l_t,I^r_t,p_t,l)$. During inference, the VLM, latent planner, and action expert are synergistically combined within the generalist policy GO-1, which initially predicts $k$ latent action tokens and subsequently conditions the denoising process to produce the final control signals. -->\n",
    "\n",
    "动作专家使用本体感知状态 $p_t$在 $H$ 个时间步内($\\mathbf{A}(A_t|I^h_t,I^l_t,I^r_t,p_t,l)$), 解码下层 action chunks, 表示为 $A_t= [a_t,a_{t+1}, \\dots,a_{t+H}]$, 其中 $H=30$。在推理过程中, VLM、latent 规划器和动作专家在通用策略 GO-1 中协同结合, 首先预测 $k$ 个 latent action tokens, 随后调节去噪过程以生成最终的控制信号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac6f3db-b4b3-4b34-b3b0-10843fdc22fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cfz/miniconda3/envs/learning/lib/python3.10/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent efforts, such as Open X-Embodiment (OXE) [6],\n",
      "have addressed by aggregating and standardizing exist-\n",
      "ing datasets. Despite advancements on large-scale cross-\n",
      "embodiment learning, the resulting policy is constrained\n",
      "within naive, short-horizon tasks and can weakly generalize\n",
      "to out-of-domain scenarios [4]. DROID [7] collected expert\n",
      "data through crowd-sourcing from diverse real-life scenes.\n",
      "The absence of data quality assurance (with human feedback)\n",
      "and the reliance on a constrained hardware setup ( i.e.,featur-\n",
      "ing fixed, single-arm robots), limit its real-world applicability\n",
      "and broader effectiveness. More recently, Lin et al. [8]\n",
      "explored scaling laws governing generalizability across intra-\n",
      "category objects and environments, albeit limited to a few\n",
      "simple, single-step tasks. These efforts represent a notable\n",
      "advancement toward developing generalist policies, moving\n",
      "beyond the traditional focus on single-task learning within\n",
      "narrow domains [9], [3]. Nevertheless, existing robot learning\n",
      "datasets remain constrained by their reliance on short-horizon\n",
      "tasks in highly controlled laboratory environments, failing\n",
      "to adequately capture the complexity and diversity inher-\n",
      "ent in real-world manipulation tasks. To achieve general-\n",
      "purpose robotic intelligence, it is essential to develop datasets\n",
      "that scale in size and diversity while capturing real-world\n",
      "variability, supported by general-purpose humanoid robots\n",
      "for robust skill acquisition, a standardized data collection\n",
      "pipeline with assured quality, and carefully curated tasks\n",
      "reflecting real-world challenges.\n",
      "As depicted in Fig. 1, we introduce AgiBot World\n",
      "Colosseo , a full-stack large-scale robot learning platform\n",
      "curated for advancing bimanual manipulation in scal-\n",
      "able and intelligent embodied systems. A full-scale 4000-\n",
      "square-meter facility is constructed to represent five major\n",
      "domains—domestic, retail, industrial, restaurant, and office\n",
      "environment—all dedicated to high-fidelity data collection in\n",
      "authentic everyday scenarios. With over 1 million trajectories\n",
      "collected from 100 real robots, AgiBot World offers un-\n",
      "precedented diversity and complexity. It spans over 100 real-\n",
      "world scenarios, addressing challenging tasks such as fine-\n",
      "grained manipulation, tool usage, and multi-robot synergistic\n",
      "collaboration. Unlike prior datasets, AgiBot World dataset\n",
      "collection is carried out with a fully standardized pipeline,\n",
      "ensuring high data quality and scalability, while incorporat-\n",
      "ing human-in-the-loop verification to guarantee reliability.\n",
      "Our hardware setup includes mobile base humanoid robots\n",
      "with whole-body control, dexterous hands, and visuo-tactile\n",
      "sensors, enabling rich, multimodal data collection. Each\n",
      "episode is meticulously designed, featuring multiple camera\n",
      "views, depth information, camera calibration, and language\n",
      "annotations for both the overall task and each individual\n",
      "sub-steps. This well-rounded hardware setup, combined with\n",
      "various long-horizon, real-world tasks, opens new avenues\n",
      "for developing next-generation generalist policies and fosters\n",
      "diverse future research in robotics.\n",
      "Our experimental results highlight the transformative po-\n",
      "tential of the AgiBot World dataset. Policies pre-trained on\n",
      "our dataset achieve an average success rate improvement\n",
      "of 30% compared to those trained on the prior large-scalerobot dataset OXE [6]. Notably, even when utilizing only\n",
      "a fraction of our dataset—equivalent to 1/10 of the data\n",
      "volume in hours compared to OXE—the generalizability\n",
      "of pretrained policies is elevated by 18%. These findings\n",
      "underscore the dataset’s efficacy in bridging the gap between\n",
      "controlled laboratory environments and real-world robotic\n",
      "applications. Following our dataset, to address the limitations\n",
      "of previous robot foundation models that heavily rely on in-\n",
      "domain robot datasets, we present Genie Operator-1 (GO-\n",
      "1), a novel generalist policy that utilizes latent action rep-\n",
      "resentations to enable learning from heterogeneous data and\n",
      "efficiently bridges general-purpose vision-language models\n",
      "(VLMs) with robotic sequential decision-making. Through\n",
      "unified pre-training on web-scale data, spanning human\n",
      "videos to our high-quality robot dataset, GO-1 achieves\n",
      "superior generalization and dexterity, outperforming prior\n",
      "generalist policies such as RDT [10] and our variant without\n",
      "latent action planner. Moreover, we demonstrate that GO-\n",
      "1’s performance exhibits robust scalability with increasing\n",
      "dataset size, underscoring its potential for sustained advance-\n",
      "ment as larger datasets become available.\n",
      "Beyond its immediate impact, AgiBot World lays a strong\n",
      "foundation for future research in robotic manipulation. By\n",
      "open-sourcing the dataset, toolchain, and pre-trained models,\n",
      "we aim to foster community-wide innovation, enabling re-\n",
      "searchers to explore more authentic and diverse applications\n",
      "from household assistant to industrial automation. AgiBot\n",
      "World is more than yet another dataset; it is a step toward\n",
      "scalable, general-purpose robotic intelligence, empowering\n",
      "robots to tackle the complexities of the real world.\n",
      "Contribution. 1) We construct AgiBot World dataset, a\n",
      "multifarious robot learning dataset accompanied by open-\n",
      "source tools to advance research on policy learning at scale.\n",
      "As a pioneering initiative, AgiBot World employs an in-\n",
      "clusive optimized pipeline, from scene configuration, task\n",
      "design, data collection, to human-in-the-loop verification,\n",
      "which ensures unparalleled data quality. 2) We propose GO-\n",
      "1, a robot foundation policy using latent action represen-\n",
      "tations to unlock web-scale pre-training on heterogeneous\n",
      "data. Empowered by AgiBot World dataset, it outperforms\n",
      "prior generalist policies in generalization and dexterity, with\n",
      "performance scaling predictably with dataset size.\n",
      "Limitation. All evaluations are conducted in real-world\n",
      "scenarios. We are currently developing the simulation en-\n",
      "vironment, aligning with the real-world setup and aiming\n",
      "to reflect real-world policy deployment outcome. It would\n",
      "thereby facilitate fast and reproducible evaluation.\n",
      "II. R ELATED WORK\n",
      "Data scaling in robotics. Robot learning datasets from\n",
      "automated scripts or human teleoperation have enabled pol-\n",
      "icy learning, with early efforts like RoboTurk [19] and\n",
      "BridgeData [12] offering small-scale datasets with 2.1k and\n",
      "7.2k trajectories, respectively. Larger datasets, such as RT-\n",
      "1 [14] (130k trajectories), expand scopes yet remain limited\n",
      "to few environments and skills. Open X-Embodiment [6]\n",
      "aggregates various datasets into a unified format, growing\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "pdf_path = \"/mnt/y/paper/AgiBot_GO-1.pdf\"\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[1]\n",
    "text = page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcc6da-54fd-4728-abfd-f40f47280e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

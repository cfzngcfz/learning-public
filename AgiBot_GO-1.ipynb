{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3dadd9-2407-455f-ac00-9ad7992eb405",
   "metadata": {},
   "source": [
    "[AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems](https://agibot-world.com/blog/go1)\n",
    "\n",
    "[github](https://github.com/OpenDriveLab/Agibot-World)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80465b5-1093-4bd8-a794-37039a5fd3e6",
   "metadata": {},
   "source": [
    "## RELATED WORK\n",
    "<!-- **Data scaling in robotics**. Robot learning datasets from automated scripts or human teleoperation have enabled policy learning, with early efforts like RoboTurk [19] and BridgeData [12] offering small-scale datasets with 2.1k and 7.2k trajectories, respectively. Larger datasets, such as RT-1 [14] (130k trajectories), expand scopes yet remain limited to few environments and skills. Open X-Embodiment [6] aggregates various datasets into a unified format, growing to more than 2.4 million trajectories, as a consequence it suffers from significant variability in embodiments, observation perspectives, and inconsistent data quality, limiting its overall effectiveness. More recently, DROID [7] moves towards scaling up scenes for greater diversity by crowd-sourcing demonstrations yet falls short in data scale and quality control. Prior datasets above generally face limitations in data scale, task practicality, and scenario naturalness, compounded by inadequate quality assurance and hardware restrictions, which impedes generalist policy training. As shown in Tab. I, our dataset addresses these gap adequately. We build a data collection facility spanning five scenarios to reconstruct real-world diversity and authenticity. With over 1 million trajectories gathered by skilled teleoperators through rigorous verification protocols, AgiBot World utilizes humanoid robots equipped with visuo-tactile sensors and dexterous hands to enable multimodal demonstrations, setting it apart from previous efforts. Unlike Pumacay et al.[20], which serves as a simulation benchmark for evaluating generalization, what we propose is a full-stack platform with data, models, benchmarks, and ecosystem. -->\n",
    "\n",
    "**机器人中的数据 scaling**. 来自自动脚本或人类远程操作的机器人学习数据集使得策略学习成为可能, 早期的努力如 RoboTurk [19] 和 BridgeData [12] 分别提供了具有 2.1k 和 7.2k 条轨迹的小规模数据集。更大的数据集, 如 RT-1 [14] (130k 条轨迹), 虽然扩大了范围, 但仍然局限于少数环境和技能。Open X-Embodiment [6] 将各种数据集聚合为统一格式, 增加至超过 240 万条轨迹, 因此其在实施、观察视角和数据质量方面存在很大差异, 限制了其整体有效性。最近, DROID [7] 通过众包演示, 扩大场景以增加多样性, 但在数据规模和质量控制方面仍显不足。上述先前的数据集通常面临数据规模、任务实用性和场景自然性的限制, 再加上质量保证不足和硬件限制, 这阻碍了通用策略训练。如表 1 所示, 我们的数据集充分弥补了这些差距。我们构建了一个涵盖五个场景的数据收集设施, 以重建现实世界的多样性和真实性。通过严格的验证协议, 熟练的远程操作员收集了超过 100 万条轨迹, AgiBot World 利用配备视觉触觉传感器和灵巧手的人形机器人进行多模态演示, 使其有别于之前的努力。与 Pumacay et al.[20] 不同, 后者作为评估泛化的模拟基准, 我们所提出的是一个全栈平台, 包含数据、模型、基准和生态系统。\n",
    "\n",
    "**Policy learning at scale**. Robotic foundation models often co-evolve with the development of dataset scale, equipping robots with escalating general-purpose capabilities through diverse, large-scale training. Several prior arts use web-scale video only to facilitate policy learning given the limited scale of action-labeled robot datasets [21], [22], [23]. Another line of work lies in the use of large, end-to-end models trained on robot trajectories with robotics data scaling up [4], [24], [14], [25]. For instance, RDT [10] employs Diffusion Transformers, initially pre-trained on heterogeneous multirobot datasets and fine-tuned on over 6k dual-arm trajectories, showcasing the benefits of pre-training on diverse sources. $\\pi_0$ [26] uses a pre-trained VLM backbone and a flow-based action expert, advancing dexterous manipulation for complex tasks like laundry. LAPA [27] introduces the use of latent actions as pre-training targets; however, its latent planning capability is not preserved for downstream tasks. Building on a variety of innovative ideas from recent research, we advance the field by transferring web-scale knowledge to robotic control through the adaptation of vision-language models (VLMs) with latent actions, leveraging both human videos and robot data for scalable training. Our work demonstrates how the integration of a latent action planner enhances long-horizon task execution and enables more efficient policy learning, significantly improving upon existing generalist policies.\n",
    "\n",
    "**大规模策略学习**。机器人基础模型通常与数据集规模的发展共同发展，通过多样化的大规模训练为机器人配备不断升级的通用能力。鉴于动作标记机器人数据集的规模有限，一些现有技术仅使用网络规模视频来促进策略学习 [21]、[22]、[23]。另一项工作是使用大型端到端模型，这些模型在机器人轨迹上进行训练，机器人数据不断扩展 [4]、[24]、[14]、[25]。例如，RDT [10] 采用了 Diffusion Transformers，最初在异构多机器人数据集上进行了预训练，并在超过 6k 个双臂轨迹上进行了微调，展示了在不同来源上进行预训练的好处。$\\pi_0$ [26] 使用预先训练的 VLM 主干和基于流程的动作专家，提高了洗衣等复杂任务的灵巧操作能力。 LAPA [27] 引入了使用潜在动作作为预训练目标；但是，其潜在规划能力并未保留用于下游任务。基于近期研究中的各种创新想法，我们通过调整视觉语言模型 (VLM) 和潜在动作，将网络规模的知识转移到机器人控制中，从而推动了该领域的发展，利用人类视频和机器人数据进行可扩展的训练。我们的工作展示了如何集成潜在动作规划器来增强长期任务执行并实现更高效的策略学习，从而显著改进现有的通用策略。\n",
    "\n",
    "\n",
    "大规模策略学习。机器人基础模型通常与数据集规模的发展共同演进，通过多样化的大规模训练为机器人提供日益增强的通用能力。一些先前的研究仅利用网络规模的视频来促进策略学习，因为带有动作标签的机器人数据集规模有限 [21]、[22]、[23]。另一类工作则集中于使用在机器人轨迹上训练的大型端到端模型，伴随机器人数据的扩展 [4]、[24]、[14]、[25]。例如，RDT [10] 采用扩散变换器，最初在异构的多机器人数据集上进行预训练，并在超过 6k 条双臂轨迹上微调，展示了在多样化源上预训练的好处。$\\pi_0$ [26] 使用了预训练的 VLM 骨干和基于流的动作专家，推动了复杂任务（如洗衣）的灵巧操作。LAPA [27] 引入了将潜在动作作为预训练目标的概念；然而，其潜在规划能力未能保留用于下游任务。基于近期研究中的多种创新思路，我们通过将网络规模知识转移到机器人控制中，推进了这一领域，适应了具有潜在动作的视觉-语言模型（VLM），利用人类视频和机器人数据进行可扩展训练。我们的工作展示了潜在动作规划器的集成如何增强长期任务执行并提高策略学习的效率，显著改善了现有的通用策略。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e2651-bbd0-4b06-872d-128f070f629b",
   "metadata": {},
   "source": [
    "# AGIBOTWORLD: MODEL\n",
    "<!-- To effectively utilize our high-quality AgiBot World dataset and enhance the policy’s generalizability, we propose a hierarchical Vision-Language-Latent-Action (ViLLA) framework with three training stages, as depicted in Fig. 4. Compared to Vision-Language-Action (VLA) model where action is vision-language conditioned, the ViLLA model predicts latent action tokens, conditioned on the generation of subsequent robot control actions. -->\n",
    "\n",
    "为了有效利用我们高质量的 AgiBot World 数据集并增强策略的通用性, 我们提出了一个具有三个训练阶段的分层 Vision-Language-Latent-Action (ViLLA) 框架, 如图 4 所示。与视觉-语言-动作 (VLA) 模型(其中动作以视觉语言为条件)相比, ViLLA 模型以后续机器人控制动作的生成为条件来预测 latent action tokens。\n",
    "\n",
    "<!-- In Stage 1, we project consecutive images into a latent action space by training an encoder-decoder latent action model (LAM) on internet-scale heterogeneous data. This allows the latent action to serve as an intermediate representation, bridging the gap between general image-text inputs and robotic actions. In Stage 2, these latent actions act as pseudo-labels for the latent planner, facilitating embodiment-agnostic long-horizon planning and leveraging the generalizability of the pre-trained VLM. Finally, in Stage 3, we introduce the action expert and jointly train it with the latent planner to support the learning of dexterous manipulation. -->\n",
    "\n",
    "在第 1 阶段, 我们通过在互联网规模的异构数据上训练编码器-解码器 latent action model (LAM), 将连续图像投射到 latent action 空间。这使得 latent action 可以作为中间表示, 弥合通用图像文本输入和机器人动作之间的差距。在第 2 阶段, 这些 latent actions 充当 latent 规划器的伪标签, 便于与具身无关的长期规划并利用预训练 VLM 的通用性。最后, 在第 3 阶段, 我们引入动作专家并将其与 latent 规划器联合训练, 以支持灵巧操作的学习。\n",
    "\n",
    "## Latent Action Model\n",
    "<!-- Despite considerable advancements in gathering diverse robot demonstrations, the volume of action-labeled robot data remains limited relative to web-scale datasets. To broaden the data pool by incorporating internet-scale human videos lacking action labels and cross-embodiment robot data, we employ latent actions [30] in Stage 1 to model the inverse dynamics of consecutive frames. This approach enables the transfer of real-world dynamics from heterogeneous data sources into universal manipulation knowledge. -->\n",
    "\n",
    "尽管在收集多样化的机器人演示方面取得了显著进展, 但相对于互联网规模的数据集, 带有动作标签的机器人数据量仍然有限。为了通过整合缺乏动作标签的互联网规模的人类视频和跨实体机器人数据来扩大数据池, 我们在第 1 阶段采用 latent actions [30] 来建模连续帧的逆动力学。这种方法可以将现实世界的动态从异构数据源转移到通用操纵知识中。\n",
    "\n",
    "<!-- To extract latent actions from video frames $\\{I_t,I_{t+H} \\}$, the latent action model is constructed around an inverse dynamics model-based encoder $\\mathbf{I}(z_t|I_t,I_{t+H})$ and a forward dynamics model-based decoder $\\mathbf{F}(I_{t+H}|I_t,z_t)$. The encoder employs a spatial-temporal transformer [31] with casual temporal masks following Bruce et al. [30], while the decoder is a spatial transformer that takes the initial frame and discretized latent action tokens $z_t = [z^0_t, \\dots ,z^{k−1}_t]$ as input, with $k$ set to 4. The latent action tokens are quantized using a VQ-VAE objective [32], with a codebook of size $|C|$. -->\n",
    "\n",
    "为了从视频帧 $\\{I_t,I_{t+H} \\}$ 中提取 latent actions, latent action 模型由一个基于逆动力学模型的编码器 $\\mathbf{I}(z_t|I_t,I_{t+H})$ 和一个基于前向动力学模型的解码器 $\\mathbf{F}(I_{t+H}|I_t,z_t)$ 构建。编码器采用时空 transformer [31], 并遵循 Bruce et al. [30] 的因果时间掩码, 而解码器是一个空间 transformer, 以初始帧和离散的 latent action tokens $z_t = [z^0_t, \\dots ,z^{k−1}_t]$ 作为输入, $k$ 设置为 4。使用 VQ-VAE 目标 [32] 量化 latent action tokens, 代码本大小为 $|C|$。\n",
    "                                   \n",
    "## Latent Planner\n",
    "<!-- With the aim of establishing a solid foundation for scene and object understanding and general reasoning ability, the ViLLA model harnesses a VLM pre-trained on web-scale vision-language data and incorporates a latent planner for embodiment-agnostic planning within the latent action space. We use InternVL2.5-2B [33] as the VLM backbone due to its strong transfer learning capabilities. The two-billion parameter scale has proven effective for robotic tasks in our preliminary experiments, as well as in prior studies [10], [26]. Multiview image observations are first encoded using InternViT before being projected into the language space. The latent planner consists of 24 transformer layers, which enable layer-by-layer conditioning from the VLM backbone with full bidirectional attention. -->\n",
    "\n",
    "为了奠定场景和物体理解以及一般推理能力的坚实基础, ViLLA 模型利用在互联网规模的视觉语言数据上预训练的 VLM, 并耦合一个 latent 规划器, 用于 latent action 空间上的具身无关的规划。我们使用 InternVL2.5-2B [33] 作为 VLM 骨干, 因为其强大的迁移学习能力。在我们的初步实验以及先前的研究 [10]、[26] 中, 20 亿参数规模已被证明对机器人任务是有效的。首先使用 InternViT 对多视角图像观测进行编码, 然后将其投影到语言空间中。latent 规划器由 24 个 transformer 层组成, 它们能够从 VLM 骨干进行逐层调节, 并具有完全双向注意力。\n",
    "\n",
    "<!-- Specifically, given multiview input images $(I^h_t, I^l_t, I^r_t)$ (typically from the head, left wrist, and right wrist) at timestep $t$, along with a language instruction $l$ describing the ongoing task, the latent planner predicts latent action tokens: $\\mathbf{P}(z_t|I^h_t,I^l_t,I^r_t,l)$, with supervision produced by the LAM encoder based on the head view: $z_t := \\mathbf{I}(I^h_t,I^h_{t+H})$. Since the latent action space is orders of magnitude smaller than the discretized low-level actions used in OpenVLA [4], this approach also facilitates the efficient adaptation of general-purpose VLMs into robot policies. -->\n",
    "\n",
    "具体而言, 给定时间步长 $t$ 的多视角输入图像 $(I^h_t, I^l_t, I^r_t)$ (通常来自头部、左手腕和右手腕), 以及描述当前任务的语言指令 $l$, latent 规划器预测 latent action tokens: $\\mathbf{P}(z_t|I^h_t,I^l_t,I^r_t,l)$, 其中 LAM 编码器基于头部视角生成监督: $z_t := \\mathbf{I}(I^h_t,I^h_{t+H})$。由于 latent action 空间比 OpenVLA [4] 中使用的离散化下层动作小几个数量级, 这种方法也有助于将通用 VLM 高效适配到机器人策略。\n",
    "\n",
    "## Action Expert\n",
    "<!-- To achieve high-frequency and dexterous manipulation, Stage 3 integrates an action expert that utilizes a diffusion objective to model the continuous distribution of low-level actions [34]. Although the action expert shares the same architectural framework as the latent planner, their objectives diverge: the latent planner generates discretized latent action tokens through masked language modeling, while the action expert regresses low-level actions via an iterative denoising process. Both expert modules are conditioned hierarchically on preceding modules, including the action expert itself, ensuring coherent integration and information flow within the dual-expert system. -->\n",
    "\n",
    "为了实现高频和灵巧的操作, 第 3 阶段集成一个动作专家, 该专家利用扩散目标来建模下层动作的连续分布 [34]。尽管动作专家与 latent 规划器共享相同的架构框架, 但它们的目标有所不同: latent 规划器通过掩码语言建模生成离散化的 latent action tokens, 而动作专家通过迭代去噪过程回归下层动作。这两个专家模块在层次上都以先前的模块(包括动作专家自身)为条件, 从而确保双专家系统内的连贯集成和信息流。\n",
    "\n",
    "<!-- The action expert decodes low-level action chunks, de-noted by $A_t= [a_t,a_{t+1}, \\dots,a_{t+H}]$ with $H=30$, using proprioceptive state $p_t$ over an interval of $H$ timesteps: $\\mathbf{A}(A_t|I^h_t,I^l_t,I^r_t,p_t,l)$. During inference, the VLM, latent planner, and action expert are synergistically combined within the generalist policy GO-1, which initially predicts $k$ latent action tokens and subsequently conditions the denoising process to produce the final control signals. -->\n",
    "\n",
    "动作专家使用本体感知状态 $p_t$在 $H$ 个时间步内($\\mathbf{A}(A_t|I^h_t,I^l_t,I^r_t,p_t,l)$), 解码下层 action chunks, 表示为 $A_t= [a_t,a_{t+1}, \\dots,a_{t+H}]$, 其中 $H=30$。在推理过程中, VLM、latent 规划器和动作专家在通用策略 GO-1 中协同结合, 首先预测 $k$ 个 latent action tokens, 随后调节去噪过程以生成最终的控制信号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac6f3db-b4b3-4b34-b3b0-10843fdc22fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent efforts, such as Open X-Embodiment (OXE) [6],\n",
      "have addressed by aggregating and standardizing exist-\n",
      "ing datasets. Despite advancements on large-scale cross-\n",
      "embodiment learning, the resulting policy is constrained\n",
      "within naive, short-horizon tasks and can weakly generalize\n",
      "to out-of-domain scenarios [4]. DROID [7] collected expert\n",
      "data through crowd-sourcing from diverse real-life scenes.\n",
      "The absence of data quality assurance (with human feedback)\n",
      "and the reliance on a constrained hardware setup ( i.e.,featur-\n",
      "ing fixed, single-arm robots), limit its real-world applicability\n",
      "and broader effectiveness. More recently, Lin et al. [8]\n",
      "explored scaling laws governing generalizability across intra-\n",
      "category objects and environments, albeit limited to a few\n",
      "simple, single-step tasks. These efforts represent a notable\n",
      "advancement toward developing generalist policies, moving\n",
      "beyond the traditional focus on single-task learning within\n",
      "narrow domains [9], [3]. Nevertheless, existing robot learning\n",
      "datasets remain constrained by their reliance on short-horizon\n",
      "tasks in highly controlled laboratory environments, failing\n",
      "to adequately capture the complexity and diversity inher-\n",
      "ent in real-world manipulation tasks. To achieve general-\n",
      "purpose robotic intelligence, it is essential to develop datasets\n",
      "that scale in size and diversity while capturing real-world\n",
      "variability, supported by general-purpose humanoid robots\n",
      "for robust skill acquisition, a standardized data collection\n",
      "pipeline with assured quality, and carefully curated tasks\n",
      "reflecting real-world challenges.\n",
      "As depicted in Fig. 1, we introduce AgiBot World\n",
      "Colosseo , a full-stack large-scale robot learning platform\n",
      "curated for advancing bimanual manipulation in scal-\n",
      "able and intelligent embodied systems. A full-scale 4000-\n",
      "square-meter facility is constructed to represent five major\n",
      "domains—domestic, retail, industrial, restaurant, and office\n",
      "environment—all dedicated to high-fidelity data collection in\n",
      "authentic everyday scenarios. With over 1 million trajectories\n",
      "collected from 100 real robots, AgiBot World offers un-\n",
      "precedented diversity and complexity. It spans over 100 real-\n",
      "world scenarios, addressing challenging tasks such as fine-\n",
      "grained manipulation, tool usage, and multi-robot synergistic\n",
      "collaboration. Unlike prior datasets, AgiBot World dataset\n",
      "collection is carried out with a fully standardized pipeline,\n",
      "ensuring high data quality and scalability, while incorporat-\n",
      "ing human-in-the-loop verification to guarantee reliability.\n",
      "Our hardware setup includes mobile base humanoid robots\n",
      "with whole-body control, dexterous hands, and visuo-tactile\n",
      "sensors, enabling rich, multimodal data collection. Each\n",
      "episode is meticulously designed, featuring multiple camera\n",
      "views, depth information, camera calibration, and language\n",
      "annotations for both the overall task and each individual\n",
      "sub-steps. This well-rounded hardware setup, combined with\n",
      "various long-horizon, real-world tasks, opens new avenues\n",
      "for developing next-generation generalist policies and fosters\n",
      "diverse future research in robotics.\n",
      "Our experimental results highlight the transformative po-\n",
      "tential of the AgiBot World dataset. Policies pre-trained on\n",
      "our dataset achieve an average success rate improvement\n",
      "of 30% compared to those trained on the prior large-scalerobot dataset OXE [6]. Notably, even when utilizing only\n",
      "a fraction of our dataset—equivalent to 1/10 of the data\n",
      "volume in hours compared to OXE—the generalizability\n",
      "of pretrained policies is elevated by 18%. These findings\n",
      "underscore the dataset’s efficacy in bridging the gap between\n",
      "controlled laboratory environments and real-world robotic\n",
      "applications. Following our dataset, to address the limitations\n",
      "of previous robot foundation models that heavily rely on in-\n",
      "domain robot datasets, we present Genie Operator-1 (GO-\n",
      "1), a novel generalist policy that utilizes latent action rep-\n",
      "resentations to enable learning from heterogeneous data and\n",
      "efficiently bridges general-purpose vision-language models\n",
      "(VLMs) with robotic sequential decision-making. Through\n",
      "unified pre-training on web-scale data, spanning human\n",
      "videos to our high-quality robot dataset, GO-1 achieves\n",
      "superior generalization and dexterity, outperforming prior\n",
      "generalist policies such as RDT [10] and our variant without\n",
      "latent action planner. Moreover, we demonstrate that GO-\n",
      "1’s performance exhibits robust scalability with increasing\n",
      "dataset size, underscoring its potential for sustained advance-\n",
      "ment as larger datasets become available.\n",
      "Beyond its immediate impact, AgiBot World lays a strong\n",
      "foundation for future research in robotic manipulation. By\n",
      "open-sourcing the dataset, toolchain, and pre-trained models,\n",
      "we aim to foster community-wide innovation, enabling re-\n",
      "searchers to explore more authentic and diverse applications\n",
      "from household assistant to industrial automation. AgiBot\n",
      "World is more than yet another dataset; it is a step toward\n",
      "scalable, general-purpose robotic intelligence, empowering\n",
      "robots to tackle the complexities of the real world.\n",
      "Contribution. 1) We construct AgiBot World dataset, a\n",
      "multifarious robot learning dataset accompanied by open-\n",
      "source tools to advance research on policy learning at scale.\n",
      "As a pioneering initiative, AgiBot World employs an in-\n",
      "clusive optimized pipeline, from scene configuration, task\n",
      "design, data collection, to human-in-the-loop verification,\n",
      "which ensures unparalleled data quality. 2) We propose GO-\n",
      "1, a robot foundation policy using latent action represen-\n",
      "tations to unlock web-scale pre-training on heterogeneous\n",
      "data. Empowered by AgiBot World dataset, it outperforms\n",
      "prior generalist policies in generalization and dexterity, with\n",
      "performance scaling predictably with dataset size.\n",
      "Limitation. All evaluations are conducted in real-world\n",
      "scenarios. We are currently developing the simulation en-\n",
      "vironment, aligning with the real-world setup and aiming\n",
      "to reflect real-world policy deployment outcome. It would\n",
      "thereby facilitate fast and reproducible evaluation.\n",
      "II. R ELATED WORK\n",
      "Data scaling in robotics. Robot learning datasets from\n",
      "automated scripts or human teleoperation have enabled pol-\n",
      "icy learning, with early efforts like RoboTurk [19] and\n",
      "BridgeData [12] offering small-scale datasets with 2.1k and\n",
      "7.2k trajectories, respectively. Larger datasets, such as RT-\n",
      "1 [14] (130k trajectories), expand scopes yet remain limited\n",
      "to few environments and skills. Open X-Embodiment [6]\n",
      "aggregates various datasets into a unified format, growing\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "pdf_path = \"/mnt/y/paper/AgiBot_GO-1.pdf\"\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[1]\n",
    "text = page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcc6da-54fd-4728-abfd-f40f47280e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

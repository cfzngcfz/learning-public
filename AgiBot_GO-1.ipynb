{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a3dadd9-2407-455f-ac00-9ad7992eb405",
   "metadata": {},
   "source": [
    "[AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems](https://agibot-world.com/blog/go1)\n",
    "\n",
    "[github](https://github.com/OpenDriveLab/Agibot-World)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf2987-1e08-4a76-8c1d-bb4939b23d04",
   "metadata": {},
   "source": [
    "## RELATED WORK\n",
    "<!-- **Data scaling in robotics**. Robot learning datasets from automated scripts or human teleoperation have enabled policy learning, with early efforts like RoboTurk [19] and BridgeData [12] offering small-scale datasets with 2.1k and 7.2k trajectories, respectively. Larger datasets, such as RT-1 [14] (130k trajectories), expand scopes yet remain limited to few environments and skills. Open X-Embodiment [6] aggregates various datasets into a unified format, growing to more than 2.4 million trajectories, as a consequence it suffers from significant variability in embodiments, observation perspectives, and inconsistent data quality, limiting its overall effectiveness. More recently, DROID [7] moves towards scaling up scenes for greater diversity by crowd-sourcing demonstrations yet falls short in data scale and quality control. Prior datasets above generally face limitations in data scale, task practicality, and scenario naturalness, compounded by inadequate quality assurance and hardware restrictions, which impedes generalist policy training. As shown in Tab. I, our dataset addresses these gap adequately. We build a data collection facility spanning five scenarios to reconstruct real-world diversity and authenticity. With over 1 million trajectories gathered by skilled teleoperators through rigorous verification protocols, AgiBot World utilizes humanoid robots equipped with visuo-tactile sensors and dexterous hands to enable multimodal demonstrations, setting it apart from previous efforts. Unlike Pumacay et al.[20], which serves as a simulation benchmark for evaluating generalization, what we propose is a full-stack platform with data, models, benchmarks, and ecosystem. -->\n",
    "\n",
    "**机器人中的数据 scaling**. 来自自动脚本或人类远程操作的机器人学习数据集使得策略学习成为可能, 早期的努力如 RoboTurk [19] 和 BridgeData [12] 分别提供了具有 2.1k 和 7.2k 条轨迹的小规模数据集。更大的数据集, 如 RT-1 [14] (130k 条轨迹), 虽然扩大了范围, 但仍然局限于少数环境和技能。Open X-Embodiment [6] 将各种数据集聚合为统一格式, 增加至超过 240 万条轨迹, 因此其在实施、观察视角和数据质量方面存在很大差异, 限制了其整体有效性。最近, DROID [7] 通过众包演示, 扩大场景以增加多样性, 但在数据规模和质量控制方面仍显不足。上述先前的数据集通常面临数据规模、任务实用性和场景自然性的限制, 再加上质量保证不足和硬件限制, 这阻碍了通用策略训练。如表 I 所示, 我们的数据集充分弥补了这些差距。我们构建了一个涵盖五个场景的数据收集设施, 以重建现实世界的多样性和真实性。通过严格的验证协议, 熟练的远程操作员收集了超过 100 万条轨迹, AgiBot World 利用配备视觉触觉传感器和灵巧手的人形机器人进行多模态演示, 使其有别于之前的努力。与 Pumacay et al.[20] 不同, 后者作为评估泛化的模拟基准, 我们所提出的是一个全栈平台, 包含数据、模型、基准和生态系统。\n",
    "\n",
    "<!-- TABLE I: **Comparison to existing datasets**. AgiBot World features the largest number of trajectories to date . We replicate real-world environment at a 1:1 scale for the industrial and retail scenarios, which are barely present before. Extensive human annotations are offered, including item, scene, skill (sub-task segmented), and task-level annotations. Notably, to expand data applicability and potential, we include imperfect data ( i.e.,failure recovery data with annotated error states) and tasks with dexterous hands. To ensure data quality, we adopt a human-in-the-loop philosophy: the policy learning is performed on collected demonstrations. The deployment results are adopted as feedback to improve the collection protocol. -->\n",
    "\n",
    "表 I. **与现有数据集的比较**。AgiBot World 拥有迄今为止最多的轨迹数量。我们以 1:1 的比例复制了现实世界环境, 用于以前几乎不存在的工业和零售场景。提供了广泛的人工标注, 包括物品、场景、技能(子任务分段)和任务级标注。值得注意的是, 为了扩展数据的适用性和潜力, 我们纳入不完美的数据(即带有标注错误状态的失败恢复数据)和灵巧手任务。为了确保数据质量, 我们采用了人机交互的理念: 在收集到的演示上进行策略学习。部署结果被用作反馈来改进收集协议。\n",
    "\n",
    "<!-- **Policy learning at scale**. Robotic foundation models often co-evolve with the development of dataset scale, equipping robots with escalating general-purpose capabilities through diverse, large-scale training. Several prior arts use web-scale video only to facilitate policy learning given the limited scale of action-labeled robot datasets [21], [22], [23]. Another line of work lies in the use of large, end-to-end models trained on robot trajectories with robotics data scaling up [4], [24], [14], [25]. For instance, RDT [10] employs Diffusion Transformers, initially pre-trained on heterogeneous multirobot datasets and fine-tuned on over 6k dual-arm trajectories, showcasing the benefits of pre-training on diverse sources. $\\pi_0$ [26] uses a pre-trained VLM backbone and a flow-based action expert, advancing dexterous manipulation for complex tasks like laundry. LAPA [27] introduces the use of latent actions as pre-training targets; however, its latent planning capability is not preserved for downstream tasks. Building on a variety of innovative ideas from recent research, we advance the field by transferring web-scale knowledge to robotic control through the adaptation of vision-language models (VLMs) with latent actions, leveraging both human videos and robot data for scalable training. Our work demonstrates how the integration of a latent action planner enhances long-horizon task execution and enables more efficient policy learning, significantly improving upon existing generalist policies. -->\n",
    "\n",
    "**大规模策略学习**. 机器人基础模型通常与数据集规模的发展共同发展, 通过多样化的大规模训练为机器人配备不断升级的通用能力。鉴于带有动作标签的机器人数据集规模有限, 一些先前的研究仅使用互联网规模的视频来促进策略学习 [21], [22], [23]。另一类工作集中于使用在机器人轨迹上训练的大型端到端模型, 伴随机器人数据的不断扩展 [4], [24], [14], [25]。例如, <font color=\"red\">RDT [10] 采用 Diffusion Transformers, 最初在异构多机器人数据集上进行预训练, 并在超过 6k 条双臂轨迹上进行微调, 展示了在不同来源上预训练的好处</font>。$\\pi_0$ [26] 使用预训练的 VLM 骨干和基于流的动作专家, 提高了洗衣等复杂任务的灵巧操作。 LAPA [27] 引入了使用 latent actions 作为预训练目标; 然而, 其 latent 规划能力并未保留用于下游任务。基于近期研究中的各种创新思路, 我们通过融合视觉语言模型 (VLM) 和 latent actions, 将互联网规模的知识迁移到机器人控制中, 推动了该领域的发展, 利用人类视频和机器人数据进行可扩展训练。我们的工作展示了 latent action 规划器的集成如何增强长时域任务执行并实现更高效的策略学习, 显著改进了现有的通用策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48967a2b-3ca2-4195-a9b8-32432626eb9e",
   "metadata": {},
   "source": [
    "# AGIBOTWORLD: PLATFORM AND DATA\n",
    "<!-- AgiBot World is a full-stack and open-source embodied intelligence ecosystem. Based on the hardware platform developed by us, AgiBot G1, we construct AgiBot World —— an open-source robot manipulation dataset collected by more than 100 homogeneous robots, providing high-quality data for challenging tasks spanning a wide spectrum of real-life scenarios. The latest version contains 1,001,552 trajectories, with a total duration of 2976.4 hours, covering 217 specific tasks, 87 skills, and 106 scenes. We go beyond basic tabletop tasks such as pick-and-place in lab environments; instead, concentrate on real-world scenarios involving dual-arm manipulation, dexterous hands, and collaborative tasks. AgiBot World aims to provide an inclusive benchmark to drive the future development of advanced and robust algorithms. -->\n",
    "\n",
    "AgiBot World 是一个全栈开源的具身智能生态系统。基于我们开发的硬件平台 AgiBot G1, 我们构建了 AgiBot World ———— 一个由 100 台同质机器人收集的开源机器人操控数据集, 为涵盖广泛现实场景的挑战性任务提供高质量数据。最新版本包含 1,001,552 条轨迹, 总时长 2976.4 小时, 涵盖 217 个具体任务、87 种技能和 106 个场景。我们不限于实验室环境中的拾取和放置等基本桌面任务, 而是专注于涉及双臂操作、灵巧手和协作任务的现实世界场景。AgiBot World 旨在提供一个包容的基准, 以推动先进且稳健算法的未来发展。\n",
    "\n",
    "<!-- We plan to release all resources to enable the community to build upon AgiBot World. **The dataset is available under the CC BY-NC-SA 4.0 license**, along with the model checkpoints, code for data processing and policy training. -->\n",
    "\n",
    "我们计划发布所有资源, 以实现基于 AgiBot World 构建的社区。数据集, 以及模型检查点、数据处理和策略训练的代码, 在 CC BY-NC-SA 4.0 许可下可用。\n",
    "\n",
    "## Hardware: A Versatile Humanoid Robot\n",
    "<!-- The hardware platform is the cornerstone of AgiBot World, determining the lower limit of its quality. The standardization of hardware is also the key to streamlining distributed data collection and ensuring reproducible results. We meticulously develop a novel hardware platform for AgiBot World, distinguished by visuo-tactile sensors, durable 6-DoF dexterous hands with humanoid configuration. -->\n",
    "\n",
    "硬件平台是 AgiBot World 的基石, 决定了其质量的下限。硬件的标准化也是简化分布式数据收集和确保可重复结果的关键。我们为 AgiBot World 精心开发了一种新型硬件平台, 其特点是视觉-触觉传感器、耐用的 6 自由度灵巧手和人形配置。\n",
    "\n",
    "<!-- As illustrated in Fig. 1, our robotic platform features dual 7-DoF arms, a mobile chassis, and an adjustable waist. The end effectors are modular, allowing for the use of either a standard gripper or a 6-DoF dexterous hand, depending on task requirements. For tasks necessitating tactile feedback, a gripper equipped with visuo-tactile sensors is utilized. The robot is outfitted with eight cameras: an RGB-D camera and three fisheye cameras for the front view, RGB-D or fisheye cameras mounted on each end-effector, and two fisheye cameras positioned at the rear. Image observations and proprioceptive states, including joint and end-effector positions, are recorded at a control frequency of 30 Hz. -->\n",
    "\n",
    "如图 1 所示, 我们的机器人平台具有双 7 自由度臂、移动底盘和可调节腰部。末端执行器是模块化的, 可根据任务需求使用标准夹爪或 6 自由度灵巧手。对于需要触觉反馈的任务, 使用配备视觉-触觉传感器的夹爪。机器人配备了八个摄像头: 一个 RGB-D 摄像头和三个鱼眼摄像头用于前视角, 每个末端执行器上安装 RGB-D 或鱼眼摄像头, 以及两个位于后方的鱼眼摄像头。<font color=\"red\">以 30 Hz 的控p制频率记录图像观察和本体感受状态, 包括 joint and end-effector positions</font>。\n",
    "\n",
    "<!-- We employ two teleoperation systems: VR headset control and whole-body motion capture control. The VR controller maps the hand gesture to the end-effector translation and rotation, which is subsequently converted to joint angles through inverse kinematics. The thumbsticks and buttons on the controller enable robot base and body movement, while the trigger buttons control end-effector actuation. However, the VR controller restricts the dexterous hand to only a few predefined gestures. To extensively unlock our robot’s capabilities, we adapt a motion capture system which records the data of human joints, including the fingers, and maps them to robot posture, enabling more nuanced control, including individual finger movements, torso pose, and head orientation. This system provides posture flexibility and execution precision that are required in achieving more complex manipulation tasks. -->\n",
    "\n",
    "我们采用了两种远程操作系统: VR 头戴式控制和全身动作捕捉控制。VR 控制器将手势映射到末端执行器的平移和旋转, 随后通过逆运动学将其转换为关节角度。控制器上的摇杆和按钮实现机器人底座和身体运动, 而触发按钮控制末端执行器的驱动。然而, VR 控制器将灵巧手限制为几个预定义的手势。为了充分发挥机器人的功能, 我们采用了一种动作捕捉系统, 该系统记录人体关节(包括手指)的数据, 并将其映射到机器人姿态, 实现更细微的控制, 包括单个手指运动、躯干姿势和头部方向。该系统提供姿态灵活性和执行精度, 这是实现更复杂操作任务所需的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf8675-c5ae-4896-98db-e0e24170e6a9",
   "metadata": {},
   "source": [
    "## Data Collection: Protocol and Quality\n",
    "The data collection session, as shown in Fig. 2, can be broadly divided into three phases. (1) Before formally commencing data collection, we first conduct preliminary data acquisition to validate the feasibility of each task and establish corresponding collection standards. (2) After feasibility validation and review of the collection standards, skilled teleoperators arrange the initial scene and formally begin data collection according to the established standards. All data undergoes an initial validity verification locally, such as verifying the absence of missing frames. Once the data is confirmed to be complete, it is uploaded to the cloud for the next phase. (3) During post-processing, the data annotators will verify whether each episode meets the collection standards established in phase 1 and provide language annotations.\n",
    "\n",
    "数据采集​​环节如图2所示，大致可分为三个阶段。（1）在正式开始数据采集之前，我们首先进行初步数据采集，验证每个任务的可行性，并建立相应的采集标准。（2）在可行性验证和采集标准审核之后，熟练的远程操作员布置初始场景，并根据既定的标准正式开始数据采集。所有数据在本地进行初步有效性验证，如验证是否存在缺失帧。一旦确认数据完整，就会上传到云端进行下一阶段。（3）在后期处理过程中，数据标注者将验证每个情节是否符合第一阶段建立的采集标准，并提供语言标注。\n",
    "\n",
    "数据收集过程，如图 2 所示，大致可分为三个阶段。 (1) 在正式开始数据收集之前，我们首先进行初步数据采集，以验证每个任务的可行性并建立相应的收集标准。 (2) 在可行性验证和收集标准审查后，熟练的遥操作员安排初始场景，并根据既定标准正式开始数据收集。所有数据都经过初步的有效性验证，例如检查是否存在缺失帧。一旦确认数据完整，即上传到云端进入下一阶段。 (3) 在后处理阶段，数据标注员将验证每个剧集是否符合第一阶段建立的收集标准，并提供语言注释。\n",
    "\n",
    "\n",
    "Fig. 2: Data collection pipeline. AgiBot World embraces a human-in-the-loop framework to ensure high quality, enriched with detailed annotations and error recovery behaviors. Human feedback plays a critical role not only in post-collection review but also in actively guiding the data collection process, which is largely overlooked in prior efforts.\n",
    "\n",
    "\n",
    "图 2：数据收集流程。AgiBot World 采用了人机协作框架，以确保高质量的数据，配有详细的注释和错误恢复行为。人类反馈在后期收集审查中发挥关键作用，同时也积极引导数据收集过程，而这一点在先前的努力中往往被忽视。\n",
    "\n",
    "\n",
    "\n",
    "**Failure recovery**. During data collection, teleoperators may occasionally commit errors, such as inadvertently dropping objects while manipulating the robotic arms. However, they are often able to recover from these errors and successfully complete the task without requiring a full reconfiguration of the setup. Rather than discarding such trajectories, we retain them and manually annotate each with corresponding failure reasons and timestamps. These trajectories, referred to asfailure recovery data, constitute approximately one percent of the dataset. We consider them invaluable for achieving policy alignment [28] and failure reflection [29], essential for advancing the next generation of robot foundation models.\n",
    "\n",
    "失败恢复。在数据收集过程中，遥操作员可能会偶尔犯错，例如在操作机器人手臂时不小心掉落物品。然而，他们通常能够从这些错误中恢复，并成功完成任务，而无需对设置进行全面重新配置。我们并不抛弃这些轨迹，而是保留它们，并手动标注每个轨迹的相应失败原因和时间戳。这些轨迹被称为失败恢复数据，大约占数据集的百分之一。我们认为它们对于实现策略对齐 [28] 和失败反思 [29] 至关重要，这对于推进下一代机器人基础模型是必不可少的。\n",
    "\n",
    "\n",
    "\n",
    "**Human-in-the-loop**. Concurrent with feedback collection from data annotators, we adopt a human-in-the-loop approach to assess and refine data quality. This process involves an iterative cycle of collecting a small set of demonstrations, training a policy, and deploying the resulting policy to evaluate data availability. Based on the policy’s performance, we iteratively refine the data collection pipeline to address identified gaps or inefficiencies. For instance, during real-world deployment, the model exhibits prolonged pauses at the onset of actions, aligning with data annotator feedback highlighting inconsistent transitions and excessive idle time in the collected data. In response, we revise the data collection protocols and introduce a post-processing step to eliminate idle frames, thereby enhancing the dataset’s overall utility for policy learning. This feedback-driven methodology ensures continuous improvement in data quality.\n",
    "\n",
    "\n",
    "人机协作。在收集数据标注员反馈的同时，我们采用人机协作的方法来评估和改进数据质量。这个过程涉及一个迭代循环，包括收集一小组演示、训练策略，并部署结果策略以评估数据的可用性。根据策略的表现，我们迭代性地完善数据收集流程，以解决识别出的差距或低效。例如，在实际部署过程中，模型在动作开始时表现出长时间的停顿，这与数据标注员反馈指出的收集数据中不一致的过渡和过长的空闲时间相一致。对此，我们修订了数据收集协议，并引入了一个后处理步骤，以消除空闲帧，从而提高数据集在策略学习中的整体实用性。这种基于反馈的方法确保了数据质量的持续改进。\n",
    "\n",
    "\n",
    "\n",
    "## Dataset Statistics and Analysis: Beyond Scale\n",
    "AgiBot World is developed through a large-scale data collection facility, which spans over 4,000 square meters. This extensive environment contains over 3,000 unique objects in a variety of scenes, meticulously designed to reflect real-world settings. The dataset covers a wide range of scenarios and scene setups, ensuring both scale and diversity in the pursuit of generalizable robot policy.\n",
    "\n",
    "**Reconstructing the diversity of the real world**. Key statistics of our dataset are presented in Fig. 3. AgiBot World provides extensive coverage across five key domains: domestic, retail, industrial, restaurant, and office environments. Within each domain, we further define specific scene categories. For instance, the domestic domain includes detailed environments such as bedrooms, kitchens, living rooms, and balconies, while the retail domain features distinct areas like shelving units and fresh produce sections. Our dataset also features over 3,000 distinct objects, systematically categorized across various scenes. These objects span a wide range of everyday items, including food, furniture, clothing, electronic devices, and more. The distribution of object categories, as illustrated in Fig. 3(a), highlights the relative frequency of different object types within each scene.\n",
    "\n",
    "Fig. 3: Dataset Statistics. a) AgiBot World dataset covers the vast majority of robotic application scenarios, as well as a wide range of interactive objects. b)Our dataset features long-horizon tasks, with the majority of trajectories ranging from 30s to 60s. In contrast, widely used datasets, such as DROID, primarily consist of trajectories ranging from 5s to 20s, while OXE v1.0 predominantly contains trajectories within 5s. c)AgiBot World dataset focuses on valuable atomic skills, spanning a wide spectrum of skills, each supported by a minimum of 100 trajectories (red dashed line above).\n",
    "\n",
    "**Long-horizon manipulation**. A distinguishing feature of the AgiBot World dataset is its emphasis on long-horizon manipulation. As shown in Fig. 3(b), prior datasets predominantly focus on tasks involving single atomic skills, with most trajectories lasting no more than 5 seconds. In contrast, AgiBot World is built upon continuous and complete tasks composed by multiple atomic skills, like “make a coffee”. Trajectories in our dataset typically span approximately 30 seconds, some of which last over 2 minutes. We also provide key-frame and instruction annotation for each sub-step to facilitate policy learning in such challenging scenarios.\n",
    "\n",
    "**Comprehensive skill coverage**. In terms of task design, while generic atomic skills, such as “pick-and-place”, dominate the majority of tasks, we have intentionally incorporated tasks that emphasize less frequently used but highly valuable skills, such as “ chop ” and “ plug ” (as shown in Fig. 3(c)). This ensures that our dataset adequately represents a broad spectrum of skills, providing sufficient data for each to support robust policy learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e2651-bbd0-4b06-872d-128f070f629b",
   "metadata": {},
   "source": [
    "# AGIBOTWORLD: MODEL\n",
    "<!-- To effectively utilize our high-quality AgiBot World dataset and enhance the policy’s generalizability, we propose a hierarchical Vision-Language-Latent-Action (ViLLA) framework with three training stages, as depicted in Fig. 4. Compared to Vision-Language-Action (VLA) model where action is vision-language conditioned, the ViLLA model predicts latent action tokens, conditioned on the generation of subsequent robot control actions. -->\n",
    "\n",
    "为了有效利用我们高质量的 AgiBot World 数据集并增强策略的通用性, 我们提出了一个具有三个训练阶段的分层 Vision-Language-Latent-Action (ViLLA) 框架, 如图 4 所示。与视觉-语言-动作 (VLA) 模型(其中动作以视觉语言为条件)相比, ViLLA 模型以后续机器人控制动作的生成为条件来预测 latent action tokens。\n",
    "\n",
    "<!-- In Stage 1, we project consecutive images into a latent action space by training an encoder-decoder latent action model (LAM) on internet-scale heterogeneous data. This allows the latent action to serve as an intermediate representation, bridging the gap between general image-text inputs and robotic actions. In Stage 2, these latent actions act as pseudo-labels for the latent planner, facilitating embodiment-agnostic long-horizon planning and leveraging the generalizability of the pre-trained VLM. Finally, in Stage 3, we introduce the action expert and jointly train it with the latent planner to support the learning of dexterous manipulation. -->\n",
    "\n",
    "在第 1 阶段, 我们通过在互联网规模的异构数据上训练编码器-解码器 latent action model (LAM), 将连续图像投射到 latent action 空间。这使得 latent action 可以作为中间表示, 弥合通用图像文本输入和机器人动作之间的差距。在第 2 阶段, 这些 latent actions 充当 latent 规划器的伪标签, 便于与具身无关的长期规划并利用预训练 VLM 的通用性。最后, 在第 3 阶段, 我们引入动作专家并将其与 latent 规划器联合训练, 以支持灵巧操作的学习。\n",
    "\n",
    "## Latent Action Model\n",
    "<!-- Despite considerable advancements in gathering diverse robot demonstrations, the volume of action-labeled robot data remains limited relative to web-scale datasets. To broaden the data pool by incorporating internet-scale human videos lacking action labels and cross-embodiment robot data, we employ latent actions [30] in Stage 1 to model the inverse dynamics of consecutive frames. This approach enables the transfer of real-world dynamics from heterogeneous data sources into universal manipulation knowledge. -->\n",
    "\n",
    "尽管在收集多样化的机器人演示方面取得了显著进展, 但相对于互联网规模的数据集, 带有动作标签的机器人数据量仍然有限。为了通过整合缺乏动作标签的互联网规模的人类视频和跨实体机器人数据来扩大数据池, 我们在第 1 阶段采用 latent actions [30] 来建模连续帧的逆动力学。这种方法可以将现实世界的动态从异构数据源转移到通用操纵知识中。\n",
    "\n",
    "<!-- To extract latent actions from video frames $\\{I_t,I_{t+H} \\}$, the latent action model is constructed around an inverse dynamics model-based encoder $\\mathbf{I}(z_t|I_t,I_{t+H})$ and a forward dynamics model-based decoder $\\mathbf{F}(I_{t+H}|I_t,z_t)$. The encoder employs a spatial-temporal transformer [31] with casual temporal masks following Bruce et al. [30], while the decoder is a spatial transformer that takes the initial frame and discretized latent action tokens $z_t = [z^0_t, \\dots ,z^{k−1}_t]$ as input, with $k$ set to 4. The latent action tokens are quantized using a VQ-VAE objective [32], with a codebook of size $|C|$. -->\n",
    "\n",
    "为了从视频帧 $\\{I_t,I_{t+H} \\}$ 中提取 latent actions, latent action 模型由一个基于逆动力学模型的编码器 $\\mathbf{I}(z_t|I_t,I_{t+H})$ 和一个基于前向动力学模型的解码器 $\\mathbf{F}(I_{t+H}|I_t,z_t)$ 构建。编码器采用时空 transformer [31], 并遵循 Bruce et al. [30] 的因果时间掩码, 而解码器是一个空间 transformer, 以初始帧和离散的 latent action tokens $z_t = [z^0_t, \\dots ,z^{k−1}_t]$ 作为输入, $k$ 设置为 4。使用 VQ-VAE 目标 [32] 量化 latent action tokens, 代码本大小为 $|C|$。\n",
    "                                   \n",
    "## Latent Planner\n",
    "<!-- With the aim of establishing a solid foundation for scene and object understanding and general reasoning ability, the ViLLA model harnesses a VLM pre-trained on web-scale vision-language data and incorporates a latent planner for embodiment-agnostic planning within the latent action space. We use InternVL2.5-2B [33] as the VLM backbone due to its strong transfer learning capabilities. The two-billion parameter scale has proven effective for robotic tasks in our preliminary experiments, as well as in prior studies [10], [26]. Multiview image observations are first encoded using InternViT before being projected into the language space. The latent planner consists of 24 transformer layers, which enable layer-by-layer conditioning from the VLM backbone with full bidirectional attention. -->\n",
    "\n",
    "为了奠定场景和物体理解以及一般推理能力的坚实基础, ViLLA 模型利用在互联网规模的视觉语言数据上预训练的 VLM, 并耦合一个 latent 规划器, 用于 latent action 空间上的具身无关的规划。我们使用 InternVL2.5-2B [33] 作为 VLM 骨干, 因为其强大的迁移学习能力。在我们的初步实验以及先前的研究 [10]、[26] 中, 20 亿参数规模已被证明对机器人任务是有效的。首先使用 InternViT 对多视角图像观测进行编码, 然后将其投影到语言空间中。latent 规划器由 24 个 transformer 层组成, 它们能够从 VLM 骨干进行逐层调节, 并具有完全双向注意力。\n",
    "\n",
    "<!-- Specifically, given multiview input images $(I^h_t, I^l_t, I^r_t)$ (typically from the head, left wrist, and right wrist) at timestep $t$, along with a language instruction $l$ describing the ongoing task, the latent planner predicts latent action tokens: $\\mathbf{P}(z_t|I^h_t,I^l_t,I^r_t,l)$, with supervision produced by the LAM encoder based on the head view: $z_t := \\mathbf{I}(I^h_t,I^h_{t+H})$. Since the latent action space is orders of magnitude smaller than the discretized low-level actions used in OpenVLA [4], this approach also facilitates the efficient adaptation of general-purpose VLMs into robot policies. -->\n",
    "\n",
    "具体而言, 给定时间步长 $t$ 的多视角输入图像 $(I^h_t, I^l_t, I^r_t)$ (通常来自头部、左手腕和右手腕), 以及描述当前任务的语言指令 $l$, latent 规划器预测 latent action tokens: $\\mathbf{P}(z_t|I^h_t,I^l_t,I^r_t,l)$, 其中 LAM 编码器基于头部视角生成监督: $z_t := \\mathbf{I}(I^h_t,I^h_{t+H})$。由于 latent action 空间比 OpenVLA [4] 中使用的离散化下层动作小几个数量级, 这种方法也有助于将通用 VLM 高效适配到机器人策略。\n",
    "\n",
    "## Action Expert\n",
    "<!-- To achieve high-frequency and dexterous manipulation, Stage 3 integrates an action expert that utilizes a diffusion objective to model the continuous distribution of low-level actions [34]. Although the action expert shares the same architectural framework as the latent planner, their objectives diverge: the latent planner generates discretized latent action tokens through masked language modeling, while the action expert regresses low-level actions via an iterative denoising process. Both expert modules are conditioned hierarchically on preceding modules, including the action expert itself, ensuring coherent integration and information flow within the dual-expert system. -->\n",
    "\n",
    "为了实现高频和灵巧的操作, 第 3 阶段集成一个动作专家, 该专家利用扩散目标来建模下层动作的连续分布 [34]。尽管动作专家与 latent 规划器共享相同的架构框架, 但它们的目标有所不同: latent 规划器通过掩码语言建模生成离散化的 latent action tokens, 而动作专家通过迭代去噪过程回归下层动作。这两个专家模块在层次上都以先前的模块(包括动作专家自身)为条件, 从而确保双专家系统内的连贯集成和信息流。\n",
    "\n",
    "<!-- The action expert decodes low-level action chunks, de-noted by $A_t= [a_t,a_{t+1}, \\dots,a_{t+H}]$ with $H=30$, using proprioceptive state $p_t$ over an interval of $H$ timesteps: $\\mathbf{A}(A_t|I^h_t,I^l_t,I^r_t,p_t,l)$. During inference, the VLM, latent planner, and action expert are synergistically combined within the generalist policy GO-1, which initially predicts $k$ latent action tokens and subsequently conditions the denoising process to produce the final control signals. -->\n",
    "\n",
    "动作专家使用本体感知状态 $p_t$在 $H$ 个时间步内($\\mathbf{A}(A_t|I^h_t,I^l_t,I^r_t,p_t,l)$), 解码下层 action chunks, 表示为 $A_t= [a_t,a_{t+1}, \\dots,a_{t+H}]$, 其中 $H=30$。在推理过程中, VLM、latent 规划器和动作专家在通用策略 GO-1 中协同结合, 首先预测 $k$ 个 latent action tokens, 随后调节去噪过程以生成最终的控制信号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac6f3db-b4b3-4b34-b3b0-10843fdc22fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(c) Extensive Skill SetTrajectories\n",
      "(a)Diverse Scenarios and ObjectsOpen X-Embodiment v1.0DROIDAgiBotWorld\n",
      "DROIDOXELineof100Trajectories(b) Long-HorizonTasks\n",
      "DomesticRetailIndustrialRestaurantOfficeProportion of DomainsReal-worldAgiBotWorldOXEControlled Env.5-10   10-15   15-20   20-25   25-30  30-60   60-9090-120120-150>150 <5seconds80%60%40%20%0%ProportionNum.3002001000Fig. 3: Dataset Statistics. a) AgiBot World dataset covers the vast majority of robotic application scenarios, as well as a\n",
      "wide range of interactive objects. b)Our dataset features long-horizon tasks, with the majority of trajectories ranging from\n",
      "30s to 60s. In contrast, widely used datasets, such as DROID, primarily consist of trajectories ranging from 5s to 20s, while\n",
      "OXE v1.0 predominantly contains trajectories within 5s. c)AgiBot World dataset focuses on valuable atomic skills, spanning\n",
      "a wide spectrum of skills, each supported by a minimum of 100 trajectories (red dashed line above).\n",
      "categories, as illustrated in Fig. 3(a), highlights the relative\n",
      "frequency of different object types within each scene.\n",
      "Long-horizon manipulation. A distinguishing feature of\n",
      "the AgiBot World dataset is its emphasis on long-horizon\n",
      "manipulation. As shown in Fig. 3(b), prior datasets predom-\n",
      "inantly focus on tasks involving single atomic skills, with\n",
      "most trajectories lasting no more than 5 seconds. In contrast,\n",
      "AgiBot World is built upon continuous and complete tasks\n",
      "composed by multiple atomic skills, like “make a coffee”.\n",
      "Trajectories in our dataset typically span approximately 30\n",
      "seconds, some of which last over 2 minutes. We also provide\n",
      "key-frame and instruction annotation for each sub-step to\n",
      "facilitate policy learning in such challenging scenarios.\n",
      "Comprehensive skill coverage. In terms of task design,\n",
      "while generic atomic skills, such as “pick-and-place”, domi-\n",
      "nate the majority of tasks, we have intentionally incorporated\n",
      "tasks that emphasize less frequently used but highly valuable\n",
      "skills, such as “ chop ” and “ plug ” (as shown in Fig. 3(c)).\n",
      "This ensures that our dataset adequately represents a broad\n",
      "spectrum of skills, providing sufficient data for each to\n",
      "support robust policy learning.\n",
      "IV. A GIBOTWORLD : M ODEL\n",
      "To effectively utilize our high-quality AgiBot World\n",
      "dataset and enhance the policy’s generalizability, we pro-\n",
      "pose a hierarchical Vision- Language- Latent- Action (ViLLA)\n",
      "framework with three training stages, as depicted in Fig. 4.\n",
      "Compared to Vision-Language-Action (VLA) model where\n",
      "action is vision-language conditioned, the ViLLA model\n",
      "predicts latent action tokens, conditioned on the generation\n",
      "of subsequent robot control actions.\n",
      "In Stage 1, we project consecutive images into a latent ac-\n",
      "tion space by training an encoder-decoder latent action model\n",
      "(LAM) on internet-scale heterogeneous data. This allowsthe latent action to serve as an intermediate representation,\n",
      "bridging the gap between general image-text inputs and\n",
      "robotic actions. In Stage 2, these latent actions act as pseudo-\n",
      "labels for the latent planner, facilitating embodiment-agnostic\n",
      "long-horizon planning and leveraging the generalizability of\n",
      "the pre-trained VLM. Finally, in Stage 3, we introduce the\n",
      "action expert and jointly train it with the latent planner to\n",
      "support the learning of dexterous manipulation.\n",
      "A. Latent Action Model\n",
      "Despite considerable advancements in gathering diverse\n",
      "robot demonstrations, the volume of action-labeled robot data\n",
      "remains limited relative to web-scale datasets. To broaden\n",
      "the data pool by incorporating internet-scale human videos\n",
      "lacking action labels and cross-embodiment robot data, we\n",
      "employ latent actions [30] in Stage 1 to model the inverse\n",
      "dynamics of consecutive frames. This approach enables the\n",
      "transfer of real-world dynamics from heterogeneous data\n",
      "sources into universal manipulation knowledge.\n",
      "To extract latent actions from video frames {It,It+H},\n",
      "the latent action model is constructed around an inverse\n",
      "dynamics model-based encoder I(zt|It,It+H)and a forward\n",
      "dynamics model-based decoder F(It+H|It,zt). The encoder\n",
      "employs a spatial-temporal transformer [31] with casual tem-\n",
      "poral masks following Bruce et al. [30], while the decoder\n",
      "is a spatial transformer that takes the initial frame and\n",
      "discretized latent action tokens zt= [z0\n",
      "t, ...,zk−1\n",
      "t]as input,\n",
      "with kset to 4. The latent action tokens are quantized using\n",
      "a VQ-V AE objective [32], with a codebook of size |C|.\n",
      "B. Latent Planner\n",
      "With the aim of establishing a solid foundation for scene\n",
      "and object understanding and general reasoning ability, the\n",
      "ViLLA model harnesses a VLM pre-trained on web-scale\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "pdf_path = \"/mnt/y/paper/AgiBot_GO-1.pdf\"\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[4]\n",
    "text = page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcc6da-54fd-4728-abfd-f40f47280e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c484fe-4f44-44fa-9e94-ca116d685a48",
   "metadata": {},
   "source": [
    "[GR00T N1: An Open Foundation Model for Generalist Humanoid Robots](https://arxiv.org/abs/2503.14734)\n",
    "\n",
    "**Abstract**  \n",
    "General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation model, trained on massive and diverse data sources, is essential for enabling the robots to reason about novel situations, robustly handle real-world variability, and rapidly learn new tasks. To this end, we introduce GR00T N1, an open foundation model for humanoid robots. GR00T N1 is a Vision-Language-Action (VLA) model with a dual-system architecture. The vision-language module (System 2) interprets the environment through vision and language instructions. The subsequent diffusion transformer module (System 1) generates fluid motor actions in real time. Both modules are tightly coupled and jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixture of real-robot trajectories, human videos, and synthetically generated datasets. We show that our generalist robot model GR00T N1 outperforms the state-of-the-art imitation learning baselines on standard simulation benchmarks across multiple robot embodiments. Furthermore, we deploy our model on the Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation tasks, achieving strong performance with high data efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257b51ea-11c2-49dd-97f0-0775fa3cbdb6",
   "metadata": {},
   "source": [
    "# GR00T N1 Foundation Model\n",
    "GR00T N1 is a Vision-Language-Action (VLA) model for humanoid robots trained on diverse data sources. The model contains a vision-language backbone that encodes language and image input and a DiT-based flow-matching policy that outputs high-frequency actions. We use the NVIDIA Eagle-2 VLM (Li et al., 2025) as the vision-language backbone. Specifically, our publicly released GR00T-N1-2B model has 2.2B parameters in total, with 1.34B in the VLM. The inference time for sampling a chunk of 16 actions is 63.9ms on an L40 GPU using bf16. Fig. 2 provides a high-level overview of our model design. We highlight three key features of GR00T N1:\n",
    "- We design a compositional model that integrates Vision-Language Model (VLM)-based reasoning module (System 2) and Diffusion Transformer (DiT)-based action module (System 1) in a unified learning framework;\n",
    "- We develop an effective pre-training strategy using a mixture of human videos, simulation and neural-generated data, and real robot demonstrations (see Fig. 1) for generalization and robustness;\n",
    "- We train a massively multi-task, language-conditioned policy that supports a wide range of robot embodiments and enables rapid adaptation to new tasks through data-efficient post-training.\n",
    "\n",
    "GR00T N1æ˜¯ä¸€ä¸ªåœ¨ä¸åŒæ•°æ®æºä¸Šè®­ç»ƒçš„äººå½¢æœºå™¨äººçš„ Vision-Language-Action (VLA) æ¨¡å‹ã€‚è¯¥æ¨¡å‹åŒ…å«ä¸€ä¸ªè§†è§‰-è¯­è¨€éª¨å¹²(ç¼–ç è¯­è¨€å’Œå›¾åƒè¾“å…¥)ä»¥åŠåŸºäº DIT çš„ flow-matching ç­–ç•¥(è¾“å‡ºé«˜é¢‘å·¥ä½œ)ã€‚æˆ‘ä»¬ä½¿ç”¨ NVIDIA Eagle-2 VLM (Li et al., 2025) ä½œä¸ºè§†è§‰-è¯­è¨€éª¨å¹²ã€‚å…·ä½“è€Œè¨€, æˆ‘ä»¬å…¬å¼€å‘å¸ƒçš„ GR00T-N1-2B æ¨¡å‹æ€»å…±æœ‰ 2.2b å‚æ•°, å…¶ä¸­ VLM ä¸­çš„å‚æ•°ä¸º1.34Bã€‚ä½¿ç”¨BF16ï¼Œåœ¨L40 GPUä¸ŠæŠ½æ ·16ä¸ªåŠ¨ä½œçš„æ¨ç†æ—¶é—´ä¸º63.9msã€‚å›¾2æä¾›äº†æˆ‘ä»¬æ¨¡å‹è®¾è®¡çš„é«˜çº§æ¦‚è¿°ã€‚æˆ‘ä»¬é‡ç‚¹ä»‹ç»äº†GR00T N1çš„ä¸‰ä¸ªå…³é”®åŠŸèƒ½ï¼š\r\n",
    " - æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç»„æˆæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†æ¨¡å—ï¼ˆç³»ç»Ÿ2ï¼‰å’Œæ‰©æ•£å˜å‹å™¨ï¼ˆDITï¼‰åŸºäºç»Ÿä¸€çš„å­¦ä¹ æ¨¡å—ï¼ˆç³»ç»Ÿ1ï¼‰åœ¨ç»Ÿä¸€çš„å­¦ä¹ æ¡†æ¶ä¸­ï¼›\r\n",
    " - æˆ‘ä»¬ä½¿ç”¨äººç±»è§†é¢‘ï¼Œä»¿çœŸå’Œç¥ç»ç”Ÿæˆçš„æ•°æ®ä»¥åŠçœŸæ­£çš„æœºå™¨äººæ¼”ç¤ºçš„æ··åˆç‰©åˆ¶å®šäº†æœ‰æ•ˆçš„è®­ç»ƒç­–ç•¥ï¼ˆè¯·å‚è§å›¾1ï¼‰ï¼›\r\n",
    " - æˆ‘ä»¬è®­ç»ƒä¸€é¡¹å¤§è§„æ¨¡çš„å¤šä»»åŠ¡ï¼Œè¯­è¨€æ¡ä»¶æ”¿ç­–ï¼Œè¯¥æ”¿ç­–æ”¯æŒå¹¿æ³›çš„æœºå™¨äººå®æ–½ä¾‹ï¼Œå¹¶é€šè¿‡æ•°æ®æœ‰æ•ˆçš„ååŸ¹è®­æ¥å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚\n",
    "\n",
    "GR00T N1 æ˜¯ä¸€ä¸ªé’ˆå¯¹äººå½¢æœºå™¨äººçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰æ¨¡å‹ï¼Œè®­ç»ƒäºå¤šç§æ•°æ®æºã€‚è¯¥æ¨¡å‹åŒ…å«ä¸€ä¸ªç¼–ç è¯­è¨€å’Œå›¾åƒè¾“å…¥çš„è§†è§‰-è¯­è¨€éª¨å¹²ï¼Œä»¥åŠä¸€ä¸ªåŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰çš„æµåŒ¹é…ç­–ç•¥ï¼Œç”¨äºè¾“å‡ºé«˜é¢‘ç‡çš„åŠ¨ä½œã€‚æˆ‘ä»¬ä½¿ç”¨ NVIDIA Eagle-2 VLMï¼ˆLi et al., 2025ï¼‰ä½œä¸ºè§†è§‰-è¯­è¨€éª¨å¹²ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å…¬å¼€å‘å¸ƒçš„ GR00T-N1-2B æ¨¡å‹æ€»å…±æœ‰ 22 äº¿ä¸ªå‚æ•°ï¼Œå…¶ä¸­ 13.4 äº¿ä¸ªå‚æ•°åœ¨ VLM ä¸­ã€‚é‡‡æ · 16 ä¸ªåŠ¨ä½œçš„æ¨ç†æ—¶é—´ä¸º 63.9 æ¯«ç§’ï¼Œä½¿ç”¨ bf16 åœ¨ L40 GPU ä¸Šè¿›è¡Œã€‚å›¾ 2 æä¾›äº†æˆ‘ä»¬æ¨¡å‹è®¾è®¡çš„é«˜å±‚æ¬¡æ¦‚è¿°ã€‚æˆ‘ä»¬å¼ºè°ƒ GR00T N1 çš„ä¸‰ä¸ªå…³é”®ç‰¹æ€§ï¼š\n",
    "\n",
    "æˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªç»„åˆæ¨¡å‹ï¼Œé›†æˆäº†åŸºäºè§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„æ¨ç†æ¨¡å—ï¼ˆç³»ç»Ÿ 2ï¼‰å’ŒåŸºäºæ‰©æ•£å˜æ¢å™¨ï¼ˆDiTï¼‰çš„åŠ¨ä½œæ¨¡å—ï¼ˆç³»ç»Ÿ 1ï¼‰ï¼Œå½¢æˆç»Ÿä¸€çš„å­¦ä¹ æ¡†æ¶ï¼›\n",
    "æˆ‘ä»¬å¼€å‘äº†ä¸€ç§æœ‰æ•ˆçš„é¢„è®­ç»ƒç­–ç•¥ï¼Œåˆ©ç”¨äººç±»è§†é¢‘ã€ä»¿çœŸå’Œç¥ç»ç”Ÿæˆæ•°æ®ä»¥åŠçœŸå®æœºå™¨äººæ¼”ç¤ºçš„æ··åˆï¼Œå¢å¼ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›å’Œé²æ£’æ€§ï¼ˆè§å›¾ 1ï¼‰ï¼›\n",
    "æˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šä»»åŠ¡ã€è¯­è¨€æ¡ä»¶çš„ç­–ç•¥ï¼Œæ”¯æŒå¤šç§æœºå™¨äººå½¢æ€ï¼Œå¹¶é€šè¿‡æ•°æ®é«˜æ•ˆçš„åè®­ç»ƒå®ç°å¿«é€Ÿé€‚åº”æ–°ä»»åŠ¡ã€‚\n",
    "\n",
    "## Model Architecture\n",
    "In this section, we describe the GR00T N1 model architecture, illustrated in Fig. 3. GR00T N1 uses flow-matching (Lipman et al.) to learn action generation. A diffusion transformer (DiT) processes the robotâ€™s proprioceptive state and action, which are then cross-attended with image and text tokens from the Eagle-2 VLM backbone to output the denoised motor actions. Below, we elaborate on each module in detail.\n",
    "\n",
    "Figure 3: **GR00T N1 Model Architecture**. GR00T N1 is trained on a diverse set of embodiments ranging from si gle-arm robot arms to bimanual humanoid dexterous hands. To deal with different robot embodimentâ€™s state observation and action, we use DiT blocks with an embodiment-aware state and action encoder to embed the robotâ€™s state and action inputs. GR00T N1 model leverages latent embeddings of the Eagle-2 model to incorporate the robotâ€™s visual observation and language instructions. The vision language tokens will then be fed into the DiT blocks through cross-attention layers.\n",
    "\n",
    "**State and Action Encoders**\n",
    "\n",
    "To process states and actions of varying dimensions across different robot embodiments, we use an MLP per embodiment to project them to a shared embedding dimension as input to the DiT. As in Black et al. (2024), the Action Encoder MLP also encodes the diffusion timestep together with the noised action vector.\n",
    "\n",
    "We use action flow matching, which samples actions through iterative denoising. The model takes as input noised actions in addition to encodings of the robotâ€™s proprioceptive state, image tokens, and text tokens. The actions are processed in chunks as in Zhao et al. (2023), meaning that at any given time ğ‘¡the model uses $ğ´_ğ‘¡ = [ğ‘_ğ‘¡, ğ‘_{ğ‘¡+1}, \\dots, ğ‘_{ğ‘¡+ğ»âˆ’1}]$ which contains the action vectors of timesteps $ğ‘¡$ through $ğ‘¡+ğ»âˆ’1$. We set $ğ»= 16$ in our implementation.\n",
    "\n",
    "**Vision-Language Module (System 2)**\n",
    "\n",
    "For encoding vision and language inputs, GR00T N1 uses the Eagle-2 (Li et al., 2025) vision-language model (VLM) pretrained on Internet-scale data. Eagle-2 is finetuned from a SmolLM2 (Allal et al., 2025) LLM and a SigLIP-2 (Tschannen et al., 2025) image encoder. Images are encoded at resolution 224 $\\times$ 224followed by pixel shuffle (Shi et al., 2016), resulting in 64 image token embeddings per frame. These embeddings are then further encoded together with text by the LLM component of the Eagle-2 VLM. The LLM and image encoder are aligned over a broad set of vision-language tasks following the general recipe of Li et al. (2025).\n",
    "\n",
    "During policy training, a text description of the task, as well as (possibly multiple) images, are passed to the VLM in the chat format used during vision-language training. We then extract vision-language features of shape (batch size $\\times$ sequence length $\\times$ hidden dimension) from the LLM. We found that using middle-layer instead of final-layer LLM embeddings resulted in both faster inference speed and higher downstream policy success rate. For GR00T-N1-2B, we use the representations from the 12th layer.\n",
    "\n",
    "**Diffusion Transformer Module (System 1)**\n",
    "\n",
    "For modeling actions, GR00T N1 uses a variant of DiT (Peebles and Xie, 2023), which is a transformer with denoising step conditioning via adaptive layer normalization, denoted as $ğ‘‰_{\\theta}$. As shown in Fig. 3, $ğ‘‰_{\\theta}$ consists of\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "861944b1-b2de-4efe-8c3e-d3c1edb0969a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GR00T N1: An Open Foundation Model for Generalist Humanoid Robots\n",
      "Robot State\n",
      "Eagle-2 VLMCross-AttentionSelf-AttentionCross-AttentionSelf-AttentionStateEncoderAction DecoderDiTBlocks MotorActionAction Encoderx Nâ€œPick up the apple and place it into the bottom shelfâ€\n",
      "Noised ActionEmbodiment-Specific ModuleVisionEncoderText Tokenizer\n",
      "K iterations\n",
      "Pre-trained and Frozenğ‘!. . .ğ‘!ğ‘!\"#ğ‘!\"$%#. . .ğ‘!ğ‘!\"#ğ‘!\"$%#\n",
      "Figure 3: GR00T N1 Model Architecture. GR00T N1 is trained on a diverse set of embodiments ranging from\n",
      "single-arm robot arms to bimanual humanoid dexterous hands. To deal with different robot embodimentâ€™s\n",
      "state observation and action, we use DiT blocks with an embodiment-aware state and action encoder to embed\n",
      "the robotâ€™s state and action inputs. GR00T N1 model leverages latent embeddings of the Eagle-2 model to\n",
      "incorporate the robotâ€™s visual observation and language instructions. The vision language tokens will then be\n",
      "fed into the DiT blocks through cross-attention layers.\n",
      "the Action Encoder MLP also encodes the diffusion timestep together with the noised action vector.\n",
      "We use action flow matching, which samples actions through iterative denoising. The model takes as input\n",
      "noised actions in addition to encodings of the robotâ€™s proprioceptive state, image tokens, and text tokens. The\n",
      "actions are processed in chunks as in Zhao et al. (2023), meaning that at any given time ğ‘¡the model uses\n",
      "ğ´ğ‘¡= [ğ‘ğ‘¡, ğ‘ğ‘¡+1, . . . , ğ‘ ğ‘¡+ğ»âˆ’1]which contains the action vectors of timesteps ğ‘¡through ğ‘¡+ğ»âˆ’1. We set ğ»= 16\n",
      "in our implementation.\n",
      "Vision-Language Module (System 2)\n",
      "For encoding vision and language inputs, GR00T N1 uses the Eagle-2 (Li et al., 2025) vision-language model\n",
      "(VLM) pretrained on Internet-scale data. Eagle-2 is finetuned from a SmolLM2 (Allal et al., 2025) LLM and\n",
      "a SigLIP-2 (Tschannen et al., 2025) image encoder. Images are encoded at resolution 224Ã—224followed by\n",
      "pixel shuffle (Shi et al., 2016), resulting in 64 image token embeddings per frame. These embeddings are then\n",
      "further encoded together with text by the LLM component of the Eagle-2 VLM. The LLM and image encoder\n",
      "are aligned over a broad set of vision-language tasks following the general recipe of Li et al. (2025).\n",
      "During policy training, a text description of the task, as well as (possibly multiple) images, are passed to the\n",
      "VLM in the chat format used during vision-language training. We then extract vision-language features of\n",
      "shape (batch size Ã—sequence lengthÃ—hidden dimension) from the LLM. We found that using middle-layer\n",
      "instead of final-layer LLM embeddings resulted in both faster inference speed and higher downstream policy\n",
      "success rate. For GR00T-N1-2B, we use the representations from the 12th layer.\n",
      "Diffusion Transformer Module (System 1)\n",
      "For modeling actions, GR00T N1 uses a variant of DiT (Peebles and Xie, 2023), which is a transformer with\n",
      "denoising step conditioning via adaptive layer normalization, denoted as ğ‘‰ğœƒ. As shown in Fig. 3, ğ‘‰ğœƒconsists of\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "pdf_path = \"/mnt/z/AutoPapers/downloaded_papers/2503.14734.GR00T_N1_An_Open_Foundation_Model_for_Generalist_Humanoid_Robots.pdf\"\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[3]\n",
    "text = page.extract_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977fb3a-f525-4e3b-bd70-c84693d292c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

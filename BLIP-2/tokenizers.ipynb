{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c131581a",
   "metadata": {},
   "source": [
    "# [Summary of the tokenizers](https://huggingface.co/docs/transformers/main/en/tokenizer_summary)\n",
    "\n",
    "正如我们在 [the preprocessing tutorial](https://huggingface.co/docs/transformers/main/zh/preprocessing) 中看到的那样, 对文本进行分词就是将其拆分为单词或子词, 然后通过查找表将其转换为 ids。将单词或子词转换为 ids 很简单, 因此在本总结中, 我们将聚焦将一段文本拆分成单词或子词(即对一段文本进行分词)。更具体地说, 我们将研究 Transformers 库中使用的三种主要类型的分词器: [Byte-Pair Encoding (BPE)](#Byte-Pair-Encoding-(BPE)), [WordPiece](#WordPiece) 和 [SentencePiece](#SentencePiece), 并展示哪种分词器类型被哪种模型使用。\n",
    "\n",
    "请注意, 在每个模型页面上, 你可以查看相关分词器的文档, 以了解预训练模型使用了哪种分词器类型。例如, 如果我们查看 [BertTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer), 我们可以看到该模型使用 [WordPiece](#WordPiece)。\n",
    "\n",
    "## Introduction\n",
    "将一段文本拆分成更小的块是一项比看起来更难的任务, 并且有很多方式可以实现。例如, 让我们看看这个句子 \"Don't you love 🤗 Transformers? We sure do.\"\n",
    "\n",
    "对这段文本分词的一个简单方式是使用空格将其拆分, 这将得到:<br>\n",
    "`[\"Don't\", \"you\", \"love\", \"🤗\", \"Transformers?\", \"We\", \"sure\", \"do.\"]`\n",
    "\n",
    "这是一个明智的开始, 但是如果我们查看词元 \"Transformers?\" 和 \"do.\", 我们注意到标点符号被附着在单词 \"Transformer\" 和 \"do\" 的后面, 这是次优的。我们应该将标点符号考虑进来, 这样模型就不必学习一个单词和每个可能跟在其后的标点符号的不同组合, 这将使模型必须学习的组合数量激增。考虑标点符号, 对示例文本进行分词将得到:<br>\n",
    "`[\"Don\", \"'\", \"t\", \"you\", \"love\", \"🤗\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]`\n",
    "\n",
    "结果更好。然而, 该分词处理单词 \"Don't\" 的方式是有缺陷的。\"Don't\" 的含义是 \"do not\", 所以将其分词为 [\"Do\", \"n't\"] 更好。现在开始事情变得复杂, 部分原因是每个模型都有自己的分词类型。根据我们应用于文本分词的规则, 相同的文本会生成不同的分词输出。仅当您向预训练模型提供一个输入(使用与训练数据相同的规则对其进行分词)时, 预训练模型才能正常运行。\n",
    "\n",
    "[spaCy](https://spacy.io/) 和 [Moses](http://www.statmt.org/moses/?n=Development.GetStarted) 是两个受欢迎的基于规则的分词器。将它们应用于我们的示例, spaCy 和 Moses 将输出类似下面的结果:<br>\n",
    "`[\"Do\", \"n't\", \"you\", \"love\", \"🤗\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]`\n",
    "\n",
    "可以看出, 这里使用了空格和标点符号的分词, 以及基于规则的分词。空格和标点符号分词以及基于规则的分词都是单词分词的例子, 其被宽松地定义为将句子拆分为单词。虽然将文本拆分为更小的块是最直观的方式, 但是这种分词方法用于大型文本语料库时, 会产生很多问题。在这种情况下, 空格和标点符号分词通常产生一个非常大的词汇表(使用的所有唯一单词和词元的集合)。例如, [Transformer XL](https://huggingface.co/docs/transformers/main/zh/model_doc/transformerxl) 使用空格和标点符号分词, 从而导致词汇表的大小为 267,735!\n",
    "\n",
    "如此大的一个词汇表大小, 迫使模型有一个巨大的嵌入矩阵, 作为输入和输出层, 这会增加内存使用量, 也会提高时间复杂度。一般而言, transformers 模型的词汇表大小很少超过 50,000, 特别是如果它们只在单一语言上进行预训练。\n",
    "\n",
    "因此, 如果简单的空格和标点符号分词令人不满意, 为什么不在字符级上进行简单地分词?\n",
    "\n",
    "尽管字符分词非常简单, 并且将极大地降低了内存和时间复杂度, 但是这会让模型很难学习有意义的输入表示。例如, 学习字母 \"t\" 的有意义的上下文独立的表示比学习单词 \"today\" 更加困难。因此, <font color=\"red\">字符分词经常伴随着性能的损失</font>。所以为了兼顾两全其美, transformers 模型在单词级别和字符级别分词之间采用了一种混合方案, 被称为**子词**分词([**subword** tokenization](#Subword-tokenization))。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba061d",
   "metadata": {},
   "source": [
    "## Subword tokenization\n",
    "子词分词算法依赖于以下原则: 频繁使用的单词不应该被拆分成更小的子词, 但是罕见的单词应该被分解为有意义的子词。例如, \"annoyingly\" 可能被视为一个罕见的单词, 能被分解为 \"annoying\" 和 \"ly\"。\"annoying\" 和 \"ly\" 作为独立子词, 出现得更频繁, 与此同时单词 \"annoyingly\" 的含义由 \"annoying\" 和 \"ly\" 的复合含义保留。这在黏着语(如土耳其语)中尤其有用, 在这些语言中, 可以通过将子词串在一起来形成(几乎)任意长的复杂单词。\n",
    "\n",
    "子词分词允许模型有一个合理的词汇表大小, 同时能学到有意义的上下文独立的表达。此外, 子词分词使模型能够处理从未见过的单词, 通过将它们分解为已知的子词。例如, [BertTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer) 对\"I have a new GPU!\"分词结果如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "tokenizer.tokenize(\"I have a new GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c74b82e",
   "metadata": {},
   "source": [
    "`[\"i\", \"have\", \"a\", \"new\", \"gp\", \"##u\", \"!\"]`\n",
    "\n",
    "因为我们正在考虑不区分大小写的模型, 句子先被转换成小写字母形式。我们可以看到, 单词 [\"i\", \"have\", \"a\", \"new\"] 出现在分词器的词汇表中, 但是单词 \"gpu\" 不在其中。因此, 分词器将\"gpu\"拆分成已知的子词: [\"gp\" and \"##u\"]。\"##\" 表示词元的剩余部分, 应该附着在前一个词元的后面, 无空格(分词的解码或逆操作)。\n",
    "\n",
    "另外一个例子, [XLNetTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetTokenizer) 对之前的示例文本分词结果如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet/xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"Don't you love 🤗 Transformers? We sure do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc89dcc",
   "metadata": {},
   "source": [
    "`[\"▁Don\", \"'\", \"t\", \"▁you\", \"▁love\", \"▁\", \"🤗\", \"▁\", \"Transform\", \"ers\", \"?\", \"▁We\", \"▁sure\", \"▁do\", \".\"]`\n",
    "\n",
    "当我们查看 [SentencePiece](#SentencePiece) 时, 回过头来解释这些\"▁\"符号的含义。正如所见, 罕见的单词 \"Transformers\" 被拆分为更高频的子词 \"Transform\" 和 \"ers\"。\n",
    "\n",
    "现在让我们看看不同子词分词算法是如何工作的。请注意, 所有这些分词算法依赖于某些训练方式, 它们通常在语料库上完成, 相应的模型将在这些语料库上训练。\n",
    "\n",
    "### Byte-Pair Encoding (BPE)\n",
    "Byte-Pair Encoding (BPE) 来自于 [Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909)。BPE 依赖于一个预分词器, 它将训练数据分割成单词。<font color=\"red\">预分词可以是简单的空格分词, 例如 [GPT-2](https://huggingface.co/docs/transformers/main/zh/model_doc/gpt2), [RoBERTa](https://huggingface.co/docs/transformers/main/zh/model_doc/roberta)。更加先进的预分词方式包括了基于规则的分词, 例如 [XLM](https://huggingface.co/docs/transformers/main/zh/model_doc/xlm), [FlauBERT](https://huggingface.co/docs/transformers/main/zh/model_doc/flaubert)</font>(在大多数语言使用[Moses](http://www.statmt.org/moses/?n=Development.GetStarted)) <font color=\"red\">或者 [GPT](https://huggingface.co/docs/transformers/main/zh/model_doc/gpt)</font>(使用 [spaCy](https://spacy.io/) 和 ftfy), 以统计训练语料库中每个单词的频次。\n",
    "\n",
    "A) 在预分词以后, 生成了唯一单词的集合, 也确定了训练数据中每个单词出现的频次。B) 接下来, BPE 产生了一个基础词汇表, 包含唯一单词集合中出现的所有符号, 并学习合并规则, 合并基础词汇表中的两个符号形成一个新的符号。它会一直学习直到词汇表达到所需的词汇表大小。请注意, 所需的词汇表大小是在训练分词器之前定义的超参数。\n",
    "\n",
    "例如, 假设在预分词后, A) 下面的单词集合以及他们的频数已确定:<br>\n",
    "`(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)`\n",
    "\n",
    "所以, 基础的词汇表是 [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]。将所有单词分割成基础词汇表中的符号, 得到:<br>\n",
    "`(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)`\n",
    "\n",
    "B) 接下来, BPE 统计每个可能的符号对的频数, 并挑选出出现最频繁的的符号对。在上面的例子中, \"h\" 后接 \"u\" 出现了 10 + 5 = 15 次 (10 次是出现了 10 次 \"hug\", 5 次是出现了 5 次 \"hugs\")。然而, 最频繁的符号对是 \"u\" 后接 \"g\", 总共出现了 10 + 5 + 5 = 20 次。因此, 分词器学到的第一个合并规则是合并所有的 \"u\" 后接 \"g\" 符号。接下来, \"ug\" 被添加到词汇表中。然后, 单词集合变成:<br>\n",
    "`(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)`<br>\n",
    "\n",
    "然后, BPE 统计出下一个最常见的符号对。也就是 \"u\" 后接 \"n\", 出现了 16 次。\"u\" 和 \"n\" 被合并成 \"un\", 并被添加到词汇表中。下一个最高频的符号对是 \"h\" 后接 \"ug\", 出现了15次。再次合并这对, \"hug\" 被添加到词汇表中。\n",
    "\n",
    "在此阶段, 词汇表是 [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"], 我们的唯一单词集合表示为:<br>\n",
    "`(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)`\n",
    "\n",
    "假设 Byte-Pair Encoding 在此时停止训练, 学到的合并规则将应用于新单词(只要这些新单词不包括 不在基础词汇表中的符号 即可)。例如, 单词 \"bug\" 会被分词为 [\"b\", \"ug\"], 但是 \"mug\" 会被分词为 [\"`<unk>`\", \"ug\"], 因为符号 \"m\" 不在基础词汇表中。通常, 单字母, 例如 \"m\", 不会被\"`<unk>`\"符号替换, 因为训练数据通常至少包含每个字母一次, 但是对于表情符号等非常特殊的字符, 这种情况(被\"`<unk>`\"符号替换)可能会发生。\n",
    "\n",
    "如前所述, <font color=\"red\">词汇表的大小, 即基础词汇表的大小 + 合并的数量, 是一个需要配置的超参数</font>。例如, [GPT](https://huggingface.co/docs/transformers/main/zh/model_doc/gpt) 的词汇表大小是 40,478, 因为它有着 478 个基础字符, 并选择在 40,000 次合并后停止训练。\n",
    "\n",
    "#### Byte-level BPE\n",
    "<font color=\"red\">如果将所有 unicode 字符作为基础字符</font>, 包含所有可能的基础字符的基础词汇表可能会非常大。为了获得一个更好的基础词汇表, [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) <font color=\"red\"> 使用字节作为基础词汇表</font>, 这是一个巧妙的技巧, <font color=\"red\">迫使基础词汇表的大小为 256, 同时确保每个基础字符都包含在词汇表中</font>。通过一些额外的规则处理标点符号, GPT2 的分词器能对每个文本进行分词, <font color=\"red\">无需</font>`<unk>`符号。[GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 的<font color=\"red\">词汇表大小为 50,257, 对应于 256 字节的基础词元, 一个特殊的文本结束词元和通过 50,000 次合并学到的符号</font>。\n",
    "\n",
    "### WordPiece\n",
    "WordPiece 是子词分词算法, <font color=\"red\">用于 [BERT](https://huggingface.co/docs/transformers/main/zh/model_doc/bert), [DistilBERT](https://huggingface.co/docs/transformers/main/zh/model_doc/distilbert) 和 [Electra](https://huggingface.co/docs/transformers/main/zh/model_doc/electra)</font>。这个算法在 [Japanese and Korean Voice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) 中提出, 并与 BPE 非常相似。WordPiece 先初始化词汇表, 以包含训练数据中存在的每个字符, 然后逐步学习一个给定数量的合并规则。和 BPE 相比, <font color=\"red\">WordPiece 不选择最高频的符号对, 而是选择(加入到词汇表后能最大化训练数据似然值的)符号对</font>。\n",
    "\n",
    "所以这到底意味着什么? 参考前面的例子, 最大化训练数据的似然值, 等价于找到一个符号对, 其概率除以其第一个符号的概率, 接着除以第二个符号的概率, 在所有符号对中最大。例如, 如果 \"ug\" 除以 \"u\" 和 \"g\" 的概率, 大于任何其他符号对, \"u\" 后接 \"g\" 才会被合并。直觉上, WordPiece 与 BPE 略有不同, WordPiece 评估合并两个符号所造成的损失, 以确保这样做是值得的。\n",
    "\n",
    "### Unigram\n",
    "Unigram 是一个子词分词器算法, 由 [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf) 引入。和 BPE 或者 WordPiece 相比较, <font color=\"red\">Unigram 将其基础词汇初始化为大量符号, 并逐步缩减每个符号以获得较小的词汇表</font>。例如, 基础词汇表可以对应所有预分词的单词, 以及最常见的子字符串。<font color=\"red\">Unigram 没有直接用于 transformers 库中的任意模型, 但与 [SentencePiece](#SentencePiece) 一起联合使用</font>。\n",
    "\n",
    "在每个训练的步骤, Unigram 算法在当前词汇表的训练数据上定义损失函数(通常定义为对数似然), 和一个 Unigram 语言模型。然后, 对词汇表中的每个符号, 算法会计算如果从词汇表中移除该字符, 总损失会增加多少。然后, Unigram 移除百分之 $p$ 的符号($p$ 通常为 10% 或 20%), 这些符号的 loss 增加是最低的, 也就是说, 这些符号在训练数据上对总损失影响最小。重复该过程, 直到词汇表达到所需大小。<font color=\"red\">Unigram 算法始终保留基础字符, 以便可以对任何单词进行分词</font>。\n",
    "\n",
    "<font color=\"red\">因为 Unigram 不是基于合并规则(与 BPE 和 WordPiece 相比), 所以该算法在训练后有多种对新文本进行分词的方式</font>。例如, 如果一个已训练的 Unigram 分词器的词汇表如下:<br>\n",
    "`[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]`\n",
    "\n",
    "\"hugs\" 可以被分词成 [\"hug\", \"s\"], [\"h\", \"ug\", \"s\"] 或 [\"h\", \"u\", \"g\", \"s\"]。那么该选择哪一个? Unigram 在保存词汇表的基础上, 还保存训练语料库中每个词元的概率, 所以在训练后可以计算每个可能的分词结果的概率。该算法在实践中只是选择最有可能的分词结果, 但是也提供概率, 以根据它们的概率采样一个可能的分词结果。\n",
    "\n",
    "这些概率由分词器训练时的损失定义。假设训练数据包含单词 $x_{1},\\dots,x_{N}$, 一个单词$x_{i}$的所有可能的分词结果的集合定义为$S(x_{i})$, 则总损失定义为:\n",
    "$$ \\mathcal{L} = - \\sum_{i=1}^N \\log\\left( \\sum_{x \\in S(x_i)} p(x) \\right)$$\n",
    "\n",
    "### SentencePiece\n",
    "目前为止描述的所有分词算法都存在相同的问题: 假设输入文本使用空格分隔单词。然而, <font color=\"red\">不是所有语言都使用空格分隔单词</font>。一个可能的解决方案是使用某种语言专用的预分词器, 例如 [XLM](https://huggingface.co/docs/transformers/main/en/model_doc/xlm) 使用一个特定的中文、日语和泰语的预分词器。为了更通用地解决这个问题, [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) 将输入视为一个原始的输入流, 因此将空格包含在要使用的字符集中。然后, 它使用 BPE 或 Unigram 算法来构建合适的词汇表。\n",
    "\n",
    "例如, [XLNetTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetTokenizer) 使用 SentencePiece, <font color=\"red\">这也是为什么在前面的例子中\"▁\"符号被包含在词汇表中</font>。用 SentencePiece 解码非常容易, 因为所有词元可以被连接起来, 然后将\"▁\"替换成空格。\n",
    "\n",
    "库中使用 SentencePiece 的所有 transformers 模型都将其与 Unigram 组合使用。<font color=\"red\">使用 SentencePiece 的模型示例是 [ALBERT](https://huggingface.co/docs/transformers/main/zh/model_doc/albert), [XLNet](https://huggingface.co/docs/transformers/main/zh/model_doc/xlnet), [Marian](https://huggingface.co/docs/transformers/main/zh/model_doc/marian) 和 [T5](https://huggingface.co/docs/transformers/main/zh/model_doc/t5)</font>。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0dbf54-37f6-431b-9ab3-b20b4fd8357b",
   "metadata": {},
   "source": [
    "## Usage\n",
    "todo: https://huggingface.co/docs/transformers/main_classes/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b07eef9-0816-407b-93fe-25057912da53",
   "metadata": {},
   "source": [
    "### 加载方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73d35fd-3a7b-4fcc-b482-8fd74b306f14",
   "metadata": {},
   "source": [
    "- 从 Processor 加载 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658300b5-9c74-46cb-9fd7-2fb353de4086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Blip2Processor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "path_blip2 = \"./downloads/blip2-opt-2.7b/\"\n",
    "prompts = [\"a photo of a cat\", \"a photo of a dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771871a2-4429-4893-badb-4be5028d78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_processor 和 tokenizer 类名\n",
    "processor_auto = AutoProcessor.from_pretrained(path_blip2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1eb9c-96a8-4e49-89ec-5c1228437b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_auto.image_processor_class, processor_auto.tokenizer_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e10958-24a0-4378-a75b-1817701f6fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blip-2 处理器, 图像处理器及分词器的输出 keys\n",
    "processor_auto.model_input_names, processor_auto.image_processor.model_input_names, processor_auto.tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced3e96-a901-4d96-905b-4fd78b46ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 从 AutoProcessor 加载 tokenizer\n",
    "inputs_text_pa = processor_auto.tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "inputs_text_pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ef0e79-c91d-4ad0-bfb1-189d93fab978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 与前一个 cell 等效, 推荐\n",
    "inputs_text_pa2 = processor_auto(text=prompts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92815ac3-51ad-4655-8180-4ba6e7e0c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_blip2 = Blip2Processor.from_pretrained(path_blip2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012c108-763a-4d49-aa4a-edfb090d2959",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_blip2.image_processor_class, processor_blip2.tokenizer_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe2143a-2bdd-47c9-a03e-2726a7a648d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_blip2.model_input_names, processor_blip2.image_processor.model_input_names, processor_blip2.tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a882d92c-93d5-4c7c-8a0b-7c1be8cc2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 从 Blip2Processor 加载 tokenizer\n",
    "inputs_text_pb = processor_blip2.tokenizer(prompts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a65ed-b0f8-4696-b4d4-10a47bdceac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 与前一个 cell 等效\n",
    "inputs_text_pb2 = processor_blip2(text=prompts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99767788-1196-485f-96fb-ee1278349c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证\n",
    "((inputs_text_pa.input_ids == inputs_text_pa2.input_ids).all().item() and \n",
    " (inputs_text_pa.input_ids == inputs_text_pb.input_ids).all().item() and \n",
    " (inputs_text_pa.input_ids == inputs_text_pb2.input_ids).all().item() and \n",
    " (inputs_text_pa.attention_mask == inputs_text_pa2.attention_mask).all().item() and\n",
    " (inputs_text_pa.attention_mask == inputs_text_pb.attention_mask).all().item() and\n",
    " (inputs_text_pa.attention_mask == inputs_text_pb2.attention_mask).all().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74a004-8f72-4874-bf55-b595988b22a1",
   "metadata": {},
   "source": [
    "- 直接加载 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd244c-d6ac-4148-814f-2bdb4a9fdc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer, BertTokenizerFast\n",
    "\n",
    "# \"bert-base-uncased\" 指一个特定的预训练 BERT 模型, 该模型已在大型语料库上进行训练。\"uncased\" 部分表示此模型不区分大小写, 这意味着它将大写和小写字母视为相同。\n",
    "path_bert = \"bert-base-uncased\"\n",
    "cache_dir = \"./downloads/\"\n",
    "prompts = [\"a photo of a cat\", \"a photo of a dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef9ccd-c1f5-4fe9-8960-e28befe9412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 从 AutoTokenizer 加载 tokenizer\n",
    "tokenizer_auto = AutoTokenizer.from_pretrained(path_bert, cache_dir=cache_dir)\n",
    "inputs_text_ta = tokenizer_auto(prompts, padding=True, return_tensors=\"pt\")\n",
    "inputs_text_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde6106-b4a0-4eff-98b8-3ee7ca87fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 从 BertTokenizer 加载 tokenizer\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(path_bert, cache_dir=cache_dir)\n",
    "inputs_text_tb = tokenizer_bert(prompts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fdb02a-911f-414c-863c-3f61eb6a76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 从 BertTokenizerFast 加载 tokenizer\n",
    "tokenizer_bert_fast = BertTokenizerFast.from_pretrained(path_bert, cache_dir=cache_dir)\n",
    "inputs_text_tbf = tokenizer_bert_fast(prompts, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6e6e3-1f44-4e98-91db-e7e368778c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证\n",
    "((inputs_text_ta.input_ids == inputs_text_tb.input_ids).all().item() and \n",
    " (inputs_text_ta.input_ids == inputs_text_tbf.input_ids).all().item() and \n",
    " (inputs_text_ta.attention_mask == inputs_text_tb.attention_mask).all().item() and\n",
    " (inputs_text_ta.attention_mask == inputs_text_tbf.attention_mask).all().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91a995-cdfa-4633-b374-57a8a19951f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: BertTokenizer.from_pretrained(BERTMODEL, cache_dir=CACHE_DIR, do_lower_case=True)\n",
    "# https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer\n",
    "# https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecee15d-1aae-461b-b18b-fb33c9fb9dd1",
   "metadata": {},
   "source": [
    "### 保存本地 & 本地加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a966a-c431-4aae-989a-6346453132ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 processor 保存至本地\n",
    "processor_auto.save_pretrained(\"./temp\")\n",
    "# 从本地 通过 AutoProcessor 加载 tokenizer\n",
    "processor_local = AutoProcessor.from_pretrained(\"./temp/\")\n",
    "inputs_text_pl = processor_local.tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "# 从本地 通过 AutoTokenizer 加载 tokenizer\n",
    "tokenizer_loal = AutoTokenizer.from_pretrained(\"./temp/\")\n",
    "inputs_text_tl = tokenizer_loal(prompts, padding=True, return_tensors=\"pt\")\n",
    "# 验证\n",
    "((inputs_text_pl.input_ids == inputs_text_tl.input_ids).all().item() and\n",
    " (inputs_text_pl.input_ids == inputs_text_pa.input_ids).all().item() and\n",
    " (inputs_text_pl.attention_mask == inputs_text_tl.attention_mask).all().item() and\n",
    " (inputs_text_pl.attention_mask == inputs_text_pa.attention_mask).all().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f5c2e3-e811-4c90-93bb-0e240c64d6c3",
   "metadata": {},
   "source": [
    "### 输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cdb0c3-3cf9-4783-934b-b213c6f18e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = processor_auto.tokenizer # blip-2\n",
    "tokenizer = tokenizer_auto # bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256c920-8155-4184-960b-4ea430ac615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(prompts[0], padding=True, return_tensors=\"pt\") # 既适用于单提示本文输入, 又适用于批量提示文本输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4d5a78-d1ca-4ec3-9f20-de6498de4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode_plus(prompts[0], padding=True, return_tensors=\"pt\") # 仅适用于单提示本文输入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c257a9-01e7-4f18-96eb-44beda3bd139",
   "metadata": {},
   "source": [
    "`input_ids`是一个整数列表，其中每个整数对应于分词后的输入序列中的一个词元\n",
    "\n",
    "`token_type_ids`用于 BERT 等可以接受多个输入序列的模型, 例如涉及问答或自然语言推理的任务。在单序列任务中, `token_type_ids`中的所有值通常都设置为 0, 表示所有词元都属于同一输入序列。在双序列任务(例如问答)中, `token_type_ids`帮助模型区分来自第一个序列的词元和来自第二个序列的词元。对于第一个序列, `token_type_ids`设置为 0, 对于第二个序列, `token_type_ids`设置为 1。\n",
    "\n",
    "`attention_mask`表示输入序列中的哪些词元应该被模型注意, 哪些词元应被忽略。通常, [PAD] 词元被掩码为 0, 以防止模型注意填充词元。这可确保模型将注意力聚焦于序列的实际内容。\n",
    "\n",
    "refer to: https://medium.com/@manjindersingh_10145/sentiment-analysis-with-bert-using-huggingface-88e99deeec9a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd6aaf-7245-406f-ade6-2f749ef0ec7b",
   "metadata": {},
   "source": [
    "### 查看词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9e50d-cb07-4378-9f03-c26e39e0e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = processor_auto.tokenizer # blip-2\n",
    "tokenizer = tokenizer_auto # bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af34bfc-ebda-4bb9-a782-18045955628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词汇表\n",
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bce088f-293d-4886-91bf-69e46ae65a4b",
   "metadata": {},
   "source": [
    "### string, index seq, token seq 互转"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd58c64-856a-4f83-9eb8-27a3422f4836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = processor_auto.tokenizer # blip-2\n",
    "tokenizer = tokenizer_auto # bert\n",
    "\n",
    "prompts = [\"A photo of a Cat\", \"A photo of a Dog\"]\n",
    "# prompts = \"Don't you love Transformers? We sure do.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3242e633-9b6e-40d7-9ef5-8d2ba051fb15",
   "metadata": {},
   "source": [
    "#### string -> index seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d4a9b-88e4-4202-a861-4247add6db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(prompts) == str:\n",
    "    ids_w = tokenizer(prompts).input_ids # 既适用于单提示本文输入, 又适用于批量提示文本输入\n",
    "    print(\"string:\", prompts, \"--tokenizer--> index seq with special tokens =\", ids_w)\n",
    "    \n",
    "    ids_wo = tokenizer(prompts, add_special_tokens=False).input_ids # 既适用于单提示本文输入, 又适用于批量提示文本输入\n",
    "    print(\"\\nstring:\", prompts, \"--tokenizer--> index seq without special tokens =\", ids_wo)\n",
    "\n",
    "    out = tokenizer.encode(prompts) # 仅适用于单提示本文输入\n",
    "    print(\"\\nstring:\", prompts, \"--encode--> index seq with special tokens =\", out)\n",
    "    print(out == ids_w)\n",
    "    \n",
    "    out = tokenizer.encode(prompts, add_special_tokens=False) # 仅适用于单提示本文输入\n",
    "    print(\"\\nstring:\", prompts, \"--encode--> index seq without special tokens =\", out)\n",
    "    print(out == ids_wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafba69-1dbe-4306-9e57-e013efb25b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(prompts) == list:\n",
    "    ids_w = tokenizer(prompts).input_ids # 既适用于单提示本文输入, 又适用于批量提示文本输入\n",
    "    print(\"string:\", prompts, \"-> index seq with special tokens =\", ids_w)\n",
    "    \n",
    "    ids_wo = tokenizer(prompts, add_special_tokens=False).input_ids # 既适用于单提示本文输入, 又适用于批量提示文本输入\n",
    "    print(\"\\nstring:\", prompts, \"-> index seq without special tokens =\", ids_wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5dae27-7058-42af-a37c-17c5dbe16c15",
   "metadata": {},
   "source": [
    "#### index seq -> string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22524a4-336b-49e8-b367-949bab38bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(prompts) == str:\n",
    "    out = tokenizer.decode(ids_w) # 仅适用于单提示本文输入\n",
    "    print(\"index seq with special tokens:\", ids_w, \"--decode--> string:\", out)\n",
    "    \n",
    "    out = tokenizer.decode(ids_w, skip_special_tokens=True) # 仅适用于单提示本文输入\n",
    "    print(\"\\nindex seq with special tokens:\", ids_w, \"--decode--> string:\", out)\n",
    "    print(out == prompts.lower())\n",
    "    \n",
    "    out = tokenizer.batch_decode(ids_w, skip_special_tokens=True) # 仅适用于批量提示本文输入\n",
    "    print(\"\\nindex seq with special tokens:\", ids_w, \"--batch_decode--> ERROR:\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a75f35c-6e46-4a39-8d45-bcb18583b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(prompts) == list:\n",
    "    out = tokenizer.batch_decode(ids_w) # 仅适用于批量提示本文输入\n",
    "    print(\"index seq with special tokens:\", ids_w, \"-> token seq with special tokens:\", out)\n",
    "\n",
    "    out = tokenizer.batch_decode(ids_w, skip_special_tokens=True) # 仅适用于批量提示本文输入\n",
    "    print(\"\\nindex seq with special tokens:\", ids_w, \"-> token seq without special tokens:\", out)\n",
    "    print(out == [string.lower() for string in prompts])\n",
    "    \n",
    "    out = tokenizer.batch_decode(ids_wo) # 仅适用于批量提示本文输入\n",
    "    print(\"\\nindex seq without special tokens:\", ids_wo, \"-> token seq without special tokens:\", out)\n",
    "    print(out == [string.lower() for string in prompts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd510b-afe4-4d06-b6e3-6724b7b8d714",
   "metadata": {},
   "source": [
    "#### index seq -> token seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4b58c-3e75-441a-9c26-82f874628a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(prompts) == str:\n",
    "    tokens_w = tokenizer.convert_ids_to_tokens(ids_w) # 仅适用于单提示本文输入\n",
    "    print(\"index seq with special tokens:\", ids_w, \"-> token seq with special tokens:\", tokens_w)\n",
    "\n",
    "    tokens_wo = tokenizer.convert_ids_to_tokens(ids_w, skip_special_tokens=True) # 仅适用于单提示本文输入\n",
    "    print(\"\\nindex seq with special tokens:\", ids_w, \"-> token seq without special tokens:\", tokens_wo)\n",
    "\n",
    "    out = tokenizer.convert_ids_to_tokens(ids_wo) # 仅适用于单提示本文输入\n",
    "    print(\"\\nindex seq without special tokens:\", ids_wo, \"-> token seq without special tokens:\", out)\n",
    "    print(out == tokens_wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf115ad-63c2-447e-a241-93fe15b4efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(prompts) == list:\n",
    "    tokens_w = [tokenizer.convert_ids_to_tokens(ids_w[index]) for index in range(len(prompts))]\n",
    "    tokens_wo = [tokenizer.convert_ids_to_tokens(ids_w[index], skip_special_tokens=True) for index in range(len(prompts))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd59328-721d-44d4-b13e-f89321a578ab",
   "metadata": {},
   "source": [
    "#### token seq -> index seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ef25d-f55f-4aa4-8409-81258df0b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(prompts) == str:\n",
    "    out = tokenizer.convert_tokens_to_ids(tokens_w) # 仅适用于单提示本文输入 ?todo\n",
    "    print(\"token seq with special tokens =\", tokens_w, \"-> index seq with special tokens =\", out)\n",
    "    print(out == ids_w)\n",
    "\n",
    "    out = tokenizer.convert_tokens_to_ids(tokens_wo) # 仅适用于单提示本文输入 ?todo\n",
    "    print(\"\\ntoken seq without special tokens =\", tokens_wo, \"-> index seq without special tokens =\", out)\n",
    "    out == ids_wo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063cfc40-601e-4e68-980e-0d48cb7994b1",
   "metadata": {},
   "source": [
    "#### string -> token seq\n",
    "分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd51d26-81e9-4fa7-aa7c-29e09d3a7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(prompts) == str:\n",
    "    out = tokenizer.tokenize(prompts, add_special_tokens=True) # 仅适用于单提示本文输入\n",
    "    print(\"string:\", prompts, \"-> token seq with special tokens:\", out)\n",
    "    print(out == tokens_w)\n",
    "    \n",
    "    out = tokenizer.tokenize(prompts) # 仅适用于单提示本文输入\n",
    "    print(\"\\nstring:\", prompts, \"-> token seq without special tokens:\", out)\n",
    "    print(out == tokens_wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fc336-0c51-4e9b-b0e9-56bb21f3f8ed",
   "metadata": {},
   "source": [
    "#### token seq -> string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60358a1b-f163-4499-9f32-d671d90647ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(prompts) == str:\n",
    "    out = tokenizer.convert_tokens_to_string(tokens_w)\n",
    "    print(\"token seq with special tokens =\", tokens_w, \"-> string:\", out)\n",
    "\n",
    "    out = tokenizer.convert_tokens_to_string(tokens_wo)\n",
    "    print(\"\\ntoken seq without special tokens =\", tokens_wo, \"-> string:\", out)\n",
    "    print(out == prompts.lower())\n",
    "    if out != prompts.lower():\n",
    "        print(\"convert_tokens_to_string:\\t\", out)\n",
    "        print(\"original string:\\t\\t\", prompts.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4cf839-bcad-4b56-a675-6b8edc83631a",
   "metadata": {},
   "source": [
    "### padding vs truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03f552-9867-479e-9521-99cd52c82181",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = \"Don't you love Transformers? We sure do.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a82802e-7ed6-4fdf-95f5-3e91743c9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(prompts, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f76981-ce5d-4649-9d64-8c1b7099e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充\n",
    "print( tokenizer.encode(prompts, add_special_tokens=False, max_length=20, padding=\"max_length\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643071e-fea9-43e2-a2a7-aec07d5dec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 截断\n",
    "tokenizer.encode(prompts, add_special_tokens=False, max_length=5, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5c0e5-5725-4d03-b10c-f1aaa0ce1295",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(tokenizer.encode(prompts, add_special_tokens=False)) == \n",
    " sum(tokenizer.encode_plus(prompts, add_special_tokens=False, max_length=20, padding=\"max_length\").attention_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75184e8-ecab-4bbd-9c8a-562c724d6fdd",
   "metadata": {},
   "source": [
    "### loop vs batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e1a02-9b79-4853-b629-04f8cb9d8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_loop = 10000\n",
    "prompts = [\"Don't you love Transformers? We sure do.\"] * num_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd378d-deba-4b7e-be69-9386576800b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(num_loop):\n",
    "    tokenizer_bert(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2906967c-ac9c-45ea-bc1c-ae58a16d304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inputs = tokenizer_bert(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67542d6a-f60b-4e33-837d-13aad7a326b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(num_loop):\n",
    "    tokenizer_bert_fast(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3689dd-1da1-4531-968b-dea2376d5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inputs = tokenizer_bert_fast(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b51e0-9480-4494-8f11-8df0d2438a07",
   "metadata": {},
   "source": [
    "Notice: 以上时间耗时对比, 不考虑 prompts 构造时间。`BertTokenizerFast`使用 C++ 实现, 速度较快; 而且自带`return_offsets_mapping`参数, 可以在NER任务中更好地找到词元在句子中的位置。\n",
    "\n",
    "> Note: 魔法命令\n",
    "> - `%%time`:  当前 cell 运行一次的时间花费\n",
    "> - `%timeit`: 当前 cell 运行100，000次(默认), 其中最快3次的平均时间花费\n",
    "> - `%time`:   当前行运行一次的时间花费\n",
    "> - `timeit`:  当前行运行100，000次(默认), 其中最快3次的平均时间花费"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f1b41e-60be-4f63-abde-53e9f5e9868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "inputs = tokenizer_bert_fast(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5bf788-e1e0-47e1-85af-1f05a4dbb862",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time inputs = tokenizer_bert_fast(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b95aaaa-ebec-4aa1-86f1-3152cdccb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit inputs = tokenizer_bert_fast(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d1b00-c672-41fd-a0e1-ed6180c22483",
   "metadata": {},
   "source": [
    "### return_offsets_mapping\n",
    "`return_offsets_mapping`：在做序列标注、信息抽取等任务时, 我们获取的原始数据标签是严格对应于原始的文本字符, 于是在 tokenizer 处理后位置会变得不一样, 因此需要返回`offset_mapping`, 知道被处理后的每个词元是对应于原始的哪些字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56551770-dda0-4081-9fda-bbb93fabed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = \"Don't you love Transformers? We sure do.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8782416-9cd0-413c-ae44-ed82e8e07b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert_fast(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432c11a-14db-489e-bb6e-4cd2038104ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bert_fast(prompts, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26560c-8475-4c22-9aa8-edc827dd07bf",
   "metadata": {},
   "source": [
    "todo: `return_offsets_mapping`具体何时使用?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07355454-d54a-4bd5-a974-de84f26eaadf",
   "metadata": {},
   "source": [
    "# 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54298a15-3a84-4321-a7cf-8329b6fe2cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72bf0849b444ddba7b4c418ef1db860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Blip2Model\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1.1.加载处理器\n",
    "processor = AutoProcessor.from_pretrained(\"/home/qj00182/robot/EmbodiedGPT_Pytorch/downloads/blip2-opt-2.7b/\")\n",
    "# 1.2.加载模型\n",
    "model = Blip2Model.from_pretrained(\"/home/qj00182/robot/EmbodiedGPT_Pytorch/downloads/blip2-opt-2.7b-fp16-sharded/\", device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd5c759-f0b5-4fd2-9324-10f4b8ec752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape = torch.Size([1, 11])\n",
      "attention_mask shape = torch.Size([1, 11])\n"
     ]
    }
   ],
   "source": [
    "# 2.1.仅预处理文本\n",
    "prompts = \"Question: how many cats are there? Answer:\"\n",
    "inputs_text = processor(text=prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "for key, value in inputs_text.items():\n",
    "    print(key, \"shape =\", value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c68a236-9e37-4a7e-896f-411f2f536319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values shape = torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 2.2.仅预处理图像\n",
    "image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n",
    "inputs_image = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "for key, value in inputs_image.items():\n",
    "    print(key, \"shape =\", value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1e89cc-10e8-4f89-ba34-9eb9b3150709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values shape = torch.Size([1, 3, 224, 224])\n",
      "input_ids shape = torch.Size([1, 11])\n",
      "attention_mask shape = torch.Size([1, 11])\n"
     ]
    }
   ],
   "source": [
    "# 2.3.同时预处理图像和文本\n",
    "inputs_both = processor(images=image, text=prompts, padding=True, return_tensors=\"pt\").to(device)\n",
    "for key, value in inputs_both.items():\n",
    "    print(key, \"shape =\", value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8d5a7b6-dd1c-48b4-99d0-931dc27a798b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = ['last_hidden_state', 'pooler_output']\n",
      "last_hidden_state shape = torch.Size([1, 257, 1408])\n",
      "pooler_output shape = torch.Size([1, 1408])\n"
     ]
    }
   ],
   "source": [
    "# 3.1.获取视觉模型的输出\n",
    "outputs_image = model.get_image_features(**inputs_image)\n",
    "print(\"keys =\", list(outputs_image.keys()))\n",
    "for key in outputs_image.keys():\n",
    "    print(key, \"shape =\", outputs_image[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3c4513c-92c7-478e-8d63-2795b71f2c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = ['last_hidden_state', 'pooler_output']\n",
      "last_hidden_state shape = torch.Size([1, 32, 768])\n",
      "pooler_output shape = torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# 3.2.获取 Q-Former 的输出\n",
    "outputs_qformer = model.get_qformer_features(**inputs_image)\n",
    "print(\"keys =\", list(outputs_qformer.keys()))\n",
    "for key in outputs_qformer.keys():\n",
    "    print(key, \"shape =\", outputs_qformer[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eae614e4-cd6b-45e9-b97e-ee07e59f6654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = ['logits', 'past_key_values']\n",
      "logits shape = torch.Size([1, 11, 50272])\n",
      "index = 0 , key shape = torch.Size([1, 32, 11, 80]) , value shape = torch.Size([1, 32, 11, 80])\n",
      "index = 8 , key shape = torch.Size([1, 32, 11, 80]) , value shape = torch.Size([1, 32, 11, 80])\n",
      "index = 16 , key shape = torch.Size([1, 32, 11, 80]) , value shape = torch.Size([1, 32, 11, 80])\n",
      "index = 24 , key shape = torch.Size([1, 32, 11, 80]) , value shape = torch.Size([1, 32, 11, 80])\n"
     ]
    }
   ],
   "source": [
    "# 3.3.获取语言模型的输出\n",
    "outputs_text = model.get_text_features(**inputs_text)\n",
    "print(\"keys =\", list(outputs_text.keys()))\n",
    "print(\"logits shape =\", outputs_text.logits.shape)\n",
    "for index in range(len(outputs_text.past_key_values)):\n",
    "    if index % 8 == 0:\n",
    "        print(\"index =\", index, \", key shape =\", outputs_text.past_key_values[index][0].shape, \", value shape =\", outputs_text.past_key_values[index][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3527198b-9ece-4e40-bff8-31fa22e8d88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = ['logits', 'vision_outputs', 'qformer_outputs', 'language_model_outputs']\n"
     ]
    }
   ],
   "source": [
    "# 3.4. 获取全部输出\n",
    "outputs_all = model(**inputs_both)\n",
    "print(\"keys =\", list(outputs_all.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68e06fde-7acc-4768-ab7e-b33edd2ac9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state True\n",
      "pooler_output True\n"
     ]
    }
   ],
   "source": [
    "for key in outputs_all.vision_outputs.keys():\n",
    "    print(key, (outputs_all.vision_outputs[key] == outputs_image[key]).all().item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7463a3ec-7e3b-4468-b1c0-176566a88a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state True\n",
      "pooler_output True\n"
     ]
    }
   ],
   "source": [
    "for key in outputs_all.qformer_outputs.keys():\n",
    "    print(key, (outputs_all.qformer_outputs[key] == outputs_qformer[key]).all().item()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2827139-2cd5-410c-8b37-c8e6f97f292a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_all.language_model_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbc85cca-abcd-4e23-b7ef-68681fbebf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(outputs_all.logits == outputs_all.language_model_outputs.logits).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3f4a6ac-df66-4eec-a263-829bfa252268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 0 , key shape = torch.Size([1, 32, 43, 80]) , value shape = torch.Size([1, 32, 43, 80])\n",
      "index = 8 , key shape = torch.Size([1, 32, 43, 80]) , value shape = torch.Size([1, 32, 43, 80])\n",
      "index = 16 , key shape = torch.Size([1, 32, 43, 80]) , value shape = torch.Size([1, 32, 43, 80])\n",
      "index = 24 , key shape = torch.Size([1, 32, 43, 80]) , value shape = torch.Size([1, 32, 43, 80])\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(outputs_all.language_model_outputs.past_key_values)):\n",
    "    if index % 8 == 0:\n",
    "        print(\"index =\", index,\n",
    "              \", key shape =\", outputs_all.language_model_outputs.past_key_values[index][0].shape,\n",
    "              \", value shape =\", outputs_all.language_model_outputs.past_key_values[index][1].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.72px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

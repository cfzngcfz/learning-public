{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95ae8f0",
   "metadata": {},
   "source": [
    "Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., ... & Finn, C. (2024). [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246). arXiv preprint arXiv:2406.09246.\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.\n",
    "\n",
    "在互联网规模的视觉语言数据和多样化的机器人演示的基础上预先训练的大型策略有可能改变我们教授机器人新技能的方式：我们可以微调这种视觉语言动作 (VLA) 模型，以获得稳健、可推广的视觉运动控制策略，而不是从头开始训练新行为。\n",
    "\n",
    "然而，机器人技术广泛采用 VLA 一直具有挑战性，因为 1) 现有的 VLA 基本上是封闭的，无法向公众开放，2) 先前的工作未能探索有效微调 VLA 以完成新任务的方法，这是采用的关键要素。\n",
    "\n",
    "为应对这些挑战，我们推出了 OpenVLA，这是一个 7B 参数的开源 VLA，在 970k 个现实世界机器人演示的多样化集合上进行了训练。\n",
    "\n",
    "OpenVLA 建立在 Llama 2 语言模型的基础上，结合了融合了 DINOv2 和 SigLIP 预训练特征的视觉编码器。\n",
    "\n",
    "作为增加的数据多样性和新模型组件的产物，OpenVLA 在通用操作方面表现出色，在 29 项任务和多个机器人实例中的绝对任务成功率比 RT-2-X (55B) 等封闭模型高出 16.5%，参数减少了 7 倍。\n",
    "\n",
    "我们进一步表明，我们可以有效地微调 OpenVLA 以适应新设置，在涉及多个对象和强大语言基础能力的多任务环境中具有特别强大的泛化效果，并且比 Diffusion Policy 等富有表现力的从头开始的模仿学习方法高出 20.4%。\n",
    "\n",
    "我们还探索了计算效率；作为一项单独的贡献，我们表明 OpenVLA 可以通过现代低秩自适应方法在消费者 GPU 上进行微调，并通过量化高效地提供服务，而不会影响下游成功率。\n",
    "\n",
    "最后，我们发布了模型检查点、微调笔记本和我们的 PyTorch 代码库，内置支持在 Open X-Embodiment 数据集上大规模训练 VLA。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

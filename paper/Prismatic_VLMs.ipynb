{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3455fbbe-fddf-4869-bf19-a7c909aeb228",
   "metadata": {},
   "source": [
    "Karamcheti, S., Nair, S., Balakrishna, A., Liang, P., Kollar, T., & Sadigh, D. (2024). [Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models](https://arxiv.org/abs/2402.07865). arXiv preprint arXiv:2402.07865.\n",
    "\n",
    "**Abstract**\n",
    "<!-- Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance − a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization, and challenge sets that probe properties such as hallucination; evaluations that provide fine-grained insight to VLM capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and training from base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible training code, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open VLMs. -->\n",
    "视觉条件语言模型 (VLMs) 已在视觉对话、场景理解和机器人任务规划等应用程序中得到越来越广泛的采用; 这种应用催生了大量新模型, 例如 LLaVa、InstructBLIP 和 PaLI-3。尽管新发布数量众多, 但围绕图像预处理、架构和优化的关键设计决策尚未得到充分探索, 使得理解哪些因素影响模型性能变得具有挑战性——缺乏客观、一致的评估使这一挑战更加复杂。为了解决这些差距, 我们首先编制了一套标准化评估, 涵盖视觉问答、物体定位和探索幻觉等属性的挑战集; 评估提供对VLM能力的细粒度洞察。其次, 我们沿关键设计轴严格调查 VLM, 包括预训练的视觉表征和从基础语言模型与指令调整语言模型进行的训练等。我们将我们的分析与三个资源贡献相结合: (1) 用于评估 VLMs 的统一框架, (2) 优化的、灵活的训练代码, (3) 所有模型的检查点, 包括 7-13B 规模的 VLMs 系列, 其严格优于开源 VLMs 中最先进的 InstructBLIP 和 LLaVa v1.5。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc8ac9-b66a-4286-b02d-dcafcf90bdba",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<!-- Visually-conditioned language models (VLMs) generate natural language responses from image input and text prompts, providing a general, expressive interface for a growing spectrum of applications – grounded chat (Li et al., 2023c; Gong et al., 2023), visual programming (Sur'is et al., 2023; Subramanian et al., 2023), robotic control (Driess et al., 2023; Brohan et al., 2023), etc. This broad adoption is fueled by a recent paradigm shift in how we develop VLMs; eschewing the complex architectures and training objectives of prior work (Tan & Bansal, 2019; Li et al., 2022; 2023b), new VLMs adopt a simple approach, treating patch features from pretrained visual backbones (e.g., CLIP; Radford et al., 2021) as tokens that can be projected into the input space of a language model (LM). This \"patch-as-token\" approach enables training with a simple objective – next-token prediction – and allows us to harness the ecosystem of powerful LMs such as Llama-2 and Mistral (Touvron et al., 2023; Jiang et al., 2023), along with the tools to efficiently train them (e.g., FSDP; Zhao et al., 2023). This combination has fueled the rapid development and release of models such as LLaVa v1.5, and PALI-3 that adopt the same underlying recipe, while varying individual ingredients such as the choice of pretrained components, data, or optimization procedure (Liu et al., 2023b; Chen et al., 2023b). -->\n",
    "视觉条件语言模型 (VLMs) 从图像输入和文本提示生成自然语言响应, 为日益广泛的应用提供通用且富有表现力的接口——基础聊天 (Li et al., 2023c; Gong et al., 2023)、可视化编程 (Sur'is et al., 2023; Subramanian et al., 2023)、机器人控制 (Driess et al., 2023; Brohan et al., 2023) 等。最近我们开发 VLM 方式的范式转变推动了这种广泛采用; 避开先前工作的复杂架构和训练目标 (Tan & Bansal, 2019; Li et al., 2022; 2023b), 新的 VLMs 采用简单的方法, 将预训练视觉 backbones (e.g., CLIP; Radford et al., 2021) 的 patch 特征视为词元, 它可以被投影到语言模型 (LM) 的输入空间。这种\"patch 即词元\"方法能够以简单的目标(下一个词元预测)进行训练, 并允许我们利用强大的 LMs (例如 Llama-2 和 Mistral)的生态系统 (Touvron et al., 2023; Jiang et al., 2023), 以及有效训练它们的工具(例如 FSDP; Zhao et al., 2023)。这种结合推动了 LLaVa v1.5 和 PALI-3 等模型的快速开发和发布, 这些模型采用相同的底层方法, 同时改变各个成分, 例如预训练组件、数据或优化程序的选择 (Liu et al., 2023b; Chen et al., 2023b)。\n",
    "\n",
    "<!-- Unfortunately, existing approaches only cover a sliver of the design space around building and training VLMs, without thoroughly evaluating the impact of given choices on downstream capabilities. This motivates the key question of this work: what are the key design decisions that influence VLM capabilities and downstream use? To provide answers to this question, we first need a way to **thoroughly evaluate** the strengths and weaknesses of a given model. Doing this effectively requires compiling a standardized evaluation suite comprised of tasks that are diverse and objective; crucially, these tasks should allow for probing specific capabilities such as spatial reasoning, out-of-distribution generalization, and commonsense understanding, amongst others. Second, we need to **rigorously explore** different VLM design axes, not only to build a concrete set of recommendations, but to tie individual choices to downstream performance. -->\n",
    "不幸的是, 现有的方法仅涵盖一小部分(围绕构建和训练 VLMs 的)设计空间, 没有彻底评估给定的选择对下游能力的影响。这激发了这项工作的关键问题: 影响 VLM 能力和下游使用的关键设计决策是什么? 为了回答这个问题, 我们首先需要一种方法以**彻底评估**给定模型的优势和劣势。有效地做到这一点, 需要编制一个标准化的评估套件, 包含多样化和客观的任务; 至关重要的是, 这些任务应该允许探索特定能力, 例如空间推理、分布外泛化和常识理解等。其次, 我们需要**严格探索**不同的 VLM 设计轴, 不仅要建立一套具体的建议, 而且要将个人选择与下游绩效联系起来。\n",
    "\n",
    "<!-- This work addresses these axes through four contributions. First, to provide fine-grained insight into VLM capabilities, **we compile a standardized evaluation suite** comprised of twelve benchmarks from the vision-and-language literature, including four tasks spanning visual question answering (Bigham et al., 2010; Goyal et al., 2017; Hudson & Manning, 2019; Singh et al., 2019), four tasks spanning object localization (Kazemzadeh et al., 2014; Yu et al., 2016; Wang et al., 2021), and four challenge tasks evaluating fine-grained spatial reasoning, hallucination, and diagram understanding (Acharya et al., 2018; Liu et al., 2022; Li et al., 2023d; Kembhavi et al., 2016). Second, we develop an optimized and modular codebase for VLM training that emphasizes flexibility, allowing users to easily swap in pretrained components, optimization procedures, data, and more (Fig. 2; right). Third, we use these resource contributions to perform **targeted experiments exploring four key design axes** (Fig. 2; left): 1) optimization procedure, 2) image processing and visual representations, 3) language models, and 4) scaling training time and data. We identify a number of insights; for example, we find that multi-stage training procedures adopted by existing work can be eliminated without impact on performance, reducing compute costs by 20-25%. We also find that fused visual backbones that merge features from different backbones such as CLIP (Radford et al., 2021) and DINOv2 (Oquab et al., 2023) lead to more performant VLMs across the board. Finally, we consolidate our findings and train a family of models – PRISMs – at the 7B/13B scale that **strictly outperform state-of-the-art open VLMs** such as InstructBLIP and LLaVa v1.5. -->\n",
    "这项工作通过四个贡献解决这些(设计)轴。首先, 为了提供对 VLM 能力的细粒度洞察, **我们编制了一个标准化评估套件**, 由来自视觉和语言文献的 12 个基准组成, 包括四个视觉问答的任务(Bigham et al., 2010; Goyal et al., 2017; Hudson & Manning, 2019; Singh et al., 2019), 四个物体定位的任务(Kazemzadeh et al., 2014; Yu et al., 2016; Wang et al., 2021), 以及四个评估细粒度空间推理、幻觉和图表理解的挑战任务(Acharya et al., 2018; Liu et al., 2022; Li et al., 2023d; Kembhavi et al., 2016)。第二, **我们开发了一个优化的且模块化的 VLM 训练代码库**, 它强调灵活性, 允许用户轻松地替换预训练组件、优化程序、数据等([图2](#fig.2); 右)。第三, 我们使用这些资源贡献进行**有针对性的实验, 探索四个关键设计轴**([图2](#fig.2); 左): 1) 优化程序, 2) 图像处理和视觉表征, 3) 语言模型, 4) 缩放训练时间和数据。我们发现了一些见解; 例如, 我们发现可以淘汰现有工作采用的多阶段训练程序, 不会影响性能, 计算成本降低 20-25%。我们还发现, 融合视觉 backbones 将全面提高 VLM 的性能, 该 backbones 融合了不同 backbones 的特征, 例如 CLIP (Radford et al., 2021) 和 DINOv2 (Oquab et al., 2023)。最后, 我们整合我们的发现, 并在 7B/13B 规模上训练了一系列模型(PRISM)<sup>[1](#superscript.1)</sup>, 其**严格优于最先进的开源 VLMs**, 例如 InstructBLIP 和 LLaVa v1.5。\n",
    "\n",
    "<!-- We release our optimized training codebase, evaluation suite, and checkpoints for all models trained as part of this work. -->\n",
    "> <sup>1</sup><span id='superscript.1'></span> 作为这项工作的一部分, 我们发布了优化的训练代码库、评估套件和所有训练模型的检查点。<br> https://github.com/TRI-ML/prismatic-vlms, https://github.com/TRI-ML/vlm-evaluation\n",
    "\n",
    "todo: 图2\n",
    "<!-- Figure 2. **Exploring VLM Design Axes**. We explore four key design axes for developing VLMs: 1) optimization procedure, 2) image processing and pretrained visual representations, 3) language models, and 4) scaling properties around training time and data (left). To enable this exploration, we make a key resource contribution: an open-source, flexible codebase for efficiently training VLMs (right). -->\n",
    "图2. **探索 VLM 设计轴**。我们探索开发 VLMs 的四个关键设计轴: 1) 优化程序, 2) 图像处理和预训练视觉表征, 3) 语言模型, 4) 围绕训练时间和数据的缩放属性(左)。为了实现这一探索, 我们做出一项关键资源贡献: 一个用于高效训练 VLMs 的开源、灵活的代码库(右)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd9901-aecd-47c7-b51c-723816e7053a",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "<!-- To ground our analysis, we require 1) a VLM model architecture, 2) pretraining data, and 3) a training implementation. -->\n",
    "为了开展分析, 我们需要 1) VLM 模型架构, 2) 预训练数据, 3) 训练实施。\n",
    "\n",
    "<!-- **Model Architecture**. We adopt the general architecture used by many recent VLMs, such as LLaVa, Qwen-VL, and PaLI-3 (Liu et al., 2023c; Bai et al., 2023; Chen et al., 2023b). These architectures use a (pretrained) visual backbone to map an input image to a sequence of patch features that are then projected individually into the embedding space of an LM. Formally, a VLM takes as input an image $x_\\text{img} \\in \\mathbb{R}^{H \\times W}$ and text prompt tokens $u_\\text{prompt}$ with arbitrary sequence length $K$. These inputs are then fed to the following components: 1) a visual representation backbone, 2) a vision-language projector, and 3) a language model. -->\n",
    "**模型架构**。我们采用许多最近的 VLMs 使用的通用架构, 例如 LLaVa, Qwen-VL, 和 PaLI-3 (Liu et al., 2023c; Bai et al., 2023; Chen et al., 2023b)。这些架构使用一个(预训练的)视觉 backbone 将输入图像映射到 patch 特征序列, 然后这些 patch 特征被逐个投影到 LM 的嵌入空间。正式地, VLM 将图像 $x_\\text{img} \\in \\mathbb{R}^{H \\times W}$ 和(具有任意序列长度 $K$ 的)文本提示词元 $u_\\text{prompt}$ 作为输入。然后这些输入被传输到以下组件: 1) 视觉表征 backbone, 2) 视觉-语言投影器, 3) 语言模型。\n",
    "\n",
    "<!-- *Visual Representation*. We first process $x_\\text{img}$ subject to a visual representation backbone $V_\\omega$ that outputs a sequence of features $p_\\text{img} \\in \\mathbb{R}^{L \\times h_\\text{vision}}$ where $p_\\text{img} = V_\\omega(x_\\text{img})$. As an example, $p_\\text{img}$ might be the patch features output by a Vision Transformer (ViT; Dosovitskiy et al., 2021). -->\n",
    "*视觉表征*。我们首先根据视觉表征 backbone $V_\\omega$ 处理 $x_\\text{img}$, 该 backbone 输出特征序列 $p_\\text{img} \\in \\mathbb{R}^{L \\times h_\\text{vision}}$, 其中 $p_\\text{img} = V_\\omega(x_\\text{img})$。例如, $p_\\text{img}$ 可能是 Vision Transformer (ViT; Dosovitskiy et al., 2021) 输出的 patch 特征。\n",
    "\n",
    "<!-- *Vision-Language Projector*. Next, we map $p_\\text{img}$ to a sequence of *embeddings* $e_\\text{img} \\in \\mathbb{R}^{L \\times h_\\text{text}}$ via a learned projector $F_\\psi$, where $e_\\text{img} = F_\\psi(p_\\text{img})$. -->\n",
    "*视觉-语言投影器*。接下来, 我们通过学习到的投影器 $F_\\psi$, 将 $p_\\text{img}$ 映射到*嵌入*序列 $e_\\text{img} \\in \\mathbb{R}^{L \\times h_\\text{text}}$, 其中 $e_\\text{img} = F_\\psi(p_\\text{img})$。\n",
    "\n",
    "<!-- *Language Model*. Finally, we concatenate the sequence $e_\\text{img}$ with the text prompt embeddings $e_\\text{prompt} = \\text{embed}(u_\\text{prompt})$, passing the result to the language model. The language model generates output text $u_\\text{gen} = \\text{LM}_\\theta \\left( \\left[ e_\\text{img}; e_\\text{prompt} \\right] \\right)$. -->\n",
    "*语言模型*。最后, 我们将序列 $e_\\text{img}$ 与文本提示嵌入 $e_\\text{prompt} = \\text{embed}(u_\\text{prompt})$ 连接起来, 将结果传递给语言模型。语言模型生成输出文本 $u_\\text{gen} = \\text{LM}_\\theta \\left( \\left[ e_\\text{img}; e_\\text{prompt} \\right] \\right)$。\n",
    "\n",
    "<!-- The composition $\\text{LM}_\\theta \\left( \\left[F_\\psi \\left( V_\\omega(o_\\text{rgb}) \\right); \\text{embed} \\left( u_\\text{prompt} \\right) \\right] \\right)$ then defines a VLM. Given a triple $\\left( x_\\text{img}, u_\\text{prompt}, \\hat{u}_\\text{gen} \\right)$ during training, we minimize the loss $L \\left( \\omega, \\psi, \\theta \\right) = - \\log p \\left( \\hat{u}_\\text{gen} | x_\\text{img}, u_\\text{prompt} \\right)$ via gradient descent. -->\n",
    "组合 $\\text{LM}_\\theta \\left( \\left[F_\\psi \\left( V_\\omega(o_\\text{rgb}) \\right); \\text{embed} \\left( u_\\text{prompt} \\right) \\right] \\right)$ 定义了一个 VLM。在训练期间给定一个三元组 $\\left( x_\\text{img}, u_\\text{prompt}, \\hat{u}_\\text{gen} \\right)$, 我们通过梯度下降最小化损失 $L \\left( \\omega, \\psi, \\theta \\right) = - \\log p \\left( \\hat{u}_\\text{gen} | x_\\text{img}, u_\\text{prompt} \\right)$。\n",
    "\n",
    "<!-- **Pretraining Dataset**. We limit our selection of pretraining data to datasets that are fully open-source (e.g., under permissive research licenses), and that have been used in prior work. Specifically, we use the LLaVa v1.5 data mixture, which consists of two subsets used for a multi-stage training pipeline. The first subset consists of a 558K sample mixture of examples sourced from various captioning datasets (e.g., Conceptual Captions, LAION Sharma et al., 2018; Schuhmann et al., 2021), while the second consists of 665K multimodal instruct tuning examples comprised of synthetic data generated in Liu et al. (2023c), as well as examples from existing vision-language training sets (e.g., GQA, TextCaps; Hudson & Manning, 2019; Sidorov et al., 2020), and notably, a sample of language-only data from ShareGPT (ShareGPT, 2023). We provide a comprehensive breakdown of the pretraining data mixture in §A.1. -->\n",
    "**预训练数据集**。我们将预训练数据的选择限制在完全开源(例如, 在宽松的研究许可下)且已在先前工作中使用过的数据集。具体而言, 我们使用 LLaVa v1.5 数据混合, 它包含用于多阶段训练流水线的两个子集。第一个子集包含源自各种字幕数据集(例如, Conceptual Captions, LAION Sharma et al., 2018; Schuhmann et al., 2021)的 558K 示例样本混合, 第二个子集包含 Liu et al. (2023c) 生成的由合成数据组成的 665K 个多模式指令调整示例, 以及来自现有视觉-语言训练集 (例如, GQA, TextCaps; Hudson & Manning, 2019; Sidorov et al., 2020) 的示例, 尤其是来自 ShareGPT (ShareGPT, 2023) 的纯语言数据样本。我们在 $\\S$ [A.1](#todo) 中提供了预训练数据混合的全面分解。\n",
    "\n",
    "<!-- **Training Implementation & Verification**. To investigate the design axes enumerated in §1, we require code for VLM training that is efficient and flexible; critically, we need the ability to easily swap out vision and LM backbones and handle arbitrary optimization procedures (e.g., freezing the vision backbone during training). With these requirements, we implement our training codebase in PyTorch, using Fully Sharded Data Parallel (FSDP; Zhao et al., 2023) and BF16 mixed precision. FSDP lets us specify precision for individual model components (e.g., FP16 for vision backbones, BF16 for LMs), enables portability to different hardware, and provides minimal implementation overhead. Following reproducibility practices from prior work (Karamcheti et al., 2021; Biderman et al., 2023), we fix initialization randomness and fix batch order during training. We leverage TIMM (Wightman, 2019) and Hugging Face Transformers (Wolf et al., 2019) to provide pretrained models. -->\n",
    "**训练实施和验证**。为了研究 $\\S$ [1](#Introduction) 中列举的设计轴, 我们需要高效灵活的 VLM 训练代码; 至关重要的是, 我们需要能够轻松替换视觉和 LM backbones 并处理任意优化程序(例如, 在训练期间冻结视觉 backbone)。针对这些要求, 我们在 PyTorch 中实现了我们的训练代码库, 使用 Fully Sharded Data Parallel (FSDP; Zhao et al., 2023) 和 BF16 混合精度。FSDP 使我们指定单个模型组件的精度(例如, 视觉 backbones 为 FP16, LMs 为 BF16), 能够移植到不同的硬件, 并提供最小的实施开销。遵循先前工作 (Karamcheti et al., 2021; Biderman et al., 2023) 中的可重复性实践, 我们修复了初始化随机性, 并在训练期间修复了 batch order。我们利用 TIMM (Wightman, 2019) 和 Hugging Face Transformers (Wolf et al., 2019) 提供预训练模型。\n",
    "\n",
    "<!-- To validate our code, we run an apples-to-apples reproduction of LLaVa v1.5 (Liu et al., 2023b) at both the 7B and 13B parameter scale. Successful reproduction results are in Fig. 4 (left). We find our implementation is considerably more efficient than the reference LLaVa v1.5 training implementation: when benchmarked on the same hardware (an AWS p4de.24xlarge node with 8 A100 GPUs), we observe 20% faster step times with our FSDP-backed implementation, a notable gain given LLaVa leverages the well-optimized DeepSpeed ZeRO library (Rasley et al., 2020). -->\n",
    "为了验证我们的代码, 我们在 7B 和 13B 参数规模上运行了 LLaVa v1.5 (Liu et al., 2023b) 的同类复制。成功的复现结果如[图4](#fig.4)(左)所示。我们发现我们的实现比参考的 LLaVa v1.5 训练实现效率高得多: 在相同硬件(具有 8 个 A100 GPU 的 AWS p4de.24xlarge 节点)上进行基准测试时, 我们观察到使用 FSDP 支持的实现, step 时间加快 20%, 鉴于 LLaVa 利用了充分优化的 DeepSpeed ZeRO 库 (Rasley et al., 2020), 这是一个显著的进步。\n",
    "\n",
    "todo: 图4\n",
    "<!-- Figure 4. Reproducing LLaVa v1.5 & Exploring Optimization Procedures. To validate our training codebase ($\\S$ [2](#Preliminaries)), we reproduce LLaVa v1.5 (green), with our models reproducing the performance reported in Liu et al. (2023b) (gray). We then run our first experiment ($\\S$ [4.1](#Optimization-Procedure)) investigating the need for expensive multi-stage training (right). We find that single-stage training produces VLMs that maintain or outperform multi-stage models (orange), saving considerable compute; as a result, we carry this change forward to all future experiments. -->\n",
    "图4. 复现 LLaVa v1.5 并探索优化程序。为了验证我们的训练代码库 ($\\S$ [2](#Preliminaries)), 我们复现了 LLaVa v1.5 (绿色), 我们的模型复现了 Liu et al. (2023b) 中报告的性能(灰色)。然后, 我们运行我们的第一个实验 ($\\S$ [4.1](#Optimization-Procedure)), 调查昂贵的多阶段训练的必要性(右)。我们发现单阶段训练生成的 VLMs 保持或超越多阶段模型(橙色), 节省大量计算; 因此, 我们将这一变化延续到所有未来的实验中。\n",
    "\n",
    "<!-- **We highlight this open-source training codebase as one of the key contributions of this work**. Unlike other open codebases, we provide a modular and expressive interface for easily specifying or adding model components, optimization procedures, and data with minimal code changes (Fig. 2; right). In providing an efficient and easily extensible framework, we enable future research around designing new evaluations, developing and training new VLMs, and finetuning or otherwise adapting existing models for diverse downstream applications – all while maintaining a high standard of reproducibility and controlled experimentation. -->\n",
    "**我们强调这个开源训练代码库是这项工作的主要贡献之一**。与其他开源码库不同, 我们提供了一个模块化且富有表现力的接口, 只需最少的代码更改即可轻松指定或添加模型组件、优化程序和数据([图2](#fig.2); 右)。通过提供高效且易于扩展的框架, 我们支持(围绕设计新评估、开发和训练新 VLMs 以及微调或以其他方式调整现有模型以适应各种下游应用的)未来研究——同时保持高标准的可重复性和受控实验。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aedf7f5-c934-4b3a-a6ec-0792a0a0833f",
   "metadata": {},
   "source": [
    "# Evaluation Suite\n",
    "<!-- The first contribution of this work is a unified evaluation suite that offers fine-grained insight into the capabilities of a given VLM. Recent work in evaluating VLMs tends to rely on automated evaluations that use powerful LMs such as GPT-4 (OpenAI et al., 2023) to judge relative and subjective performance(Liu et al., 2023e; Yu et al., 2023), making it hard to measure the absolute impact of a given design change. Instead, we focus on evaluations with well-defined metrics, spanning the following three areas: -->\n",
    "这项工作的第一个贡献是统一的评估套件, 它提供对给定 VLM 能力的细粒度洞察。近期评估 VLMs 的工作趋向依赖于自动化评估, 该评估使用强大的 LMs, 例如 GPT-4 (OpenAI et al., 2023), 来判断相对和主观的性能 (Liu et al., 2023e; Yu et al., 2023), 使得难以衡量给定设计变更的绝对影响。相反, 我们聚焦于具有明确定义的指标的评估, 涵盖以下三个领域:\n",
    "\n",
    "<!-- *Open-Ended Visual Question Answering*. We evaluate on VizWiz (Bigham et al., 2010), VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019), and TextVQA (Singh et al., 2019). Both VizWiz and VQAv2 assess general visual reasoning; VizWiz also contains a series of unanswerable questions. GQA evaluates spatial reasoning, while TextVQA assesses reasoning around text (e.g., labels, signage) present in an image. -->\n",
    "*开放式视觉问答*。我们在VizWiz (Bigham et al., 2010), VQAv2 (Goyal et al., 2017), GQA (Hudson & Manning, 2019) 和 TextVQA (Singh et al., 2019)上进行评估。VizWiz 和 VQAv2 评估通用视觉推理; VizWiz 还包含一系列无法回答的问题。GQA 评估空间推理, 而 TextVQA 评估围绕图像中出现的文本(例如标签、指示牌)的推理。\n",
    "\n",
    "<!-- *Localization*. Part of the pretraining data mixture (from $\\S$ [2](#Preliminaries)) contains examples of predicting normalized bounding box coordinates given referring expressions in language. As such, we evaluate bounding box prediction accuracy on RefCOCO, RefCOCO+, and RefCOCOg (Kazemzadeh et al., 2014; Yu et al., 2016), and on OCID-Ref (Wang et al., 2021). RefCOCO focuses on short descriptions with spatial anchors, RefCOCO+ on strictly appearance based descriptions, and RefCOCOg on long, rich descriptions; OCID-Ref is a robotics dataset probing out-of-distribution generalization, with a focus on localizing objects in clutter. -->\n",
    "*定位*。部分预训练数据混合物(来自 $\\S$ [2](#Preliminaries))包含在给定语言中的参考表达式的情况下预测标准化边界框坐标的示例。因此, 我们在 RefCOCO, RefCOCO+, RefCOCOg (Kazemzadeh et al., 2014; Yu et al., 2016) 和 OCID-Ref (Wang et al., 2021) 上评估了边界框预测准确性。RefCOCO 聚焦于具有空间锚点的简短描述, RefCOCO+ 聚焦于严格基于外观的描述, RefCOCOg 聚焦于长而丰富的描述; OCID-Ref 是一个探索分布外泛化的机器人数据集, 聚焦于定位杂乱中的物体。\n",
    "\n",
    "<!-- *Challenge Sets (Closed-Set Prediction)*. We evaluate on Visual Spatial Reasoning (VSR; Liu et al., 2022), TallyQA (Acharya et al., 2018), POPE (Li et al., 2023d), and AI2 Diagrams (AI2D; Kembhavi et al., 2016). VSR consists of challenging True/False questions about individual spatial relationships in diverse scenes (e.g., \"the cake is at the edge of the dining table\"); this is an especially challenging task, with most existing models failing to outperform the majority class baseline (51%). TallyQA consists of questions that assess a VLM's ability to count objects described in language, with expressions that range in complexity. POPE consists of targeted Yes/No questions that assess a VLM's propensity to hallucinate. Finally, AI2D consists of multiple choice questions about scientific diagrams and charts, many of which require reading labels or text annotations (e.g., flowchart labels, plot axes). -->\n",
    "*挑战集(闭集预测)*。我们在 Visual Spatial Reasoning (VSR; Liu et al., 2022), TallyQA (Acharya et al., 2018), POPE (Li et al., 2023d) 和 AI2 Diagrams (AI2D; Kembhavi et al., 2016) 上进行评估。VSR 包括(关于不同场景中个体空间关系的)具有挑战性的真/假问题(例如, \"蛋糕在餐桌边缘\"); 这是一项特别具有挑战性的任务, 大多数现有模型都无法超越多数类基线(51%)。TallyQA 包含一些问题, 用于评估 VLM 统计用语言描述的对象的能力, 其表达方式的复杂程度各不相同。POPE 包括有针对性的是/否问题, 这些问题评估 VLM 对幻觉的倾向。最后, AI2D 包含有关科学图表的多选择问题, 其中许多问题需要阅读标签或文本注释(例如, 流程图标签、绘图轴)。\n",
    "\n",
    "<!-- We use the validation sets for all benchmarks except GQA (where use the recommended the test-dev split), VSR (where we use the zero-shot test split), and POPE (where there is only a single evaluation split). We provide further detail around evaluation protocols in [Appx. B](#todo). -->\n",
    "我们对除 GQA (使用推荐的测试开发 split)、VSR (使用零样本测试 split) 和 POPE (只有一个评估 split) 之外的所有基准测试, 都使用验证集。我们在 [附录 B](#todo) 中提供了有关评估协议的更多详细信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75adac13-c2e8-47cf-9661-ac91bf849c2c",
   "metadata": {},
   "source": [
    "# Experiments – Investigating Design Axes\n",
    "<!-- Our second contribution is a series of targeted experiments exploring the VLM design space along four key axes: ($\\S$ [4.1](#Optimization-Procedure)) optimization procedure, ($\\S$ [4.2](#Image-Processing-&-Visual-Representations)) image processing and visual representations, ($\\S$ [4.3](#Integrating-Language-Models)) language models, and ($\\S$ [4.4](#Scaling-Properties:-Training-Time-&-Data)) scaling properties such as training time and data diversity. -->\n",
    "我们的第二个贡献是一系列有针对性的实验, 沿着四个关键轴探索 VLM 设计空间: ($\\S$ [4.1](#Optimization-Procedure)) 优化程序, ($\\S$ [4.2](#Image-Processing-&-Visual-Representations)) 图像处理和视觉表示, ($\\S$ [4.3](#Integrating-Language-Models)) 语言模型, ($\\S$ [4.4](#Scaling-Properties:-Training-Time-&-Data)) 缩放属性, 例如训练时间和数据多样性。\n",
    "\n",
    "<!-- **Experiment Design: Protocols & Drawing Conclusions**. We first validate our VLM training implementation by reproducing LLaVa v1.5 (see $\\S$ [2](#Preliminaries)), adopting the design choices of the original work – the same choices used by many other recent VLMs: \"letterbox padding\" to process images, CLIP ViT-Large with a patch size of 14 and input resolution of 336px (CLIP ViT-L/14 @ 336px; Radford et al., 2021) as the visual representation, Vicuna v1.5 as the LM backbone, and the two-stage training pipeline using both data subsets described in $\\S$ [2](#Preliminaries). Successful reproduction results at both the 7B and 13B scale are in Fig. 4 (left). Given both the fidelity of our reproduction and the prevalence of these design choices, we anchor our analyses around this parameterization. Critically, each of the experiments in $\\S$ [4.2](#Image-Processing-&-Visual-Representations), $\\S$ [4.3](#Integrating-Language-Models), and $\\S$ [4.4](#Scaling-Properties:-Training-Time-&-Data) are formulated as *single-step changes of this base architecture, with all other choices held constant*. -->\n",
    "**实验设计: 协议和总结**。我们首先通过复现 LLaVa v1.5 (参见 $\\S$ [2](#Preliminaries)), 验证我们的 VLM 训练实现, 采用原始工作的设计选择——许多其他最近的 VLMs 使用的相同选择: \"letterbox 填充\" 以处理图像, CLIP ViT-Large 作为视觉表示, 其 patch 大小为 14, 输入分辨率为 336px (CLIP ViT-L/14 @ 336px; Radford et al., 2021), Vicuna v1.5 作为 LM backbone, 和使用 $\\S$ [2](#Preliminaries) 中描述的数据子集的两阶段训练流水线。7B 和 13B 规模的成功复现结果如[图4](#fig.4)(左)所示。考虑到我们复现的保真度和这些设计选择的普遍性, 我们围绕这个参数化进行分析。至关重要的是, $\\S$ [4.2](#Image-Processing-&-Visual-Representations)、$\\S$ [4.3](#Integrating-Language-Models) 和 $\\S$ [4.4](#Scaling-Properties:-Training-Time-&-Data) 中的每个实验都被表述为 *此基础架构的单步改变, 所有其他选择保持不变*。\n",
    "\n",
    "<!-- As each evaluation in $\\S$ [3](#Evaluation-Suite) uses different metrics with different scales, direct comparison is challenging. We address this by computing normalized Z-scores for each model and evaluation (using the mean and standard deviation across all models). These scores are used to compute statistical significance (further details in $\\S$ [B.2](#todo)), and to set the relative scales of each radar plot (for completeness, we also provide the absolute metrics as colored and bolded labels). -->\n",
    "由于 $\\S$ [3](#Evaluation-Suite) 中的每个评估都使用具有不同尺度的不同指标, 直接比较具有挑战性。我们通过计算每个模型和评估的归一化 Z-分数(使用所有模型的平均值和标准差)来解决这个问题。这些分数用于计算统计显着性(更多详细信息请参阅 $\\S$ [B.2](#todo)), 并设置每个雷达图的相对尺度(为了完整起见, 我们还以彩色和粗体标签的形式提供绝对指标)。\n",
    "\n",
    "## Optimization Procedure\n",
    "<!-- In this section we focus on design choices around the optimization procedure used to initialize and train each of the three components described in $\\S$ [2](#Preliminaries). Specifically, we examine the effects of multi-stage training where different VLM components are frozen at different points in training. -->\n",
    "在本节中, 我们专注于围绕优化程序的设计选择, 用于初始化和训练$\\S$ [2](#Preliminaries)中描述的三个组件中的每一个。具体而言, 我们研究了多阶段训练的效果, 其中不同的 VLM 组件在训练的不同点被冻结。\n",
    "\n",
    "<!-- **Multi-Stage Training**. One of the prevalent design choices adopted by many VLMs (Chen et al., 2023a; Ye et al., 2023) is the inclusion of a two-stage training pipeline: (1) an alignment stage to align vision and language features by training the randomly initialized projector $F_\\psi$ in isolation, freezing all other components (Fig. 4, right) and (2) a finetuning stage, where only the visual representation is frozen while both the projection and LM are trained. -->\n",
    "**多阶段训练**。许多 VLMs (Chen et al., 2023a; Ye et al., 2023) 采用的流行设计选择之一是两阶段训练流水线: (1) 对齐阶段, 通过单独训练随机初始化的投影器 $F_\\psi$ 来对齐视觉和语言特征, 冻结所有其他组件([图4](#fig.4), 右); (2) 微调阶段, 其中仅冻结视觉表征, 同时训练投影和 LM。\n",
    "\n",
    "<!-- Adopting multi-stage training complicates implementation and adds to training cost; therefore, as an initial experiment, we evaluate the need for this first stage through a targeted ablation. We compare the default two-stage training procedure with a single-stage approach that skips directly to finetuning $F_\\psi$ and the LM. We find (Fig. 4; left) that including the explicit projector pretraining stage is unnecessary, with single-stage training improving aggregate performance ($p = 0.00558$). Eliminating this first stage saves 20-25% of training cost, and removes the need for additional, stage-specific data (e.g., the captioning subset from $\\S$ [2](#Preliminaries)). *As this change strictly improves performance and efficiency, we adopt single-stage training for all following experiments.* -->\n",
    "采用多阶段训练使实施复杂化并增加训练成本; 因此, 作为初步实验, 我们通过有针对性的消融, 评估第一阶段的必要性。我们将默认的两阶段训练程序与(直接跳到微调 $F_\\psi$ 和 LM 的)单阶段方法进行比较。我们发现包括精密投影器预训练阶段是不必要的([图4](#fig.4); 左), 单阶段训练提高总体性能($p = 0.00558$)。消除第一阶段节省 20-25% 的训练成本, 并且无需额外的特定阶段数据(例如, 来自 $\\S$ [2](#Preliminaries) 的字幕子集)。*由于这一变化确实提高了性能和效率, 因此我们对所有后续实验都采用单阶段训练*。\n",
    "\n",
    "<!-- **Full Finetuning through Visual Backbones**. Another popular design choice in existing VLMs that leverage pretrained visual representations is to leave the visual backbone frozen during the entirety of training (Liu et al., 2023b; Driess et al., 2023; Li et al., 2023b). Such a choice limits the potential to learn improved visual representations conducive to language generation during the course of training. Thus, we ask – *is there potential to improve VLM performance by finetuning the full model, including the visual backbone*? We find (Fig. 5) that this is not the case, and that finetuning the visual backbone significantly degrades performance ($p = 0.00381$), especially on tasks requiring fine-grained spatial reasoning such as RefCOCO and OCID-Ref. -->\n",
    "**通过视觉 Backbones 进行完全微调**。现有的(利用预训练视觉表征的) VLMs 的另一种流行设计选择是在整个训练期间冻结视觉 backbone (Liu et al., 2023b; Driess et al., 2023; Li et al., 2023b)。这样的选择限制了在训练过程中学习(有利于语言生成的)改进视觉表征的潜力。因此, 我们要问——*通过微调整个模型, 包括视觉 backbone, 是否有可能提高 VLM 性能?* 我们发现事实并非如此([图5](#fig.5)), 并且微调视觉 backbone 显著降低性能($p = 0.00381$), 尤其是在需要细粒度空间推理的任务上, 例如 RefCOCO 和 OCID-Ref。\n",
    "\n",
    "todo: 图5\n",
    "\n",
    "<!-- *Remark*. The degraded performance from full finetuning could be for a number of reasons ranging from the scale and diversity of the vision-language data we train on to language generation as a learning objective (vs. objectives that encourage learning fine-grained perceptual features). Especially given the existence of closed-source models such as Fuyu-8B (AI, 2023) that adopt this paradigm to great success, we believe that identifying ways to prevent such feature collapse during VLM training (e.g., via auxiliary objectives) to be a rich direction for future work. -->\n",
    "*注释*。完全微调导致的性能下降可能有多种原因, 从我们训练用的视觉-语言数据的规模和多样性, 到将语言生成作为学习目标(vs. 鼓励学习细粒度感知特征的目标)。特别是考虑到闭源模型的存在, 例如 Fuyu-8B (AI, 2023), 它采用这种模式取得巨大成功, 我们认为, 确定在 VLM 训练期间防止此类特征崩溃的方法(例如, 通过辅助目标)将成为未来工作的丰富方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cd3b0-2953-4664-9276-5c75aa33cfad",
   "metadata": {},
   "source": [
    "## Image Processing & Visual Representations\n",
    "<!-- **Choosing a Pretrained Vision Representation**. CLIP (Radford et al., 2021) has become the default choice for visual representation for almost all VLMs, despite a wealth of visual representations trained on diverse data sources. In this experiment, we perform a head-to-head comparison between CLIP, SigLIP (Zhai et al., 2023), DINOv2 (Oquab et al., 2023), and a standard Vision Transformer pretrained for classification (on ImageNet-21K, finetuned on ImageNet-1K; Dosovitskiy et al., 2021; Steiner et al., 2021); for fair comparison, we use the ViT-Large model variant.<sup>[2](#superscript.2)</sup> We find (Fig. 6; left) that the backbones trained with vision-language contrastive objectives (i.e., CLIP, SigLIP) are significantly more performant than alternatives ($p = 7.11e-8$). -->\n",
    "**选择预训练的视觉表征**. CLIP (Radford et al., 2021)已成为几乎所有 VLMs 的视觉表征的默认选择, 尽管在不同的数据源上训练了丰富的视觉表征。在本实验中, 我们对 CLIP, SigLIP (Zhai et al., 2023), DINOv2 (Oquab et al., 2023) 和预训练用于分类的标准 Vision Transformer (在 ImageNet-21K 上训练, 在 ImageNet-1K 上微调; Dosovitskiy et al., 2021; Steiner et al., 2021) 进行了正面比较; 为了公平比较, 我们使用 ViT-Large 模型变体<sup>[2](#superscript.2)</sup>。我们发现([图6](#fig.6); 左)使用视觉-语言对比目标(即 CLIP、SigLIP)训练的 backbones 比替代方案性能显着提高($p = 7.11e-8$)。\n",
    "\n",
    "todo: 图6\n",
    "\n",
    "<!-- To evaluate on an image resolution common to all representations (224px), we use the shape-optimized SigLIP model (ViT-SO Alabdulmohsin et al., 2023) that is slightly larger than a ViT-Large at 400M parameters (vs 307M). -->\n",
    "> <sup>2</sup><span id='superscript.2'></span>评估所有表征通用的图像分辨率 (224px), 我们使用形状优化的 SigLIP 模型 (ViT-SO Alabdulmohsin et al., 2023), 该模型比 400M 参数的 ViT-Large 略大 (vs 307M)。\n",
    "\n",
    "<!-- *Remark*. While the vision-language contrastive objective is one explanation for the strengths of CLIP and SigLIP, another possible explanation is one of training image distribution. Both CLIP and SigLIP contain internet-sourced images (e.g., sketches, diagrams, animated graphics, etc.) not in ImageNet or in the DINOv2 pretraining data. -->\n",
    "*注释*. 尽管视觉-语言对比目标是 CLIP 和 SigLIP 优势的一个解释, 但另一个可能的解释是训练图像分布。CLIP 和 SigLIP 都包含来自互联网的图像(例如草图、图表、动画图形等), 这些图像不在 ImageNet 或 DINOv2 预训练数据中。\n",
    "\n",
    "<!-- **Image Processing across Visual Backbones**. Most images have resolutions and aspect ratios that widely vary, yet most visual backbones expect square images at a fixed size; to reconcile this, the overwhelming default is to \"resize & crop\" an image to size. While this tends to work well for applications such as classification, cropping out parts of an image is especially harmful for tasks requiring full-scene reasoning. In this experiment, we evaluate three different image processing schemes – the default \"resize & crop\" scheme, the \"letterbox padding\" scheme used by LLaVa v1.5 that pads non-square images to square, and a \"naive resize\" scheme that warps the original image aspect ratio, squeezing or stretching an image to square. Our findings (Fig. 6; middle) are surprising: while cropping is clearly suboptimal, the \"naive resize\" scheme is the most performant for CLIP. For SigLIP, both \"naive resize\" and \"letterbox padding\" perform similarly. In general, our results favor \"naive resizing\" over \"letterbox padding\" but we cannot rule the improvement statistically significant ($p = 0.0176$). -->\n",
    "**跨视觉 Backbones 的图像处理**. 大多数图像的分辨率和长宽比差异很大, <font color=\"red\">但大多数视觉 backbones 都希望图像为固定大小的方形</font>; 为了解决这个问题, 绝大多数默认设置是\"调整并裁剪\"图像以达到所需大小. 尽管这对于分类等应用程序来说往往很有效, 但是裁剪掉图像的某些部分对于需要全场景推理的任务尤其有害. 在本实验中, 我们评估了三种不同的图像处理方案 - 默认的\"调整并裁剪\"方案、LLaVa v1.5 使用的 \"letterbox padding\" 方案, 该方案将非方形图像填充为方形, 以及 \"naive resize\" 方案, 该方案扭曲原始图像长宽比, 将图像压缩或拉伸为方形. <font color=\"red\">我们的发现令人惊讶([图6](#fig.6) 中): 虽然裁剪显然不是最佳的, 但 \"naive resize\" 方案对于 CLIP 来说是性能最高的. 对于 SigLIP, \"naive resize\" 和 \"letterbox padding\" 表现相似</font>. 总体而言, 我们的结果更倾向于 \"naive resizing\" 而不是 \"letterbox padding\", 但我们不能判定这种改进具有统计显著性 ($p = 0.0176$).\n",
    "\n",
    "<!-- *Remark*. Two speculative arguments for naively resizing an image over padding are those of minimizing \"dead pixels\" and distribution shift. An image with a 16:9 aspect ratio that is padded to square introduces a large amount of uninformative pixels (exceeding 40%); warping the aspect ratio is possibly less of a shift. Coupled with the innate patch dimensionality of a Vision Transformer ($d = 1024$ for a $16 \\times 16$ pixel patch), naively resizing an image may preserve enough information for the downstream LM (with 7B+ parameters) to extract the properties necessary for downstream tasks. -->\n",
    "*备注*. 关于通过填充来简单调整图像大小的两个推测性论点是最小化\"死像素\"和分布偏移. 将长宽比为 16:9 的图像填充为正方形会引入大量无信息的像素(超过 40%); 扭曲长宽比可能偏移较少. 结合 Vision Transformer 固有的 patch 维度(对于 $16 \\times 16$ 像素 patch, $d = 1024$), 简单调整图像大小可能为下游 LM (具有 7B+ 参数) 保留足够的信息, 以提取下游任务所需的属性.\n",
    "\n",
    "<!-- **Scaling Image Resolution**. Another trend in recent VLMs is increasing input image resolution with the hope of capturing fine-grained details that improve downstream performance (Liu et al., 2023b; Li et al., 2023a). Our findings (Fig. 6; right) confirm this hypothesis, with scaling to 336px or 384px offering significant improvements ($p = 6.05e-4$). -->\n",
    "**缩放图像分辨率**. 近期 VLMs 的另一个趋势是提高输入图像分辨率, 希望捕获细粒度细节, 这些细节提高下游性能 (Liu et al., 2023b; Li et al., 2023a). 我们的发现证实了这一假设([图6](#fig.6) 右), 缩放至 336px 或 384px 可提供显着改进 ($p = 6.05e-4$).\n",
    "\n",
    "<!-- *Remark*. While scaling up image resolution seems like a clear win, we caution that it comes with a significant increase in compute complexity for VLMs that project individual ViT patches into the embedding space of an LM. Assuming a fixed patch granularity, doubling the input resolution results in four times the number of input patches fed to the LM. Coupled with the quadratic cost of traditional Transformer attention as a function of sequence length, this is a sixteen-fold increase in time complexity (with a comparable explosion in memory requirements). -->\n",
    "*备注*. 虽然扩大图像分辨率似乎是一个明显的胜利, 但我们要提醒的是, 对于将单个 ViT patches 投影到 LM 的嵌入空间的 VLMs, 这会带来计算复杂度的显著增加. <font color=\"red\">假设一个固定的 patch 粒度, 将输入分辨率加倍导致输入到 LM 的输入 patches 数量增加四倍. 再加上传统 Transformer 注意力的二次成本作为序列长度的函数, 时间复杂度增加了十六倍(内存需求也出现类似的激增)</font>.\n",
    "\n",
    "<!-- **Ensembling Different Visual Representations**. A rich body of prior work in vision identifies that different types of visual representations trained with different inductive biases can lead to improved performance for a broad spectrum of applications (Kobayashi et al., 2022; Karamcheti et al., 2023). Motivated by this, we ask if this same trend holds true for VLM training – specifically whether ensembling DINOv2 features with vision-language contrastive features from CLIP and SigLIP can lead to improved performance, following the approach taken in Kerr et al. (2023). To implement this efficiently, we simply concatenate patch features from different backbones along the channel dimension for each patch, resulting in the same number of input patch embeddings, just with double the feature dimension. To adjust for this, we just increase the input dimension to our projector $F_\\psi$ (a 2-layer MLP) at negligible cost. We find (Fig. 7 - left) that fusing DINOv2 and SigLIP features provides significant gains across the board ($p = 0.00164$), with a notable exception for the DINOv2 + CLIP models ($p = 0.37313$), where combining DINOv2 features seem to be particularly harmful on TextVQA. Looking at the remaining results, we see especially impressive gains of 5-10% on localization and challenge tasks; in general, the DINOv2 + SigLIP fused representations are the most performant visual representations we try, with virtually no added parameters. -->\n",
    "**组合不同的视觉表征**. 视觉领域中大量的先前研究表明, 使用不同的归纳偏差训练的不同类型的视觉表征可以提高广泛应用的性能 (Kobayashi et al., 2022; Karamcheti et al., 2023). 受此启发, 我们想知道同样的趋势是否也适用于 VLM 训练————具体而言, 遵循 Kerr et al. (2023) 所采用的方法, 将 DINOv2 特征与 CLIP 和 SigLIP 中的视觉语言对比特征组合是否可以提高性能. 为了有效地实现这一点, 我们只需沿每个 patch 的通道维度, 拼接来自不同 backbones 的 patch 特征, 从而产生相同数量的输入 patch 嵌入, 只将特征维度增加一倍. 为了适应这一点, 我们只需以可忽略不计的成本, 增加投影器 $F_\\psi$ (2 层 MLP) 的输入维度. 我们发现融合 DINOv2 和 SigLIP 特征可全面带来显著提升 ($p = 0.00164$) ([图7](#fig.7) 左), 但 DINOv2 + CLIP 模型 ($p = 0.37313$) 是一个明显的例外, 其中结合 DINOv2 特征似乎对 TextVQA 特别有害. 查看其余结果, 我们在定位和挑战任务上发现尤其可观的收益, 达到 5-10%; 总体而言, DINOv2 + SigLIP 融合表征是我们尝试过的性能最高的视觉表征, 几乎没有添加任何参数。\n",
    "\n",
    "todo: 图7\n",
    "\n",
    "<!-- *Remark*. Following the hypotheses in Kerr et al. (2023) and similar work, we believe that DINOv2 features provide features that capture low-level spatial properties of an image, augmenting the higher-level \"semantic\" properties captured by vision-language contrastive models. We note that this conclusion may generalize beyond DINO-style backbones; the only reason we do not evaluate the fusion of ImageNet and CLIP/SigLIP backbones as well is due to a mismatch in patch granularity (the ImageNet backbone uses a patch granularity of $16 \\times 16$ vs. the $14 \\times 14$ granularity used by all other backbones). We believe that further exploring the impact on these type of fused, multi-resolution features for VLMs is a compelling avenue for future work. -->\n",
    "*备注*. 遵循 Kerr et al. (2023) 和类似工作中的假设, 我们认为 DINOv2 特征提供了捕捉图像下层空间属性的特征, 增强了视觉语言对比模型捕捉的上层\"语义\"属性. 我们注意到, 这个结论可能超越 DINO 风格的 backbones; 我们没有评估 ImageNet 和 CLIP/SigLIP backbones 融合的唯一原因是 patch 粒度不匹配 (ImageNet backbone 使用的patch 粒度为 $16 \\times 16$, 而所有其他 backbones 使用的粒度为 $14 \\times 14$). 我们相信进一步探索此类融合的多分辨率特征对 VLMs 的影响是未来工作的一个引人注目的途径."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73980e7-19b5-45c6-b159-fe9b2e946f30",
   "metadata": {},
   "source": [
    "##  Integrating Language Models\n",
    "<!-- **Base vs. Instruct-Tuned LMs**. Instruct tuning (or chat tuning; Ouyang et al., 2022; Chung et al., 2022) is a way to finetune base LMs (trained for next-token prediction) to behave as dialogue agents, offering a natural input/output interface for a wide spectrum of applications. As a result, instruct tuned models like Vicuna (Zheng et al., 2023) have become the default backbone for VLMs. Unfortunately, instruct tuning has drawbacks, introducing bias and regressions in performance (Ouyang et al., 2022). Thus, in this experiment we evaluate the impact of instruct-tuned LM backbones on downstream VLM performance via a head-to-head comparison between a base LM (Llama-2; Touvron et al., 2023), and an instruct-tuned variant (Vicuna v1.5). We find (Fig. 7 - right) that instruction-tuned LMs yield no statistically significant improvement in performance over base LMs ($p = 0.34854$), but differ in qualitative performance. Specifically, we observe that instruct-tuned LMs lead to VLMs that are more verbose, prone to hallucination, and generally less specific in their responses (Fig. 11). -->\n",
    "**基础 LMs 与指令调整 LMs**. 指令调整 (或聊天调整; Ouyang et al., 2022; Chung et al., 2022) 是一种微调基础 LMs 方法 (为下一个词元预测进行训练) 以充当对话智能体, 为广泛的应用提供自然的输入/输出接口. 因此, 指令调整模型, 如 Vicuna (Zheng et al., 2023) 已成为 VLMs 的默认 backbone. 不幸的是, 指令调整存在缺陷, 会引入性能偏差和回归 (Ouyang et al., 2022). 因此, 在本实验中, 我们通过对基础 LM (Llama-2; Touvron et al., 2023) 和指令调整变体 (Vicuna v1.5) 进行正面比较, 评估指令调整的 LM backbones 对下游 VLM 性能的影响. 我们发现与基础 LMs 相比, 指令调整的 LMs 在性能方面没有产生统计上显着的改进 ($p = 0.34854$) ([图7](#fig.7) 右), 但在定性性能上有所不同. 具体而言, 我们观察到指令调整的 LMs 导致 VLM 更加冗长, 容易产生幻觉, 并且它们的回答通常不太具体([图11](#fig.11)).\n",
    "\n",
    "todo: 图11\n",
    "\n",
    "<!-- **Do Better LMs Lead to Better VLMs**? We investigate how LM performance on language-only benchmarks translates to downstream VLM performance, training VLMs from Mistral v1 7B and Mistral Instruct v1 7B (Jiang et al., 2023), recent LMs that outperform Llama-2 on language and code benchmarks (Hendrycks et al., 2021; Chen et al., 2021). We find (Fig. 12) that these VLMs are not significantly more performant than VLMs trained from Llama-2 ($p = 0.03097$); an exciting avenue for future work is investigating how LM pretraining mixtures correlate with VLM performance. -->\n",
    "**更好的 LM 是否会带来更好的 VLM**? 我们研究了 LM 在纯语言基准测试中的性能如何转化为下游 VLM 性能, 从 Mistral v1 7B 和 Mistral Instruct v1 7B (Jiang et al., 2023) 训练 VLMs, 这些最近的 LMs 在语言和代码基准测试上优于 Llama-2 (Hendrycks et al., 2021; Chen et al., 2021). 我们发现这些 VLMs 的性能没有显著优化从 Llama-2 训练的 VLMs ($p = 0.03097$)([图12](#fig.12)); 未来工作的一个令人兴奋的途径是研究 LM 预训练混合与 VLM 性能如何相关.\n",
    "\n",
    "todo: 图12\n",
    "\n",
    "<!-- **Co-training on Language-only Safety Data**. The LLaVa v1.5 pretraining dataset we use for training consists of 40K examples of language-only data sourced from ShareGPT (ShareGPT, 2023); this data consists of a diverse set of user-uploaded conversations with OpenAI's ChatGPT; crucially many of the examples in this dataset contain toxic, inappropriate, or otherwise unsafe inputs, and the corresponding \"guarded\" responses from ChatGPT (e.g., \"as an AI, I cannot comment on...\"). In this experiment, we ablate the impact of co-training on this language-only data on downstream performance, with a goal of understanding if adding language-only data unrelated to visual reasoning hurts performance relative to training on multimodal data alone. We find (Fig. 8; left) that removing language-only data only slightly improves performance ($p = 0.13655$). -->\n",
    "**在纯语言安全数据上的联合训练**. 我们用于训练的 LLaVa v1.5 预训练数据集包含来自 ShareGPT (ShareGPT, 2023) 的 40K 个纯语言数据示例; 这些数据由用户上传的与 OpenAI 的 ChatGPT 的多种对话组成; 至关重要的是, 该数据集中的许多示例包含有害、不适当或其他不安全的输入, 以及来自 ChatGPT 的相应\"保守\"响应(例如, \"作为 AI, 我无法评论...\"). 在这个实验中, 我们消融了在纯语言数据上进行联合训练对下游性能的影响, 目的是了解添加与视觉推理无关的纯语言数据是否会损害性能, 相对于仅在多模态数据上进行训练. 我们发现移除纯语言数据只会略微提高性能 ($p = 0.13655$) ([图8](#fig.8) 左).\n",
    "\n",
    "todo: 图8\n",
    "\n",
    "<!-- However, given that the language-only data is the only source of \"safety\" data during finetuning, we explicitly probe our VLMs with directly offensive and toxic prompts, to evaluate how important this data is for inducing safeguards on VLM outputs. In our adversarial testing, we find that especially for VLMs trained from base LMs such as Llama-2, *including this co-training data is important for inducing at least a minimal set of safeguards*; Fig. 8 demonstrates the importance of co-training on VLM generations when prompted with questions with direct racist intent. -->\n",
    "然而, 鉴于纯语言数据是微调期间\"安全\"数据的唯一来源, 我们用直接攻击性和有害提示明确地探测我们的 VLMs, 以评估这些数据对于在 VLM 输出上引入保障措施的重要性. 在我们的对抗测试中, 我们发现, 特别是对于从 Llama-2 等基础 LMs 训练的 VLMs, *包括这些联合训练数据对于引入至少一组最低限度的保障措施非常重要*; [图8](#fig.8) 展示了当用(具有直接种族主义意图的)问题提示时, 联合训练对 VLM 生成的重要性.\n",
    "\n",
    "<!-- *Remark*. We focus our probing mostly around unsafe responses around racism, xenophobia, and gender bias. These biases are also prevalent in language, and explicitly represented in the ShareGPT co-training data. We address VLM-specific harms in our discussion of broader impacts. -->\n",
    "*备注*. 我们的调查重点主要围绕种族主义、排外主义和性别偏见的不安全响应. 这些偏见在语言中也很普遍, 并在 ShareGPT 联合训练数据中明确体现. 我们在更广泛影响的讨论中讨论了 VLM 专有的危害."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d61cf0-406f-44f9-907e-f36dc85aea23",
   "metadata": {},
   "source": [
    "## Scaling Properties: Training Time & Data\n",
    "<!-- In this section, we investigate how existing VLMs scale with training time and added data; critically, we examine choices of training time (are we undertraining our models), and how adding diverse datasets impacts downstream performance. -->\n",
    "在本节中, 我们研究现有的 VLM 如何根据训练时间和添加的数据进行扩展; 至关重要的是，我们检验训练时间的选择(我们的模型训练是否不足), 以及添加不同的数据集如何影响下游性能.\n",
    "\n",
    "<!-- **Are we Undertraining**? We explore the impact of training time as a function of training epochs. Unlike existing VLMs like PaLI or LLaVa that perform at most a single epoch, we compare performance when training at different numbers of epochs. We find (Fig. 10; middle) evidence of severe underfitting with a single epoch, with steady improvement (especially for tasks requiring structured output such as RefCOCO) until two epochs, when performance plateaus. We find that training for two epochs yields a significant improvement over training for one epoch ($p = 0.00496$). -->\n",
    "**我们是否训练不足**? 我们探索了训练时间(作为训练周期的函数)的影响. 与现有的 VLMs (如 PaLI 或 LLaVa) 不同, 它们最多只执行一个周期, 我们比较了在不同周期数训练时的性能. 我们发现单周期严重欠拟合的证据([图10](#fig.10) 中), 直到两个周期后性能才达到稳定状态(尤其是对于需要结构化输出的任务, 如 RefCOCO). 我们发现两个周期的训练比一个周期的训练产生显著的提高 ($p = 0.00496$).\n",
    "\n",
    "todo: 图10\n",
    "\n",
    "<!-- **Adding Additional Vision-Language Data**. We identify two recently proposed datasets: LVIS-Instruct-4V (Wang et al., 2023), obtained by prompting GPT-4V to generate rich synthetic examples from images sourced from LVIS (Gupta et al., 2019), and LRV-Instruct (Liu et al., 2023a) that specifically optimizes for image diversity relative to existing datasets (adding e.g., charts, scientific diagrams, and news printings). We find (Fig. 10; right) that adding both datasets improves performance ($p = 0.01459$), but that LRV-Instruct has a larger impact, indicating the importance of diverse images for scaling future VLMs. -->\n",
    "**添加额外的视觉语言数据**. 我们鉴定了两个最近提出的数据集: LVIS-Instruct-4V (Wang et al., 2023), 通过提示 GPT-4V 从来自 LVIS (Gupta et al., 2019) 的图像中生成丰富的合成示例而获得, 以及 LRV-Instruct (Liu et al., 2023a), 它相对于现有数据集, 专门针对图像多样性进行优化(例如添加图表、科学图表和新闻印刷品). 我们发现添加这两个数据集可以提高性能 ($p = 0.01459$) ([图10](#fig.10) 右), 但 LRV-Instruct 的影响更大, 这表明多样化图像对于扩展未来的 VLMs 至关重要."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8736463d-8876-4fb2-bf58-e8cd367f5998",
   "metadata": {},
   "source": [
    "# PRISM – Distilling Key Insights\n",
    "<!-- We identify a series of individual insights that simplify VLM training and improve downstream performance:\n",
    "1) *Optimization Procedure*: Single-stage training reduces compute cost without harming downstream performance.\n",
    "2) *Image Processing and Visual Representations*: Fused DINOv2 and SigLIP backbones with high resolution images and naive image resizing yield strong performance.\n",
    "3) *Language Models*: Base LMs such as Llama-2 match or exceed the performance of instruct-tuned LMs, with co-training on language-only data important for safety.\n",
    "4) *Scaling Properties*: Adding diverse data and extending training time significantly boost performance. -->\n",
    "我们确定了一系列可简化 VLM 训练并提高下游性能的个人见解:\n",
    "1) *优化程序*: 单阶段训练降低计算成本, 不损害下游性能.\n",
    "2) *图像处理和视觉表征*: 融合的 DINOv2 和 SigLIP backbones, 以及高分辨率图像和简单的图像大小调整, 实现强大的性能.\n",
    "3) *语言模型*: Llama-2 等基础 LMs 匹配或超过指令调整 LMs 的性能, 在纯语言数据上进行联合训练对于安全性非常重要.\n",
    "4) *扩展属性*: 添加不同的数据并延长训练时间显著提高性能.\n",
    "\n",
    "<!-- As a final step, we combine these insights to inform a new family of VLMs – PRISMs – at the 7B and 13B parameter scale. We present results comparing our PRISM models to InstructBLIP and LLaVa v1.5 in Fig. 9. We additionally run a head-to-head comparison against LLaVa v1.5, training a model – PRISM (Controlled) – given the *exact same data and training budget*. Both sets of PRISM models uniformly outperform baselines by large margins across our evaluation suite, with strong qualitative performance (Fig. 9; right). -->\n",
    "最后, 我们将这些见解结合起来, 在 7B 和 13B 参数规模上形成全新的 VLMs 系列 —— PRISM. 我们在[图9](#fig.9)中展示了我们的 PRISM 模型与 InstructBLIP 和 LLaVa v1.5 的对比结果. 我们还与 LLaVa v1.5 进行了正面比较, 给定*完全相同的数据和训练预算*下, 训练模型 —— PRISM(受控). 在我们的评估套件中, 两组 PRISM 模型均大幅优于基线, 并且具有强大的定性性能([图9](#fig.9) 右).\n",
    "\n",
    "todo: 图9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f78934-b967-4427-b5d3-79aa02d5cda8",
   "metadata": {},
   "source": [
    "# Limitations & Future Work\n",
    "<!-- There are two key limitations in our approach. Of primary concern is the generality of our model architecture; while the three component architecture we define in §2 is reflective of the majority of existing VLMs, there are other architecture innovations and optimization procedures that our study does not currently capture; as a notable example, we do not study architectures that learn to downsample image patches, such as the Perceiver-based architectures used by Flamingo and IDEFICS (Alayrac et al., 2022; Laurencon et al., 2023) for interleaved image-text training. Though many of our takeaways are general (e.g., these models also use backbones such as CLIP and autoregressive LMs), there remain open questions about how our findings generalize, especially at larger scales (e.g., 70B+ parameters). -->\n",
    "我们的方法有两个关键局限性. 主要关注的是我们的模型架构的通用性; 尽管我们在 $\\S$ [2](#Preliminaries) 中定义了三个组件架构反映了大多数现有的 VLMs, 但还有其他架构创新和优化程序, 它们目前捕没有被我们的研究捕捉到; 作为一个值得注意的例子, 我们没有研究学习对图像 patches 进行下采样的架构, 例如 Flamingo 和 IDEFICSDEFICS (Alayrac et al., 2022; Laurencon et al., 2023) 用于交错图像文本训练的基于感知器的架构. 虽然我们的许多想法都是通用的(例如, 这些模型也使用了 CLIP 和自回归 LMs 等 backbones), 但关于我们的发现如何泛化仍然是开放问题, 尤其是在更大规模(例如 70B+ 参数)下.\n",
    "\n",
    "<!-- A separate limitation is that of evaluation; we make the intentional choice in this work to focus on standardized evaluations, with objective metrics. While this lets us probe fine-grained capabilities, we do not capture the scope of the dyadic interactions afforded by existing VLMs – the ability to carry on extending dialogues that flit across topics grounded in a visual context. While some of the automated evaluations discussed in §3 provide initial steps for evaluating these open-ended behaviors, future work will investigate how to extend such evaluations to longer, richer contexts. Related to this are the downstream applications built on top of broadly capable VLMs – applications such as using VLMs to learn robotic control policies or for visual programming (Brohan et al., 2023; Sur’is et al., 2023); a compelling avenue for future work is understanding how to co-design VLMs with downstream applications. -->\n",
    "另一个局限性是评估; 我们在本研究中有意选择专注于标准化评估, 及客观指标. 虽然这让我们能够探索细粒度的能力, 但我们没有捕捉到现有 VLMs 所提供的二元交互的范围————继续扩展对话的能力, 在视觉背景的基础上快速浏览主题. 虽然 $\\S$ [3](#Evaluation-Suite) 中讨论的一些自动化评估为评估这些开放式行为提供了初步步骤, 但未来的工作将研究如何将这些评估扩展到更长、更丰富的上下文. 与此相关的是建立在广泛功能的 VLMs 之上的下游应用————例如使用 VLMs 学习机器人控制策略或用于可视化编程的应用 (Brohan et al., 2023; Sur’is et al., 2023); 未来工作的一条令人信服的途径是了解如何与下游应用共同设计 VLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6678ded-f20f-4043-a7a4-340758b9ef98",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "<!-- We present a rigorous investigation of the design space of visually-conditioned language models, distilling key insights for training future models. This investigation is enabled by two key resource contributions: 1) an evaluation suite that enables fine-grained insight into a VLM’s capabilities, and 2) an optimized, extensible codebase for training VLMs with an emphasis on *flexibility* – flexibility over optimization procedures, image processing and visual representations, language models, and scaling. Our insights allow us to train a family of VLMs – PRISM s – that outperform state-of-the-art open VLMs such as InstructBLIP and LLaVa-v1.5. However, these models are secondary to the central goal of this work – *establishing a foundation for future work in training and evaluating VLMs*. We hope that our investigation and resources serve as a starting point; a template for reasoning about what matters in developing the next generation of broadly capable VLMs. -->\n",
    "我们对视觉条件语言模型的设计空间进行了严格地调查, 蒸馏出训练未来模型的关键见解. 这项调查得益于两个关键资源贡献: 1) 一个评估套件, 能够对 VLM 的功能进行*细粒度的洞察*; 2) 一个优化的可扩展代码库, 用于训练 VLMs, 并注重*灵活性*———优化程序、图像处理和视觉表示、语言模型和扩展方面的灵活性. 我们的见解使我们能够训练一系列 VLMs —— PRISMs ——它优于最先进的开放式 VLMs, 例如 InstructBLIP 和 LLaVa-v1.5. 然而, 这些模型对于这项工作的核心目标来说是次要的——*为未来训练和评估 VLMs 的工作奠定基础*. 我们希望我们的调查和资源可以作为一个起点; 一个用于推理(开发下一代功能广泛的 VLMs 中)重要事项的模板."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4bf71-0a5e-411b-bb7c-43d202a54edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "pdf_path = \"/mnt/d/github/paper/具身机器人/Prismatic VLMs_Investigating the Design Space of Visually-Conditioned Language Models.pdf\"\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[8]\n",
    "text = page.extract_text()\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "031e3906",
   "metadata": {},
   "source": [
    "Michał, Z., William, C., Karl, P., Oier, M., Chelsea, F., & Sergey, L. (2024). [Robotic Control via Embodied Chain-of-Thought Reasoning](https://arxiv.org/abs/2407.08693). arXiv preprint arXiv:2407.08693.\n",
    "\n",
    "**Abstract**\n",
    "<!-- A key limitation of learned robot control policies is their inability to generalize outside their training data. Recent works on vision-language-action models (VLAs) have shown that the use of large, internet pre-trained vision-language models as the backbone of learned robot policies can substantially improve their robustness and generalization ability. Yet, one of the most exciting capabilities of large vision-language models in other domains is their ability to reason iteratively through complex problems. Can that same capability be brought into robotics to allow policies to improve performance by reasoning about a given task before acting? Naive use of \"chain-of-thought\" (CoT) style prompting is significantly less effective with standard VLAs because of the relatively simple training examples that are available to them. Additionally, purely semantic reasoning about sub-tasks, as is common in regular CoT, is insufficient for robot policies that need to ground their reasoning in sensory observations and the robot state. To this end, we introduce Embodied Chain-of-Thought Reasoning (ECoT) for VLAs, in which we train VLAs to perform multiple steps of reasoning about plans, sub-tasks, motions, and visually grounded features like object bounding boxes and end effector positions, before predicting the robot action. We design a scalable pipeline for generating synthetic training data for ECoT on large robot datasets. We demonstrate, that ECoT increases the absolute success rate of OpenVLA, the current strongest open-source VLA policy, by 28% across challenging generalization tasks, without any additional robot training data. Additionally, ECoT makes it easier for humans to interpret a policy's failures and correct its behavior using natural language. -->\n",
    "学习机器人控制策略的一个关键限制是它们无法在训练数据之外进行泛化. 最近关于视觉-语言-动作模型 (VLAs) 的研究表明, 使用大型、互联网预训练的视觉语言模型作为学习机器人策略的 backbone 可以大幅提高其鲁棒性和泛化能力. 然而, 大型视觉语言模型在其他领域最令人兴奋的能力之一是它们能够迭代推理复杂问题. 能否将同样的能力引入机器人, 通过在行动前推理给定任务, 让策略提高性能? 对于标准 VLAs 而言, 单纯使用\"思路链\"(CoT)风格提示的效果会差很多, 因为它们可用的训练示例相对简单. 此外, 对于(需要以感官观察和机器人状态为基础进行推理的)机器人策略而言, 关于子任务的纯语义推理(如常规 CoT 中常见的那样)是不够的. 为此, 我们为 VLAs 引入具身思维链推理 (ECoT), 在预测机器人动作之前, 我们训练 VLAs 对规划、子任务、运动和视觉基础特征(如物体边界框和末端执行器位置)执行多步推理. 我们设计了一个可扩展的流水线, 用于在大型机器人数据集上为 ECoT 生成合成训练数据. 我们证明, ECoT 在具有挑战性的泛化任务上, 将 OpenVLA (目前最强大的开源 VLA 策略)的绝对成功率提高了 28%, 无需任何额外的机器人训练数据. 此外, ECoT 使人类更容易解释策略的失败, 并使用自然语言纠正其行为."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a11866",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<!-- Robotic policies trained end-to-end for outputting raw low-level actions in response to sensory observations provide a powerful and appealing learning-based approach to robotic control, obviating the need for complex sensing and control stacks, and processing complex observations into dexterous low-level controls [1, 2, 3]. However, this kind of \"reactive\" control is not necessarily the best choice in settings that demand broad generalization, such as novel scenes or interactions with unfamiliar objects. In such situations, a person might need to think more carefully – if they were asked to put fruit on one plate and vegetables on another, they might first try to figure out which objects are fruits and which are vegetables, rather than simply performing a learned skill from \"muscle memory.\" In the same way, we would like our robotic policies to both perform well-practiced end-to-end control, and to \"reason through\" novel situations before grounding their commands into actions. Such reasoning might include identifying and locating task relevant objects, producing a plan to accomplish a task, and translating sub-tasks and observations into movements. -->\n",
    "端到端训练的机器人策略根据感知观察输出原始的下层动作, 为机器人控制提供了一种强大且有吸引力的基于学习的方法, 无需复杂的感知和控制栈, 并将复杂的观察处理为灵巧的下层控制 [1, 2, 3]。然而, 这种\"反应式\"控制在需要广泛泛化的配置中, 不一定是最好的选择, 例如新场景或与不熟悉物体的交互。在这种情况下, 一个人可能需要更仔细地思考——如果要求他们把水果放在一个盘子里, 把蔬菜放在另一个盘子里, 他们可能首先试图弄清楚哪些物体是水果, 哪些是蔬菜, 而不是简单地根据\"肌肉记忆\"执行所学的技能。同样, 我们希望我们的机器人策略既能执行经过良好练习的端到端控制, 又能在将命令付诸行动之前, \"推理\"新情况。这种推理可能包括识别和定位与任务相关的对象, 制定一个计划以完成任务, 以及将子任务和观察转化为运动。\n",
    "\n",
    "<!-- Vision-language-action models (VLAs) – pre-trained vision-language models (VLMs) fine-tuned to produce robot actions – have gained popularity as an approach for leveraging the diversity of Internet data captured within large foundation models [4] in a simple and scalable policy learning recipe. Despite achieving state-of-the-art performance across a wide range of tasks and robot embodiments [5, 6, 7], VLAs typically learn a direct mapping from observations to actions without any intermediate reasoning. However, there have been many recent works exploring how language models (which serve as the backbone of VLAs) can be prompted to textually \"think step-by-step\" about a given task. Such chain-of-thought reasoning (CoT) [8] significantly improves their performance on complex reasoning tasks and is now de-facto a standard practice in language modeling [9]. -->\n",
    "视觉-语言-动作模型 (VLAs) —— 微调预训练的视觉-语言模型 (VLMs) 以生成机器人动作 —— 作为一种利用大型基础模型 [4] 中捕获的互联网数据多样性的方法, 在简单且可扩展的策略学习方法中广受欢迎。尽管在广泛的任务和机器人具身中实现了最先进的性能 [5、6、7], 但 VLAs 通常学习从观察到的动作的直接映射, 而无任何中间推理。然而, 最近有许多研究探索如何提示语言模型(作为 VLAs 的 backbone)以文本形式\"逐步思考\"给定任务。这种思路链推理 (CoT) [8] 显著提高了它们在复杂推理任务上的表现, 现在已成为语言建模领域中的一种事实上的标准做法 [9]。\n",
    "\n",
    "<!-- We thus hypothesize that we can similarly boost VLA performance by training them to **textually reason** about their plan, environment, and motions, thereby allowing them to produce more accurate and robust robot actions. However, simply applying the CoT techniques from language modeling to the robotics domain faces several challenges. For one, current VLAs build on relatively small, open-source VLMs that cannot match closed models in their ability to perform meaningful reasoning when simply prompted to think step-by-step [8]. Additionally, the most common CoT reasoning in language models, breaking tasks into sub-tasks, albeit helpful, is insufficient for reasoning about robotic tasks. The VLA policy needs to ground its plans and reasoning in its *observations* of the environment and robot state. Only then can the reasoning direct the agent's attention toward fine-grained spatial or semantic perceptual features that are key for solving robot manipulation tasks. Put simply, we need VLAs to not only \"think carefully\", but also \"look carefully.\" -->\n",
    "因此, 我们假设我们可以通过训练 VLA 对其规划、环境和动作进行**文本推理**来类似地提高 VLA 的性能, 从而使它们能够生成更准确、更鲁棒的机器人动作。然而, 仅将语言建模中的 CoT 技术应用于机器人领域面临一些挑战。首先, 当前的 VLAs 建立在相对较小的开源 VLMs 上, 这些 VLMs 在仅被提示逐步思考时, 在执行有意义的推理的能力方面无法与封闭模型相媲美 [8]。此外, 语言模型中最常见的 CoT 推理, 将任务分解为子任务, 虽然有帮助, 但不足以推理机器人任务。VLA 策略需要将其计划和推理建立在其对环境和机器人状态的*观察*上。只有这样, 推理才能将智能体的注意力引向细粒度的空间或语义感知特征, 它们是解决机器人操作任务的关键。简而言之, 我们不仅需要 VLAs \"仔细思考\", 还需要\"仔细观察\"。\n",
    "\n",
    "<!-- To this end, we introduce **Embodied Chain-of-Thought Reasoning** (**ECoT**) for VLA policies. In contrast to prior VLAs, embodied chain-of-thought policies perform multiple steps of textual reasoning before predicting the next robot action (see Fig. 1, right). In contrast to existing CoT reasoning approaches for language models, they interleave semantic-level reasoning about sub-tasks with \"embodied\" reasoning tasks that require the policy to pay attention to its multi-modal inputs, from predicting bounding boxes of objects in the scene to reasoning about low-level movement primitives that need to be executed based on the current robot state. To enable the relatively weak LLM backbones of open-source VLAs to perform such reasoning effectively, we design a scalable pipeline for synthetically generating embodied CoT training data for large robot datasets. Concretely, we use powerful pre-trained open-vocabulary object detectors and large language models to generate the reasoning supervision for our policies. -->\n",
    "为此, 我们为 VLA 策略提出**具身思维链推理** (**ECoT**)。与之前的 VLAs 相比, 具身思维链策略在预测下一个机器人动作之前, 执行多步文本推理(见[图1](#fig.1) 右)。与现有的语言模型 CoT 推理方法相比, 它们将子任务的语义级推理与\"具身\"推理任务交织在一起, 这些任务需要策略注意其多模态输入, 从(预测场景中对象的边界框)到(推理下层运动原语, 需要基于当前机器人状态来执行这些运动原语)。为了使开源 VLAs 相对较弱的 LLM backbones 能够有效地执行这种推理, 我们设计了一个可扩展的流水线, 用于(为大型机器人数据集)合成生成具身 CoT 训练数据。具体而言, 我们使用强大的预训练开放词汇表对象检测器和大语言模型, 为我们的策略生成推理指导。\n",
    "\n",
    "todo: 图1\n",
    "\n",
    "Our experiments show that by training state-of-the-art VLAs to perform multiple steps of reasoning before action prediction, we can substantially boost their ability to perform challenging generalization tasks. Our approach increases the absolute success rate of OpenVLA [7], the current best-performing open-source VLA policy, by 28% across a suite of robot manipulation tasks that involve generalization to new objects, scenes, viewpoints, and instructions without any additional robot training data. Beyond raw performance improvements, our experiments show that training VLAs with embodied CoT makes policy failures more interpretable and allows humans to easily correct policy behavior by modifying faulty reasoning chains via natural language feedback.\n",
    "我们的实验表明, 通过训练最先进的 VLAs, 在动作预测之前, 执行多步推理, 我们可以大幅提升它们执行具有挑战性的泛化任务的能力。\n",
    "\n",
    "我们的方法将当前表现最佳的开源 VLA 策略 OpenVLA [7] 的绝对成功率提高了 28%，该策略适用于一系列机器人操作任务，这些任务涉及对新对象、场景、视点和指令的泛化，而无需任何额外的机器人训练数据。\n",
    "\n",
    "除了原始性能改进之外，我们的实验表明，使用具身 CoT 训练 VLA 使策略失败更易于解释，并允许人类通过自然语言反馈修改错误的推理链，从而轻松纠正策略行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c62c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision-language-action models (VLAs) – pre-trained vision-language models (VLMs) fine-tuned\n",
      "to produce robot actions – have gained popularity as an approach for leveraging the diversity of\n",
      "Internet data captured within large foundation models [ 4] in a simple and scalable policy learning\n",
      "recipe. Despite achieving state-of-the-art performance across a wide range of tasks and robot\n",
      "embodiments [ 5,6,7], VLAs typically learn a direct mapping from observations to actions without any\n",
      "intermediate reasoning. However, there have been many recent works exploring how language models\n",
      "(which serve as the backbone of VLAs) can be prompted to textually “think step-by-step” about a\n",
      "given task. Such chain-of-thought reasoning (CoT) [ 8] significantly improves their performance on\n",
      "complex reasoning tasks and is now de-facto a standard practice in language modeling [9].\n",
      "Regular VLA Policy\n",
      "User: Clean the sink.OpenVLA:[x, , Grip] = …ΔΔθΔ\n",
      "Chain-of-Thought\n",
      "User: Clean the sink.\n",
      "CoT OpenVLA:[x, , Grip] = …ΔΔθΔ\n",
      "VLA w/ EmbodiedCoT OpenVLA:Object bounding boxes:\n",
      "Gripper position:\n",
      "CoT OpenVLA:Task Plan: 1. …, 2. …Next Task: pick up carrotNext Move: move right\n",
      "Figure 1: We propose embodied chain-of-\n",
      "thought reasoning for vision-language-action mod-\n",
      "els (VLAs): prior VLAs directly predict the next\n",
      "robot action given the task ( left), we instead train\n",
      "VLA policies to think “step-by-step” ( right ). Cru-\n",
      "cially, reasoning through low-level visual and em-\n",
      "bodied features like object bounding boxes and\n",
      "gripper positions in addition to purely textual CoT\n",
      "elements like sub-task plans, forces the policy to\n",
      "“think carefully” and“look carefully” before act-\n",
      "ing. Embodied CoT reasoning increases the ab-\n",
      "solute success rate of state-of-the-art OpenVLA\n",
      "policies [ 7] by 28% in challenging generalization\n",
      "tasks.We thus hypothesize that we can similarly boost\n",
      "VLA performance by training them to textually\n",
      "reason about their plan, environment, and mo-\n",
      "tions, thereby allowing them to produce more\n",
      "accurate and robust robot actions. However, sim-\n",
      "ply applying the CoT techniques from language\n",
      "modeling to the robotics domain faces several\n",
      "challenges. For one, current VLAs build on rel-\n",
      "atively small, open-source VLMs that cannot\n",
      "match closed models in their ability to perform\n",
      "meaningful reasoning when simply prompted to\n",
      "think step-by-step [ 8]. Additionally, the most\n",
      "common CoT reasoning in language models,\n",
      "breaking tasks into sub-tasks, albeit helpful, is\n",
      "insufficient for reasoning about robotic tasks.\n",
      "The VLA policy needs to ground its plans and\n",
      "reasoning in its observations of the environment\n",
      "and robot state. Only then can the reasoning\n",
      "direct the agent’s attention toward fine-grained\n",
      "spatial or semantic perceptual features that are\n",
      "key for solving robot manipulation tasks. Put\n",
      "simply, we need VLAs to not only “think care-\n",
      "fully”, but also “look carefully.”\n",
      "To this end, we introduce Embodied Chain-of-\n",
      "Thought Reasoning (ECoT) for VLA policies.\n",
      "In contrast to prior VLAs, embodied chain-of-\n",
      "thought policies perform multiple steps of tex-\n",
      "tual reasoning before predicting the next robot\n",
      "action (see Fig. 1, right). In contrast to existing\n",
      "CoT reasoning approaches for language models,\n",
      "they interleave semantic-level reasoning about\n",
      "sub-tasks with “embodied” reasoning tasks that\n",
      "require the policy to pay attention to its multi-modal inputs, from predicting bounding boxes of objects\n",
      "in the scene to reasoning about low-level movement primitives that need to be executed based on the\n",
      "current robot state. To enable the relatively weak LLM backbones of open-source VLAs to perform\n",
      "such reasoning effectively, we design a scalable pipeline for synthetically generating embodied CoT\n",
      "training data for large robot datasets. Concretely, we use powerful pre-trained open-vocabulary object\n",
      "detectors and large language models to generate the reasoning supervision for our policies.\n",
      "Our experiments show that by training state-of-the-art VLAs to perform multiple steps of reasoning\n",
      "before action prediction, we can substantially boost their ability to perform challenging generalization\n",
      "tasks. Our approach increases the absolute success rate of OpenVLA [ 7], the current best-performing\n",
      "open-source VLA policy, by 28% across a suite of robot manipulation tasks that involve generalization\n",
      "to new objects, scenes, viewpoints, and instructions without any additional robot training data. Beyond\n",
      "raw performance improvements, our experiments show that training VLAs with embodied CoT makes\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "pdf_path = \"/mnt/d/cfz/github/Robotic Control via Embodied Chain-of-Thought Reasoning.pdf\"\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[1]\n",
    "text = page.extract_text()\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41695d3",
   "metadata": {},
   "source": [
    "Fu, Z., Zhao, T. Z., & Finn, C. (2024). [Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost Whole-Body Teleoperation](https://arxiv.org/abs/2401.02117). arXiv preprint arXiv:2401.02117.\n",
    "\n",
    "**Abstract**\n",
    "\n",
    "<!-- Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks. In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control. We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system [104] with a mobile base, and a whole-body teleoperation interface. Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks. With 50 demonstrations for each task, co-training can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet. -->\n",
    "&emsp;&emsp;人类示范的模仿学习在机器人技术中表现出令人印象深刻的性能。然而，大多数成果专注于桌面操作，缺乏执行一般实用任务所需的移动性和灵巧性。 在这项工作中，我们开发了一个模仿移动操作任务的系统，这些任务是双手的，并且需要全身控制。 我们首先介绍Mobile ALOHA，这是一个低成本的全身远程操作系统，用于数据收集。它通过移动基座和全身远程操作界面增强了ALOHA系统。 使用Mobile ALOHA收集的数据，我们然后执行监督行为克隆，并发现与现有的静态ALOHA数据集共同训练可以提高移动操作任务的性能。 通过每项任务50次演示，共同训练可以将成功率提高到90%，使Mobile ALOHA能够自主完成复杂的移动操作任务，例如煎炒并上菜一只虾、 打开双门墙橱存放重型烹饪锅具、呼叫并进入电梯、 以及使用厨房水龙头轻轻冲洗使用过的平底锅。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe430692",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "# Related Work\n",
    "# Mobile ALOHA Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d64502",
   "metadata": {},
   "source": [
    "# Co-training with Static ALOHA Data\n",
    "\n",
    "<!-- The typical approach for using imitation learning to solve real-world robotics tasks relies on using the datasets that are collected on a specific robot hardware platform for a targeted task. This straightforward approach, however, suffers from lengthy data collection processes where human operators collect demonstration data from scratch for every task on the a specific robot hardware platform. The policies trained on these specialized datasets are often not robust to the perceptual perturbations (e.g. distractors and lighting changes) due to the limited visual diversity in these datasets [95]. Recently, co-training on diverse real-world datasets collected from different but similar types of robots have shown promising results on single-arm manipulation [11, 20, 31, 61], and on navigation [79]. -->\n",
    "&emsp;&emsp;使用模仿学习解决现实世界机器人任务的典型方法依赖于使用在特定机器人硬件平台上收集的数据集来完成目标任务。然而, 这种简单的方法存在冗长的数据收集过程, 其中人类操作员在特定的机器人硬件平台上, 从头开始为每项任务收集演示数据。 由于这些数据集中的视觉多样性有限, 在这些专门数据集上训练的策略通常对感知扰动（例如干扰因素和照明变化）并不鲁棒[95]。 最近, 对(从不同但相似类型的机器人收集的各种现实世界)数据集进行共同训练, 在单臂操作 [11,20,31,61] 和导航 [79] 方面已显示出有希望的结果。\n",
    "\n",
    "<!-- In this work, we use a co-training pipeline that leverages the existing *static ALOHA* datasets to improve the performance of imitation learning for mobile manipulation, specifically for the bimanual arm actions. The *static ALOHA* datasets [81, 104] have 825 demonstrations in total for tasks including Ziploc sealing, picking up a fork, candy wrapping, tearing a paper towel, opening a plastic portion cup with a lid, playing with a ping pong, tape dispensing, using a coffee machine, pencil hand-overs, fastening a velcro cable, slotting a battery, and handling over a screw driver. Notice that the *static ALOHA* data is all collected on a black table-top with the two arms fixed to face towards each other. This setup is different from *Mobile ALOHA* where the background changes with the moving base and the two arms are placed in parallel facing the front. We do not use any special data processing techniques on either the RGB observations or the bimanual actions of the *static ALOHA* data for our co-training. -->\n",
    "&emsp;&emsp;在这项工作中, 我们使用共同训练pipeline, 它利用已有的*静态 ALOHA*数据集, 提高移动操作的模仿学习性能, 特别是双手操作。*静态 ALOHA* 数据集 [81, 104] 总共有 825 个任务演示, 包括塑料密封、拿起叉子、糖果包装、撕纸巾、打开带盖的塑料杯、打乒乓球、 胶带分配、使用咖啡机、铅笔交接、紧固魔术贴电缆、插入电池以及递上螺丝刀。请注意, *静态 ALOHA* 数据全部在黑色桌面上收集, 桌上两只手臂面对面固定。该配置与\"移动 ALOHA\"不同, 其中背景随着底座的移动而变化, 并且两个手臂平行放置, 面向前方。我们在共同训练中没有对(RGB观察或*静态 ALOHA* 数据的双手操作)使用任何特殊的数据处理技术。\n",
    "\n",
    "<!-- Denote the aggregated *static ALOHA* data as as $D_{static}$, and the *Mobile ALOHA* dataset for a task $m$ as $D_{mobile}^m$. The bimanual actions are formulated as target joint positions $a_{arms} \\in \\mathbb{R}^{14}$ which includes two continuous gripper actions, and the base actions are formulated as target base linear and angular velocities $a_{base} \\in \\mathbb{R}^2$. The training objective for a mobile manipulation policy $\\pi^m$ for a task $m$ is -->\n",
    "&emsp;&emsp;将聚合的 *static ALOHA* 数据表示为 $D_{static}$, 将任务 $m$ 的 *Mobile ALOHA* 数据集表示为 $D_{mobile}^m$。双手操作被建模为目标关节位置 $a_{arms} \\in \\mathbb{R}^{14}$, 其中包括两个连续的夹持操作, 基本操作被建模为目标基本线性速度和角速度 $a_{base} \\in \\mathbb{R}^2$。任务 $m$ 的移动操作策略 $\\pi^m$ 的训练目标为\n",
    "\n",
    "$$ \\mathbb{E}_{\\left( o^i, a_{arms}^i, a_{base}^i \\right) \\sim D_{mobile}^m } \\left[ L\\left( a_{arms}^i, a_{base}^i, \\pi^m (o^i) \\right) \\right] + \\mathbb{E}_{\\left( o^i, a_{arms}^i \\right) \\sim D_{static} } \\left[ L\\left( a_{arms}^i, [0, 0], \\pi^m (o^i) \\right) \\right] $$\n",
    "\n",
    "<!-- where $o^i$ is the observation consisting of two wrist camera RGB observations, one egocentric top camera RGB observation mounted between the arms, and joint positions of the arms, and $L$ is the imitation loss function. We sample with equal probability from the *static ALOHA* data $D_{static}$ and the *Mobile ALOHA* data $D_{mobile}^m$. We set the batch size to be 16. Since *static ALOHA* datapoints have no mobile base actions, we zero-pad the action labels so actions from both datasets have the same dimension. We also ignore the front camera in the *static ALOHA* data so that both datasets have 3 cameras. We normalize every action based on the statistics of the *Mobile ALOHA* dataset $D_{mobile}^m$ alone. In our experiments, we combine this co-training recipe with multiple base imitation learning approaches, including ACT [104], Diffusion Policy [18], and VINN [63]. -->\n",
    "其中 $o^i$ 是观察值, 包括两台手腕相机 RGB 观察值、一台(安装在手臂之间的以自我为中心的)顶部摄像头 RGB 观察值以及手臂关节位置, $L$ 是模仿损失函数。我们以相等概率从 *static ALOHA* 数据 $D_{static}$ 和 *Mobile ALOHA* 数据 $D_{mobile}^m$ 中采样。我们设置批量大小为16。由于*静态 ALOHA* 数据点没有移动基本操作, 我们对操作标签进行零填充, 以便两个数据集的操作具有相同的维度。我们还忽略 *static ALOHA* 数据中的前置摄像头, 以便两个数据集都有3个摄像头。我们仅基于 *Mobile ALOHA* 数据集 $D_{mobile}^m$ 的统计数据, 对每个操作进行标准化。在我们的实验中, 我们将这种共同训练方法与多种基准模仿学习方法结合, 包括 ACT [[104](#paper.104)]、扩散策略 [[18](#paper.18)] 和 VINN [[63](#paper.63)]。\n",
    "> - <span id='paper.104'></span> Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. RSS, 2023.\n",
    "> - <span id='paper.18'></span> Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and Systems (RSS), 2023.\n",
    "> - <span id='paper.63'></span> Jyothish Pari, Nur Muhammad Shafiullah, Sridhar Pandian Arunachalam, and Lerrel Pinto. The surprising effectiveness of representation learning for visual imitation. arXiv preprint arXiv:2112.01511, 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc15a09",
   "metadata": {},
   "source": [
    "# 安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/MarkFzp/act-plus-plus.git\n",
    "cd act-plus-plus\n",
    "\n",
    "conda create -n aloha python=3.8.10\n",
    "\n",
    "conda activate aloha\n",
    "\n",
    "pip install torchvision\n",
    "pip install torch # note: not pytorch and pytorch-cuda\n",
    "pip install pyquaternion\n",
    "pip install pyyaml\n",
    "pip install rospkg\n",
    "pip install pexpect\n",
    "pip install mujoco==2.3.7\n",
    "pip install dm_control==1.0.14\n",
    "pip install opencv-python\n",
    "pip install matplotlib\n",
    "pip install einops\n",
    "pip install packaging\n",
    "pip install h5py\n",
    "pip install ipython\n",
    "cd detr && pip install -e .\n",
    "\n",
    "cd ../\n",
    "git clone https://github.com/ARISE-Initiative/robomimic/tree/r2d2\n",
    "cd robomimic-r2d2\n",
    "pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e5f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进入python进行验证\n",
    "import torch\n",
    "print(\"Torch version:\",torch.__version__)\n",
    "print(\"Is CUDA enabled?\",torch.cuda.is_available())\n",
    "exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcf919c",
   "metadata": {},
   "source": [
    "- `TypeError: forward() got an unexpected keyword argument 'pos'`<br>\n",
    "Solution: Modify `build_transformer` at line 285 of `detr/models/detr_vae.py` to `build_encoder`<br>\n",
    "refer to: [源码复现|10分钟带你复现Mobile ALOHA SIM，超详细教程（附范文+代码）](https://zhuanlan.zhihu.com/p/678274908)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbddccd",
   "metadata": {},
   "source": [
    "- 译文 https://zhuanlan.zhihu.com/p/676704359\n",
    "- https://zhuanlan.zhihu.com/p/680100021\n",
    "\n",
    "- generate episodes\n",
    "    - python3 record_sim_episodes.py --task_name sim_transfer_cube_scripted --dataset_dir data/sim_transfer_cube_scripted --num_episodes 50\n",
    "    - python3 record_sim_episodes.py --task_name sim_insertion_scripted --dataset_dir data/sim_insertion_scripted --num_episodes 50 --onscreen_render # 实时渲染\n",
    "- 可视化 \n",
    "    - python3 visualize_episodes.py --dataset_dir data/sim_transfer_cube_scripted --episode_idx 9\n",
    "    - python3 visualize_episodes.py --dataset_dir data/sim_insertion_scripted --episode_idx 1\n",
    "- 训练\n",
    "    - python3 imitate_episodes.py --task_name sim_transfer_cube_scripted --ckpt_dir trainings_cube --lr 1e-5 --seed 0 --batch_size 8 --num_steps 20000 --policy_class ACT --kl_weight 10 --chunk_size 100 --hidden_dim 512 --dim_feedforward 3200 --temporal_agg\n",
    "    - python3 imitate_episodes.py --task_name sim_insertion_scripted --ckpt_dir trainings_insertion --lr 1e-5 --seed 0 --batch_size 8 --num_steps 20000 --policy_class ACT --kl_weight 10 --chunk_size 100 --hidden_dim 512 --dim_feedforward 3200 --temporal_agg\n",
    "    - python3 imitate_episodes.py --task_name all --ckpt_dir trainings_all --lr 1e-5 --seed 0 --batch_size 8 --num_steps 1 --policy_class ACT --kl_weight 10 --chunk_size 100 --hidden_dim 512 --dim_feedforward 3200 --temporal_agg\n",
    "- 评估\n",
    "    - python3 imitate_episodes.py --eval --task_name sim_transfer_cube_scripted --ckpt_dir trainings_cube --lr 1e-5 --seed 0 --batch_size 8 --num_steps 20000 --policy_class ACT --kl_weight 10 --chunk_size 100 --hidden_dim 512 --dim_feedforward 3200 --temporal_agg\n",
    "    - python3 imitate_episodes.py --eval --onscreen_render --task_name sim_insertion_scripted --ckpt_dir trainings_insertion --lr 1e-5 --seed 0 --batch_size 8 --num_steps 20000 --policy_class ACT --kl_weight 10 --chunk_size 100 --hidden_dim 512 --dim_feedforward 3200 --temporal_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad01e6",
   "metadata": {},
   "source": [
    "todo download:\n",
    "Kaur, D. P., Singh, N. P., & Banerjee, B. (2023). A review of platforms for simulating embodied agents in 3D virtual environments. Artificial Intelligence Review, 56(4), 3711-3753."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c64ba8d",
   "metadata": {},
   "source": [
    "- [ ] [DETR原理与代码超详细解读](https://blog.csdn.net/weixin_38252409/article/details/133935125)\n",
    "- [ ] [vision transformer DETR原理及代码详解（一）](https://blog.csdn.net/qq_35831906/article/details/124118569)\n",
    "- [ ] [vision transformer DETR原理及代码详解（二）](https://blog.csdn.net/qq_35831906/article/details/124123410)\n",
    "- [ ] [vision transformer DETR原理及代码详解（三）](https://blog.csdn.net/qq_35831906/article/details/124124588)\n",
    "- [ ] [vision transformer DETR原理及代码详解（四）](https://blog.csdn.net/qq_35831906/article/details/124168819)\n",
    "- [ ] [从DETR backbone 的NestedTensor 到DataLoader, Sampler,collate_fn，再到DETR transformer](https://blog.csdn.net/qq_35831906/article/details/124524455)\n",
    "- [ ] [DETR原理与代码精讲](https://edu.csdn.net/course/detail/36768)\n",
    "- [ ] [手撕DETR(原理与代码精讲)](https://www.bilibili.com/cheese/play/ep301097)\n",
    "- [ ] [Transformer系列](https://blog.csdn.net/sinat_41942180/category_12464985.html)\n",
    "- [ ] [Transformer实战](https://blog.csdn.net/weixin_50592077/category_12572201.html)\n",
    "- [ ] [DETR 代码解读](https://shihan-ma.github.io/posts/2021-04-15-DETR_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc4dac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

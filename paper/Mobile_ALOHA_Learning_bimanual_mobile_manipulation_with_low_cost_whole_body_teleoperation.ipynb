{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41695d3",
   "metadata": {},
   "source": [
    "**Abstract**\n",
    "\n",
    "<!-- Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks. In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control. We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system [104] with a mobile base, and a whole-body teleoperation interface. Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks. With 50 demonstrations for each task, co-training can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet. -->\n",
    "&emsp;&emsp;人类示范的模仿学习在机器人技术中表现出令人印象深刻的性能。然而，大多数成果专注于桌面操作，缺乏执行一般实用任务所需的移动性和灵巧性。 在这项工作中，我们开发了一个模仿移动操作任务的系统，这些任务是双手的，并且需要全身控制。 我们首先介绍Mobile ALOHA，这是一个低成本的全身远程操作系统，用于数据收集。它通过移动基座和全身远程操作界面增强了ALOHA系统。 使用Mobile ALOHA收集的数据，我们然后执行监督行为克隆，并发现与现有的静态ALOHA数据集共同训练可以提高移动操作任务的性能。 通过每项任务50次演示，共同训练可以将成功率提高到90%，使Mobile ALOHA能够自主完成复杂的移动操作任务，例如煎炒并上菜一只虾、 打开双门墙橱存放重型烹饪锅具、呼叫并进入电梯、 以及使用厨房水龙头轻轻冲洗使用过的平底锅。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe430692",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "# Related Work\n",
    "# Mobile ALOHA Hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d64502",
   "metadata": {},
   "source": [
    "# Co-training with Static ALOHA Data\n",
    "\n",
    "<!-- The typical approach for using imitation learning to solve real-world robotics tasks relies on using the datasets that are collected on a specific robot hardware platform for a targeted task. This straightforward approach, however, suffers from lengthy data collection processes where human operators collect demonstration data from scratch for every task on the a specific robot hardware platform. The policies trained on these specialized datasets are often not robust to the perceptual perturbations (e.g. distractors and lighting changes) due to the limited visual diversity in these datasets [95]. Recently, co-training on diverse real-world datasets collected from different but similar types of robots have shown promising results on single-arm manipulation [11, 20, 31, 61], and on navigation [79]. -->\n",
    "&emsp;&emsp;使用模仿学习解决现实世界机器人任务的典型方法依赖于使用在特定机器人硬件平台上收集的数据集来完成目标任务。然而, 这种简单的方法存在冗长的数据收集过程, 其中人类操作员在特定的机器人硬件平台上, 从头开始为每项任务收集演示数据。 由于这些数据集中的视觉多样性有限, 在这些专门数据集上训练的策略通常对感知扰动（例如干扰因素和照明变化）并不鲁棒[95]。 最近, 对(从不同但相似类型的机器人收集的各种现实世界)数据集进行共同训练, 在单臂操作 [11,20,31,61] 和导航 [79] 方面已显示出有希望的结果。\n",
    "\n",
    "<!-- In this work, we use a co-training pipeline that leverages the existing *static ALOHA* datasets to improve the performance of imitation learning for mobile manipulation, specifically for the bimanual arm actions. The *static ALOHA* datasets [81, 104] have 825 demonstrations in total for tasks including Ziploc sealing, picking up a fork, candy wrapping, tearing a paper towel, opening a plastic portion cup with a lid, playing with a ping pong, tape dispensing, using a coffee machine, pencil hand-overs, fastening a velcro cable, slotting a battery, and handling over a screw driver. Notice that the *static ALOHA* data is all collected on a black table-top with the two arms fixed to face towards each other. This setup is different from *Mobile ALOHA* where the background changes with the moving base and the two arms are placed in parallel facing the front. We do not use any special data processing techniques on either the RGB observations or the bimanual actions of the *static ALOHA* data for our co-training. -->\n",
    "&emsp;&emsp;在这项工作中, 我们使用共同训练pipeline, 它利用已有的*静态 ALOHA*数据集, 提高移动操作的模仿学习性能, 特别是双手操作。*静态 ALOHA* 数据集 [81, 104] 总共有 825 个任务演示, 包括塑料密封、拿起叉子、糖果包装、撕纸巾、打开带盖的塑料杯、打乒乓球、 胶带分配、使用咖啡机、铅笔交接、紧固魔术贴电缆、插入电池以及递上螺丝刀。请注意, *静态 ALOHA* 数据全部在黑色桌面上收集, 桌上两只手臂面对面固定。该配置与\"移动 ALOHA\"不同, 其中背景随着底座的移动而变化, 并且两个手臂平行放置, 面向前方。我们在共同训练中没有对(RGB观察或*静态 ALOHA* 数据的双手操作)使用任何特殊的数据处理技术。\n",
    "\n",
    "<!-- Denote the aggregated *static ALOHA* data as as $D_{static}$, and the *Mobile ALOHA* dataset for a task $m$ as $D_{mobile}^m$. The bimanual actions are formulated as target joint positions $a_{arms} \\in \\mathbb{R}^{14}$ which includes two continuous gripper actions, and the base actions are formulated as target base linear and angular velocities $a_{base} \\in \\mathbb{R}^2$. The training objective for a mobile manipulation policy $\\pi^m$ for a task $m$ is -->\n",
    "&emsp;&emsp;将聚合的 *static ALOHA* 数据表示为 $D_{static}$, 将任务 $m$ 的 *Mobile ALOHA* 数据集表示为 $D_{mobile}^m$。双手操作被建模为目标关节位置 $a_{arms} \\in \\mathbb{R}^{14}$, 其中包括两个连续的夹持操作, 基本操作被建模为目标基本线性速度和角速度 $a_{base} \\in \\mathbb{R}^2$。任务 $m$ 的移动操作策略 $\\pi^m$ 的训练目标为\n",
    "\n",
    "$$ \\mathbb{E}_{\\left( o^i, a_{arms}^i, a_{base}^i \\right) \\sim D_{mobile}^m } \\left[ L\\left( a_{arms}^i, a_{base}^i, \\pi^m (o^i) \\right) \\right] + \\mathbb{E}_{\\left( o^i, a_{arms}^i \\right) \\sim D_{static} } \\left[ L\\left( a_{arms}^i, [0, 0], \\pi^m (o^i) \\right) \\right] $$\n",
    "\n",
    "<!-- where $o^i$ is the observation consisting of two wrist camera RGB observations, one egocentric top camera RGB observation mounted between the arms, and joint positions of the arms, and $L$ is the imitation loss function. We sample with equal probability from the *static ALOHA* data $D_{static}$ and the *Mobile ALOHA* data $D_{mobile}^m$. We set the batch size to be 16. Since *static ALOHA* datapoints have no mobile base actions, we zero-pad the action labels so actions from both datasets have the same dimension. We also ignore the front camera in the *static ALOHA* data so that both datasets have 3 cameras. We normalize every action based on the statistics of the *Mobile ALOHA* dataset $D_{mobile}^m$ alone. In our experiments, we combine this co-training recipe with multiple base imitation learning approaches, including ACT [104], Diffusion Policy [18], and VINN [63]. -->\n",
    "其中 $o^i$ 是观察值, 包括两台手腕相机 RGB 观察值、一台(安装在手臂之间的以自我为中心的)顶部摄像头 RGB 观察值以及手臂关节位置, $L$ 是模仿损失函数。我们以相等概率从 *static ALOHA* 数据 $D_{static}$ 和 *Mobile ALOHA* 数据 $D_{mobile}^m$ 中采样。我们设置批量大小为16。由于*静态 ALOHA* 数据点没有移动基本操作, 我们对操作标签进行零填充, 以便两个数据集的操作具有相同的维度。我们还忽略 *static ALOHA* 数据中的前置摄像头, 以便两个数据集都有3个摄像头。我们仅基于 *Mobile ALOHA* 数据集 $D_{mobile}^m$ 的统计数据, 对每个操作进行标准化。在我们的实验中, 我们将这种共同训练方法与多种基准模仿学习方法结合, 包括 ACT [[104](#paper.104)]、扩散策略 [[18](#paper.18)] 和 VINN [[63](#paper.63)]。\n",
    "> - <span id='paper.104'></span> Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. RSS, 2023.\n",
    "> - <span id='paper.18'></span> Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and Systems (RSS), 2023.\n",
    "> - <span id='paper.63'></span> Jyothish Pari, Nur Muhammad Shafiullah, Sridhar Pandian Arunachalam, and Lerrel Pinto. The surprising effectiveness of representation learning for visual imitation. arXiv preprint arXiv:2112.01511, 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61343a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b261a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c3ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install robomimic\n",
    "refer to: https://github.com/ARISE-Initiative/robomimic/tree/r2d2\n",
    "Clone the repo with the --recurse-submodules flag.\n",
    "(if applicable) switch to r2d2 branch\n",
    "Run pip install -e . in robomimic\n",
    "Run pip install -e . in robomimic/act/detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ecd722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MarkFzp/act-plus-plus 安装依赖库\n",
    "conda create -n aloha python=3.9\n",
    "conda activate aloha\n",
    "conda install conda=24.1.2\n",
    "conda install pytorch=2.0.0 pytorch-cuda=11.8 torchvision=0.15.0 -c pytorch -c nvidia\n",
    "conda install pyquaternion=0.9.9 -c conda-forge\n",
    "conda install pyyaml=6.0 -c conda-forge\n",
    "conda install rospkg=1.5.0 -c conda-forge\n",
    "conda install pexpect=4.8.0 -c conda-forge\n",
    "conda install mujoco=2.3.3 -c conda-forge\n",
    "conda install dm_control=1.0.9 -c conda-forge\n",
    "conda install einops=0.6.0 -c conda-forge\n",
    "conda install packaging=23.0 -c conda-forge\n",
    "conda install h5py=3.8.0 -c conda-forge\n",
    "conda install ipython=8.12.0 -c conda-forge\n",
    "conda install matplotlib=3.7.1 -c conda-forge\n",
    "pip install opencv-python==4.7.0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ARISE-Initiative/robomimic 安装依赖库\n",
    "conda install numpy -c conda-forge\n",
    "conda install psutil -c conda-forge\n",
    "conda install tqdm -c conda-forge\n",
    "conda install termcolor -c conda-forge\n",
    "conda install tensorboard -c conda-forge\n",
    "conda install tensorboardX -c conda-forge\n",
    "conda install imageio -c conda-forge\n",
    "conda install imageio-ffmpeg -c conda-forge\n",
    "conda install diffusers=0.11.1 -c conda-forge\n",
    "conda install pytorch3d -c conda-forge\n",
    "conda install tianshou=0.4.10 -c conda-forge\n",
    "conda install transformers -c conda-forge\n",
    "conda install huggingface_hub=0.16.4 -c conda-forge\n",
    "pip install egl_probe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23d1196",
   "metadata": {},
   "source": [
    "**py-opencv vs opencv-python**  \n",
    "refer to: https://juejin.cn/s/py-opencv%20vs%20opencv-python  \n",
    "Py-OpenCV是OpenCV 3的Python绑定，而OpenCV-Python则是OpenCV 4的Python绑定。  \n",
    "在安装方面，Py-OpenCV需要在系统上安装C++的OpenCV库，并在Python中引用对应的Python模块。而OpenCV-Python则只需要通过pip安装即可，不需要额外安装C++库。  \n",
    "在引用方式上，二者也有一些不同。在Py-OpenCV中，需要先导入cv2模块，然后使用cv2.XXX的方式来调用OpenCV的各种函数。而在OpenCV-Python中，直接导入cv2即可，然后使用cv2.XXX的方式来调用OpenCV的各种函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d56b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ..../act-plus-plus/deter/\n",
    "pip install -e .\n",
    "cd ..../robomimic-r2d2/\n",
    "pip install -e .\n",
    "cd ..../act-plus-plus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbedda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Cube task\n",
    "python3 imitate_episodes.py --task_name sim_transfer_cube_scripted --ckpt_dir /home/qj00182/cfz/ALOHO --policy_class ACT --kl_weight 10 --chunk_size 100 --hidden_dim 512 --batch_size 8 --dim_feedforward 3200 --num_steps 2000  --lr 1e-5 --seed 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18d0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb396b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e5f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b98cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d5ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d09d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d17085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a7a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e8568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9223708b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

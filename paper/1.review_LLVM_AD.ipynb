{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f019e67",
   "metadata": {},
   "source": [
    "- review\n",
    "    - [x] A Survey on Multimodal Large Language Models for Autonomous Driving\n",
    "[github](https://github.com/IrohXu/Awesome-Multimodal-LLM-Autonomous-Driving) [paper](https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Cui_A_Survey_on_Multimodal_Large_Language_Models_for_Autonomous_Driving_WACVW_2024_paper.html) [ipynb](./A_survey_on_multimodal_large_language_models_for_autonomous_driving.ipynb)\n",
    "    - [x] https://github.com/PJLab-ADG/awesome-knowledge-driven-AD\n",
    "    - [x] https://github.com/Thinklab-SJTU/Awesome-LLM4AD\n",
    "        - [LLM4Drive: A Survey of Large Language Models for Autonomous Driving](https://arxiv.org/abs/2311.01043)\n",
    "- [人工智能顶刊顶](https://zhuanlan.zhihu.com/p/585191008)\n",
    "    - [CVPR 2024 Accepted Papers](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers)\n",
    "    - https://github.com/amusi/CVPR2024-Papers-with-Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdcaf0c",
   "metadata": {},
   "source": [
    "Model | Year | Backbone | Task | Modality | Learning | Input | Output | Github | Website | Datasets | Conferences | Team\n",
    ":- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- | :- \n",
    "[P-MapNet: Far-seeing Map Generator Enhanced by both SDMap and HDMap Priors](https://arxiv.org/abs/2403.10521) | 2024 | | map | | | | | [github](https://github.com/jike5/P-MapNet) | | | | <font color=\"red\"> &#10004; </font>\n",
    "[Neural Map Prior for Autonomous Driving](https://openaccess.thecvf.com/content/CVPR2023/papers/Xiong_Neural_Map_Prior_for_Autonomous_Driving_CVPR_2023_paper.pdf) | 2023 | | map | | | | | [github](https://github.com/Tsinghua-MARS-Lab/neural_map_prior)\n",
    "[Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving](https://arxiv.org/abs/2310.01957) | 2024 | LLaMA | Perception Control | Vision, Language | Finetuning | Vector Query | Response / Actions | [github](https://github.com/wayveai/driving-with-llms) | [ipynb](./Driving_with_LLMs_Fusing_Object_Level_Vector_Modality_for_Explainable_Autonomous_Driving.ipynb) | | ICRA | Wayve\n",
    "[LingoQA: Video Question Answering for Autonomous Driving](https://arxiv.org/abs/2312.14115) | 2023 | | | | | | | [github](https://github.com/wayveai/LingoQA) | | | | Wayve\n",
    "[GAIA-1: A Generative World Model for Autonomous Driving](https://arxiv.org/abs/2309.17080) | 2023 | - | Planning | Vision, Language | Pretraining | Video Prompt | Video | | | | |  Wayve\n",
    " |\n",
    "[DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models](https://arxiv.org/abs/2309.16292) | 2024 | GPT-3.5 GPT-4 | Planning Control | Language | In-context learning | Text | Action | [github](https://github.com/PJLab-ADG/DiLu) | [website](https://pjlab-adg.github.io/DiLu/) | [HighwayEnv](https://github.com/Farama-Foundation/HighwayEnv) [CitySim](https://github.com/UCF-SST-Lab/UCF-SST-CitySim1-Dataset) | ICLR | [PJLab-ADG](https://pjlab-adg.github.io/)\n",
    "[Empowering Autonomous Driving with Large Language Models: A Safety Perspective](https://arxiv.org/abs/2312.00812) | 2024 | | Planning | | | | | [github](https://github.com/wangyixu14/llm_conditioned_mpc_ad) | | [HighwayEnv](https://github.com/Farama-Foundation/HighwayEnv) | ICLR | &#10004;\n",
    "[Drive Like a Human: Rethinking Autonomous Driving with Large Language Models](https://arxiv.org/abs/2307.07162) | 2023 | GPT-3.5 | Planning Control | Language | In-context learning | Text | Action | [github](https://github.com/PJLab-ADG/DriveLikeAHuman) | | [HighwayEnv](https://github.com/Farama-Foundation/HighwayEnv) | [WACV](https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Fu_Drive_Like_a_Human_Rethinking_Autonomous_Driving_With_Large_Language_WACVW_2024_paper.html) | [PJLab-ADG](https://pjlab-adg.github.io/) &#10008;\n",
    "[LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous Driving](https://arxiv.org/abs/2402.01246) | 2024 | LimSim, GPT-4 | Planning | Simulator BEV, Language | In-context learning | Simulator Vision, Language | Text / Action | [github](https://github.com/PJLab-ADG/LimSim/tree/LimSim_plus) | [website](https://pjlab-adg.github.io/limsim_plus/) | | | [PJLab-ADG](https://pjlab-adg.github.io/)\n",
    "[On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving](https://arxiv.org/abs/2311.05332) | 2023 | GPT-4Vision | Perception | Vision, Language | In-context learning | RGB Image Text | Text Description | | | [nuScenes](https://www.nuscenes.org/nuscenes) [BDD-X](https://github.com/JinkyuKimUCB/BDD-X-dataset) [Chinese Traffic Sign](https://nlpr.ia.ac.cn/pal/trafficdata/detection.html) [Waymo](https://arxiv.org/abs/1912.04838) [DAIR-V2X](https://thudair.baai.ac.cn/index) [CitySim](https://github.com/UCF-SST-Lab/UCF-SST-CitySim1-Dataset) [D2-City](https://arxiv.org/abs/1904.01975) [CODA](https://arxiv.org/abs/2203.07724) [carla-simulator](https://github.com/carla-simulator) | | [PJLab-ADG](https://pjlab-adg.github.io/)\n",
    " |\n",
    "[DriveLM: Driving with Graph Visual Question Answering](https://arxiv.org/abs/2312.14150) | 2023 | GVQA | Perception Planning | Vision, Language | Training | RGB Image Text | Text / Action | [github](https://github.com/OpenDriveLab/DriveLM) | [website](https://opendrivelab.com/DriveLM/) | [OpenLane-V2](https://github.com/OpenDriveLab/OpenLane-V2) | | [OpenDriveLab](https://opendrivelab.com/team/) <font color=\"red\"> &#10004; </font>\n",
    "[Embodied Understanding of Driving Scenarios](https://arxiv.org/abs/2403.04593) | 2024 | | | | | | | [github](https://github.com/OpenDriveLab/ELM/) | | | | [OpenDriveLab](https://opendrivelab.com/team/) &#10004;\n",
    "[UniAD: Planning-oriented Autonomous Driving](https://arxiv.org/abs/2212.10156) | 2023 | | | | | | | [github](https://github.com/OpenDriveLab/UniAD) | | | CVPR | [OpenDriveLab](https://opendrivelab.com/team/) &#10008;\n",
    " |\n",
    "[A Language Agent for Autonomous Driving](https://arxiv.org/abs/2311.10813) | 2023 | GPT-3.5 | Planning | Language | Training | Text | Action | [github](https://github.com/USC-GVL/Agent-Driver) | [website](https://usc-gvl.github.io/Agent-Driver/) | [nuScenes](https://www.nuscenes.org/nuscenes) | | <font color=\"red\"> &#10004; </font>\n",
    "[GPT-Driver: Learning to Drive with GPT](https://arxiv.org/abs/2310.01415) | 2023 | GPT-3.5 | Planning | Vision, Language | In-context learning | Text | Trajectory | [github](https://github.com/PointsCoder/GPT-Driver) | [website](https://pointscoder.github.io/projects/gpt_driver/) | [nuScenes](https://www.nuscenes.org/nuscenes) | [NeurIPS](https://neurips.cc/virtual/2023/82857) | &#10008;\n",
    "[GenAD: Generative End-to-End Autonomous Driving](https://arxiv.org/abs/2402.11502) | 2024 | | | | | | | [github](https://github.com/wzzheng/GenAD) | | [nuScenes](https://www.nuscenes.org/nuscenes) | | &#10008;\n",
    "[VAD: Vectorized Scene Representation for Efficient Autonomous Driving](https://arxiv.org/abs/2303.12077) | 2023 | | | | | | | [github](https://github.com/hustvl/VAD) | | | ICCV | &#10008;\n",
    " |\n",
    "[Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving](https://arxiv.org/abs/2310.02251) | 2024 | Flan5XXL Vicuna-13b | Perception Planning | Vision, Language | In-context learning | Image Query | Response | [github](https://github.com/llmbev/talk2bev) | [website](https://llmbev.github.io/talk2bev/) | | ICRA | &#10004;\n",
    "[LMDrive: Closed-Loop End-to-End Driving with Large Language Models](https://arxiv.org/abs/2312.07488) | 2024 | CARLA + LLaVA | Planning Control | Vision, Language | Training | RGB Image LiDAR Text | Control Signal | [github](https://github.com/opendilab/LMDrive) | [website](https://hao-shao.com/projects/lmdrive.html) | [carla-simulator](https://github.com/carla-simulator) | [CVPR](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers)\n",
    "[LangProp: A code optimization framework using Language Models applied to driving](https://arxiv.org/abs/2401.10314) | 2024 | IL, DAgger, RL + ChatGPT | Planning (Code / Action Generation) | CARLA simulator Vsion, Language | Training | CARLA simulator Text | Code as action | [github](https://github.com/shuishida/LangProp) | | | ICLR\n",
    "[ChatSim: Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents](https://arxiv.org/abs/2402.05746) | 2024 | GPT-4 | Perception (Image Editing) | Image, Language | In-context learning | Vision, Language | Image | [github](https://github.com/yifanlu0227/ChatSim) | [website](https://yifanlu0227.github.io/ChatSim/) | [Waymo](https://waymo.com/open/) | ICLR\n",
    "[Human-Centric Autonomous Systems With LLMs for User Command Reasoning](https://arxiv.org/abs/2311.08206) | 2024 | | | | | | | [github](https://github.com/KTH-RPL/DriveCmd_LLM) | | [UCU](https://github.com/LLVM-AD/ucu-dataset) | [WACV](https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Yang_Human-Centric_Autonomous_Systems_With_LLMs_for_User_Command_Reasoning_WACVW_2024_paper.html)\n",
    "[ADAPT: Action-aware Driving Caption Transformer](https://arxiv.org/abs/2302.00673) | 2023 | | | | | | | [github](https://github.com/jxbbb/ADAPT) | | [BDD-X](https://github.com/JinkyuKimUCB/BDD-X-dataset) | ICRA | &#10008;\n",
    "[GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation](https://arxiv.org/abs/2306.04607) | 2024 | | | | | | | [github](https://github.com/KaiChen1998/GeoDiffusion) | | | ICLR\n",
    "[Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving](https://arxiv.org/abs/2311.17918) | 2024 | | | | | | | [github](https://github.com/BraveGroup/Drive-WM) | [website](https://drive-wm.github.io/) | [nuScenes](https://www.nuscenes.org/nuscenes) [Waymo](https://waymo.com/open/) | [CVPR](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers) | &#10008;\n",
    "[MagicDrive: Street View Generation with Diverse 3D Geometry Control](https://arxiv.org/abs/2310.02601) | 2023 | | | | | | | [github](https://github.com/cure-lab/MagicDrive) | [website](https://gaoruiyuan.com/magicdrive/) | [nuScenes](https://www.nuscenes.org/nuscenes) | | &#10008;\n",
    "[A Game of Bundle Adjustment - Learning Efficient Convergence](https://openaccess.thecvf.com/content/ICCV2023/html/Belder_A_Game_of_Bundle_Adjustment_-_Learning_Efficient_Convergence_ICCV_2023_paper.html) | 2023 | | localization mapping | | | | | [github](https://github.com/amirbelder/A-Game-of-Bundle-Adjustment---Learning-Efficient-Convergence) | | | ICCV | &#10008;\n",
    "[VLAAD: Vision and Language Assistant for Autonomous Driving](https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Park_VLAAD_Vision_and_Language_Assistant_for_Autonomous_Driving_WACVW_2024_paper.html) | 2024 | | | | | | | [github](https://github.com/sungyeonparkk/vision-assistant-for-driving) | | | WACV | &#10004;\n",
    "[NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets Using Markup Annotations](https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Inoue_NuScenes-MQA_Integrated_Evaluation_of_Captions_and_QA_for_Autonomous_Driving_WACVW_2024_paper.html) | 2024 | | data | | | | | [github](https://github.com/turingmotors/NuScenes-MQA) | | | WACV\n",
    "[NuScenes-QA: A Multi-Modal Visual Question Answering Benchmark for Autonomous Driving Scenario](https://ojs.aaai.org/index.php/AAAI/article/view/28253) | 2024 | | | | | | | [github](https://github.com/qiantianwen/NuScenes-QA) | | | AAAI\n",
    "[TrafficBots: Towards World Models for Autonomous Driving Simulation and Motion Prediction](https://arxiv.org/abs/2303.04116) | 2023 | | simulator | | | | | [github](https://github.com/zhejz/TrafficBots) | | | ICRA | &#10008;\n",
    "[Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research](https://arxiv.org/abs/2310.08710) | 2023 | | simulator | | | | | [github](https://github.com/waymo-research/waymax) | | | [NeurIPS](https://neurips.cc/virtual/2023/poster/73687) | &#10008;\n",
    "[SwapTransformer: highway overtaking tactical planner model via imitation learning on OSHA dataset](https://arxiv.org/abs/2401.01425) | 2024 | | | | | | | [github](https://github.com/VWIECCResearch/Swaptransformer) | | | | &#10008;\n",
    "[GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models](https://arxiv.org/abs/2312.03543) | 2023 | | | | | | | [github](https://github.com/Petrichor625/Talk2car_CAVG)\n",
    "[LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments](https://arxiv.org/abs/2403.08337) | 2024 | | | | | | | [github](https://github.com/Traffic-Alpha/LLM-Assisted-Light)\n",
    "[DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model](https://arxiv.org/abs/2310.07771) | 2023 | | | | | | | [github](https://github.com/shalfun/DrivingDiffusion) | [website](https://drivingdiffusion.github.io/) | [nuScenes](https://www.nuscenes.org/nuscenes) | | &#10008;\n",
    "[TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models](https://arxiv.org/abs/2309.06719) | 2023 | | Planning | | | | | [github](https://github.com/lijlansg/TrafficGPT) | | | | &#10008;\n",
    "[MOTR: End-to-End Multiple-Object Tracking with Transformer](https://arxiv.org/pdf/2105.03247.pdf) | 2022 | | | | | | | [github](https://github.com/megvii-research/MOTR) | | | | &#10008;\n",
    "[NuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles](https://arxiv.org/abs/2106.11810) | 2022 | | | | | | | [github](https://github.com/motional/nuplan-devkit) | | | | &#10008;\n",
    "[OASim: an Open and Adaptive Simulator based on Neural Rendering for Autonomous Driving](https://arxiv.org/abs/2402.03830) | 2024 | | simulator | | | | | [github](https://github.com/PJLab-ADG/OASim) | [website](https://pjlab-adg.github.io/OASim/) | | | &#10008;\n",
    "[OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving](https://arxiv.org/abs/2311.16038) | 2023 | | | | | | | [github](https://github.com/wzzheng/OccWorld) | | | | &#10008;\n",
    "[OccNeRF: Advancing 3D Occupancy Prediction in LiDAR-Free Environments](https://arxiv.org/abs/2312.09243) | 2023 | | | | | | | [github](https://github.com/LinShan-Bin/OccNeRF) | [website](https://linshan-bin.github.io/OccNeRF/) | | | &#10008;\n",
    "[Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models](https://arxiv.org/abs/2310.17642) | 2023 | | | | | | | TBU | [website](https://drive-anywhere.github.io/)\n",
    "[DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model](https://arxiv.org/abs/2310.01412) | 2023 | Llama 2 | Planning Control | Vision, Language | In-context learning | Image Text Action | Text / Action | soon | [website](https://tonyxuqaq.github.io/projects/DriveGPT4/) | [BDD-X](https://github.com/JinkyuKimUCB/BDD-X-dataset)\n",
    "[Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles](https://arxiv.org/abs/2309.10228) | 2023 | GPT-4 | Planning | Language | In-context learning | Text | Code | | | | [WACV](https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Cui_Drive_As_You_Speak_Enabling_Human-Like_Interaction_With_Large_Language_WACVW_2024_paper.html)\n",
    "[LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs](https://arxiv.org/abs/2312.04372) | 2024 | GPT-4 / LLaMA-2 / PaLM2 | Planning (Code Generation) | Language | In-context learning | Text | Code as action | | | | [CVPR](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers)\n",
    "[VLP: Vision Language Planning for Autonomous Driving](https://arxiv.org/abs/2401.05577) | 2024 | CLIP Text Encoder | Planning | Image, Language | Training | Vision, Language | Text / Action | | | [nuScenes](https://www.nuscenes.org/nuscenes) | [CVPR](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers)\n",
    "[Driving Everywhere with Large Language Model Policy Adaptation](https://arxiv.org/abs/2402.05932) | 2024 | GPT-4 | | | | | | | [website](https://boyiliee.github.io/llada/) | [nuScenes](https://www.nuscenes.org/nuscenes) | [CVPR](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers)\n",
    "[Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models](https://arxiv.org/abs/2401.00988) | 2024 | | | | | | | | | | [CVPR](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers)\n",
    "[A Safer Vision-Based Autonomous Planning System for Quadrotor UAVs With Dynamic Obstacle Trajectory Prediction and Its Application With LLMs](https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Zhong_A_Safer_Vision-Based_Autonomous_Planning_System_for_Quadrotor_UAVs_With_WACVW_2024_paper.html) | 2024 | | | | | | | | | | WACV\n",
    "[Latency Driven Spatially Sparse Optimization for Multi-Branch CNNs for Semantic Segmentation](https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Zampokas_Latency_Driven_Spatially_Sparse_Optimization_for_Multi-Branch_CNNs_for_Semantic_WACVW_2024_paper.html) | 2024 | | | | | | | | | | WACV\n",
    "[LIP-Loc: LiDAR Image Pretraining for Cross-Modal Localization](https://openaccess.thecvf.com/content/WACV2024W/LLVM-AD/html/Shubodh_LIP-Loc_LiDAR_Image_Pretraining_for_Cross-Modal_Localization_WACVW_2024_paper.html) | 2024 | | | | | | | | | | WACV\n",
    "[DRAMA: Joint Risk Localization and Captioning in Driving](https://openaccess.thecvf.com/content/WACV2023/html/Malla_DRAMA_Joint_Risk_Localization_and_Captioning_in_Driving_WACV_2023_paper.html) | 2024 | | | | | | | | | [DRAMA](https://usa.honda-ri.com/drama) | WACV\n",
    "[Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion](https://arxiv.org/abs/2311.01017) | 2024 | | | | | | | | | | ICLR\n",
    "[Driving Through the Concept Gridlock: Unraveling Explainability Bottlenecks in Automated Driving](https://openaccess.thecvf.com/content/WACV2024/html/Echterhoff_Driving_Through_the_Concept_Gridlock_Unraveling_Explainability_Bottlenecks_in_Automated_WACV_2024_paper.html) | | | | | | | | | | | WACV\n",
    "[UniSim: A Neural Closed-Loop Sensor Simulator](https://openaccess.thecvf.com/content/CVPR2023/html/Yang_UniSim_A_Neural_Closed-Loop_Sensor_Simulator_CVPR_2023_paper.html) | 2023 | | | | | | | | [website](https://waabi.ai/unisim/) | | CVPR\n",
    "[LightSim: Neural Lighting Simulation for Urban Scenes](https://arxiv.org/abs/2312.06654) | 2023 | | | | | | | | [website](https://waabi.ai/lightsim/) | | [NeurIPS](https://neurips.cc/virtual/2023/poster/70544)\n",
    "[Generalized Predictive Model for Autonomous Driving](https://arxiv.org/abs/2403.09630) | 2024 | | | | | | | | | | [CVPR](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers)\n",
    "[Panacea: Panoramic and Controllable Video Generation for Autonomous Driving](https://arxiv.org/abs/2311.16813) | 2024 | | | | | | | | [website](https://panacea-ad.github.io/) | | [CVPR](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers)\n",
    "[DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes](https://arxiv.org/abs/2312.07920) | 2024 | | | | | | | | | | [CVPR](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers)\n",
    "[Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles](https://arxiv.org/abs/2310.08034) | 2023 | GPT-4 | Planning Control | Language | In-context learning | Text | Action | | | [HighwayEnv](https://github.com/Farama-Foundation/HighwayEnv)\n",
    "[SurrealDriver: Designing Generative Driver Agent Simulation Framework in Urban Contexts based on Large Language Model](https://arxiv.org/abs/2309.13193) | 2023 | GPT-4 | Planning Control | Language | In-context learning | Text | Text / Action | | | [carla-simulator](https://github.com/carla-simulator)\n",
    "[LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving](https://arxiv.org/abs/2310.03026) | 2023 | GPT-3.5 | Planning | Language | In-context learning | Text | Action | | | [Complex Urban Scenarios](https://github.com/liuyuqi123/ComplexUrbanScenarios) [carla-simulator](https://github.com/carla-simulator)\n",
    "[Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain](https://arxiv.org/abs/2307.11769) | 2023 | GPT-3.5 | Text Generation | Language | In-context learning | Text | Concept\n",
    "[DriveLLM: Charting the path toward full autonomous driving with large language models](https://ieeexplore.ieee.org/abstract/document/10297415) | 2023 | GPT-4 | Planning Control | Language | In-context learning | Text | Action\n",
    "[DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving](https://arxiv.org/abs/2312.09245) | 2023 | LLaMA+Q-Former | Perception Planning | Vision, Language | Training | RGB Image LiDAR Text | Decision State | | | [carla-simulator](https://github.com/carla-simulator)\n",
    "[DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models](https://arxiv.org/abs/2402.12289) | 2024 | Qwen-VL | Planning | Sequence of Images, Language | Training | Vision, Language | Text / Action | | [website](https://tsinghua-mars-lab.github.io/DriveVLM/) | [nuScenes](https://www.nuscenes.org/nuscenes)\n",
    "[RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model](https://arxiv.org/abs/2402.10828) | 2024 | Vicuna1.5-7B | Planning Control | Video, Language | Training | Vision, Language | Text / Action | | [website](https://yuanjianhao508.github.io/RAG-Driver/)\n",
    "[DriveCoT: Integrating Chain-of-Thought Reasoning with End-to-End Driving](https://arxiv.org/abs/2403.16996) | 2024 | | | | | | | | [website](https://drivecot.github.io/)\n",
    "[DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving](https://arxiv.org/abs/2309.09777) | 2023 | | | | | | | | [website](https://drivedreamer.github.io/) | [nuScenes](https://www.nuscenes.org/nuscenes) | | &#10008;\n",
    "[DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation](https://arxiv.org/abs/2403.06845) | 2024 | | | | | | | | [website](https://drivedreamer2.github.io/) | [nuScenes](https://www.nuscenes.org/nuscenes)\n",
    "[AccidentGPT: Accident Analysis and Prevention from V2X Environmental Perception with Multi-modal Large Model](https://arxiv.org/abs/2312.13156) | 2023 | | | | | | | | [website](https://accidentgpt.github.io/)\n",
    "[LLM-Assist: Enhancing Closed-Loop Planning with Language-Based Reasoning](https://arxiv.org/abs/2401.00125) | 2023 | | | | | | | | [website](https://llmassist.github.io/)\n",
    "[Dolphins: Multimodal Language Model for Driving](https://arxiv.org/abs/2312.00438) | 2023 | | | | | | | | [website](https://vlm-driver.github.io/) | [GQA](https://cs.stanford.edu/people/dorarad/gqa/about.html) [COCO](https://cocodataset.org/#home) [VQA](https://visualqa.org/) [OK-VQA](https://okvqa.allenai.org/) [TDIUC](https://kushalkafle.com/projects/tdiuc.html) [Visual Genome](https://homes.cs.washington.edu/~ranjay/visualgenome/index.html) [BDD-X](https://github.com/JinkyuKimUCB/BDD-X-dataset)\n",
    "[BEVGPT: Generative Pre-trained Large Model for Autonomous Driving Prediction, Decision-Making, and Planning](https://arxiv.org/abs/2310.10357) | 2023 \n",
    "[BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving](https://arxiv.org/abs/2401.01065) | 2024\n",
    "[Radar Spectra-language Model for Automotive Scene Parsing](https://openreview.net/pdf?id=bdJaYLiOxi) | 2023 | | | | | | | | | | [ICLR Reject](https://openreview.net/forum?id=bdJaYLiOxi)\n",
    "[SPOT: Scalable 3D Pre-training via Occupancy Prediction for Autonomous Driving](https://arxiv.org/abs/2309.10527) | 2023 | | | | | | | | | | [ICLR Reject](https://openreview.net/forum?id=9zEBK3E9bX)\n",
    "[3D Dense Captioning beyond Nouns: A Middleware for Autonomous Driving](https://openreview.net/pdf?id=8T7m27VC3S) | 2023 | | | | | | | | | | [ICLR Reject](https://openreview.net/forum?id=8T7m27VC3S)\n",
    "[DME-Driver: Integrating Human Decision Logic and 3D Scene Perception in Autonomous Driving](https://arxiv.org/abs/2401.03641) | 2024\n",
    "[Large Language Models for Autonomous Driving: Real-World Experiments](https://arxiv.org/abs/2312.09397) | 2024\n",
    "[Evaluation of Large Language Models for Decision Making in Autonomous Driving](https://arxiv.org/abs/2312.06351) | 2023\n",
    "[Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving](https://arxiv.org/abs/2312.03661) | 2023 | | | | | | | | | [nuScenes](https://www.nuscenes.org/nuscenes) [Waymo](https://waymo.com/open/) [ONCE](https://once-for-auto-driving.github.io/index.html)\n",
    "[GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior Prediction](https://arxiv.org/abs/2311.14786) | 2024 | | | | | | | | | [JAAD](https://data.nvision2.eecs.yorku.ca/JAAD_dataset/) [PIE](https://data.nvision2.eecs.yorku.ca/PIE_dataset/) [WiDEVIEW](https://github.com/unmannedlab/UWB_Dataset)\n",
    "[ADriver-I: A General World Model for Autonomous Driving](https://arxiv.org/abs/2311.13549) | 2023 | | | | | | | | | [nuScenes](https://www.nuscenes.org/nuscenes)\n",
    "[ChatGPT as Your Vehicle Co-Pilot: An Initial Attempt](https://ieeexplore.ieee.org/iel7/7274857/7448921/10286969.pdf) | 2023 | | Planning\n",
    "[Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving](https://arxiv.org/abs/2309.05282) | 2023 | | Prediction | | | | | | | [nuScenes](https://www.nuscenes.org/nuscenes)\n",
    "[HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving](https://arxiv.org/abs/2309.05186) | 2023 | | | | | | | | | [DRAMA](https://usa.honda-ri.com/drama)\n",
    "[Language Prompt for Autonomous Driving](https://arxiv.org/abs/2309.04379) | 2023 | | | | | | | | | [MMDetection3D](https://github.com/open-mmlab/mmdetection3d) [nuScenes](https://github.com/nutonomy/nuscenes-devkit) [PF-Track](https://github.com/TRI-ML/PF-Track) [PETR](https://github.com/megvii-research/PETR) [MOTR](https://github.com/megvii-research/MOTR) [nuScenes](https://www.nuscenes.org/nuscenes)\n",
    "[MTD-GPT: A Multi-Task Decision-Making GPT Model for Autonomous Driving at Unsignalized Intersections](https://arxiv.org/abs/2307.16118) | 2023 | | Prediction　| | | | | | | [HighwayEnv](https://github.com/Farama-Foundation/HighwayEnv)\n",
    "[Language-Guided Traffic Simulation via Scene-Level Diffusion](https://arxiv.org/abs/2306.06344) | 2023\n",
    "[OpenAnnotate3D: Open-Vocabulary Auto-Labeling System for Multi-modal 3D Data](https://arxiv.org/abs/2310.13398) | 2023\n",
    "[Planning with an Ensemble of World Models](https://openreview.net/forum?id=cvGdPXaydP) | 2024 | | | | | | | | | | ICLR Reject\n",
    "[Large Language Models Can Design Game-Theoretic Objectives for Multi-Agent Planning](https://openreview.net/forum?id=DnkCvB8iXR) | 2024\n",
    "[Semantic Anomaly Detection with Large Language Models](https://arxiv.org/abs/2305.11307) | 2023\n",
    "[Addressing Limitations of State-Aware Imitation Learning for Autonomous Driving](https://arxiv.org/abs/2310.20650) | 2023\n",
    "[Driving Style Alignment for LLM-powered Driver Agent](https://arxiv.org/abs/2403.11368) | 2024\n",
    "[Large Language Models Powered Context-aware Motion Prediction](https://arxiv.org/abs/2403.11057) | 2024\n",
    "[LORD: Large Models based Opposite Reward Design for Autonomous Driving](https://arxiv.org/abs/2403.18965) | 2024\n",
    "[WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation](https://arxiv.org/abs/2312.02934) | 2024\n",
    "[MUVO: A Multimodal Generative World Model for Autonomous Driving with Geometric Representations](https://arxiv.org/abs/2311.11762) | 2023\n",
    "[Natural-language-driven Simulation Benchmark and Copilot for Efficient Production of Object Interactions in Virtual Road Scenes](https://arxiv.org/abs/2312.04008) | 2023\n",
    "[Street Gaussians for Modeling Dynamic Urban Scenes](https://arxiv.org/abs/2401.01339) | 2024 | | | | | | | | [website](https://zju3dv.github.io/street_gaussians/)\n",
    "[Neural Rendering based Urban Scene Reconstruction for Autonomous Driving](https://arxiv.org/abs/2402.06826) | 2024\n",
    "[SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject Control](https://arxiv.org/abs/2403.19438) | 2024 | | | | | | | | [website](https://subjectdrive.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a4e55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

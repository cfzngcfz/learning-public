{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c94841d",
   "metadata": {},
   "source": [
    "**Abstract**\n",
    "\n",
    "Motion forecasting for autonomous driving is a challenging task because complex driving scenarios result in a heterogeneous mix of static and dynamic inputs. \n",
    "It is an open problem how best to represent and fuse information about road geometry, lane connectivity, time-varying traffic light state, and history of a dynamic set of agents and their interactions into an effective encoding.\n",
    "To model this diverse set of input features, many approaches proposed to design an equally complex system with a diverse set of modality specific modules. \n",
    "This results in systems that are difficult to scale, extend, or tune in rigorous ways to trade off quality and efficiency.\n",
    "\n",
    "In this paper, we present Wayformer, a family of attention based architectures for motion forecasting that are simple and homogeneous. \n",
    "Wayformer offers a compact model description consisting of an attention based scene encoder and a decoder. \n",
    "In the scene encoder we study the choice of early, late and hierarchical fusion of input modalities. \n",
    "For each fusion type we explore strategies to trade off efficiency and quality via factorized attention or latent query attention. \n",
    "We show that early fusion, despite its simplicity of construction, is not only modality agnostic but also achieves state-of-the-art results on both Waymo Open Motion Dataset (WOMD) and Argoverse leaderboards, demonstrating the effectiveness of our design philosophy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c07518",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this work, we focus on the general task of future behavior prediction of agents (pedestrians, vehicles, cyclists) in real-world driving environments. \n",
    "This is an essential task for safe and comfortable human-robot interactions, enabling high-impact robotics applications like autonomous driving.\n",
    "\n",
    "The modeling needed for such scene understanding is challenging for many reasons. \n",
    "For one, the output is highly unstructured and multimodal—e.g., a person driving a vehicle could carry out one of many underlying intents unknown to an observer, and representing a distribution over diverse and disjoint possible futures is required. \n",
    "A second challenge is that the input consists of a heterogeneous mix of modalities, including agents' past physical state, static road information (e.g. location of lanes and their connectivity), and time-varying traffic light information.\n",
    "\n",
    "Many previous efforts address how to model the multimodal output [1, 2, 3, 4, 5, 6], and develop hand-engineered architectures to fuse different input types, each requiring their own preprocessing (e.g., image rasterization [7, 2, 8]). \n",
    "Here, we focus on the multimodality of the input space, and develop a simple yet effective modality-agnostic framework that avoids complex and heterogeneous architectures, and leads to a simpler architecture parameterization. \n",
    "This compact description of a family of architectures results in a simpler design space and allows us to more directly and effectively control for trade-offs in model quality and latency by tuning model computation and capacity.\n",
    "\n",
    "To keep complexity under control without sacrificing quality or efficiency, we need to find general modeling primitives, which can handle multimodal features that exist in temporal and spatial dimensions concurrently. \n",
    "Recently, several approaches proposed Transformer networks as the networks of choice for motion forecasting problems [9, 10, 11, 12, 13]. \n",
    "While these approaches offer simplified model architectures, they still require domain expertise and excessive modality specific tuning. [14] proposed a stack of cross attention layers sequentially processing one modality at a time. \n",
    "The order in which to process each modality is left to the designer and enumerating all possibilities is combinatorially prohibitive. \n",
    "[3] proposed using separate encoders for each modality, where the type of network and its capacity is open for tuning on a per-modality basis. \n",
    "Then modalities' embeddings are flattened and one single vector is fed to the predictor. \n",
    "While these approaches allow for many degrees of freedom, they increase the search space significantly. \n",
    "Without efficient network architecture search or significant human input and hand engineering, the chosen models will likely be sub-optimal given that a limited amount of the modeling options have been explored.\n",
    "\n",
    "Our experiments suggest the domain of motion forecasting conforms to Occam's Razor. \n",
    "We show state of the art results with the simplest design choices and making minimal domain specific assumptions, which is in stark contrast to previous work. \n",
    "When tested in simulation and on real AVs, these Wayformer models showed good understanding of the scene.\n",
    "\n",
    "Our contributions can be summarized as follows:\n",
    "• We design a family of models with two basic primitives: a self-attention encoder, where we fuse one or more modalities across temporal and spatial dimensions, and a cross-attention decoder, where we attend to driving scene elements to produce a diverse set of trajectories.\n",
    "• We study three variations of the scene encoder that differ in how and when different input modalities are fused.\n",
    "• To keep our proposed models within practical real time constraints of motion forecasting, we study two common techniques to speed up self-attention: factorized attention and latent query attention.\n",
    "• We achieve state-of-the-art results on both WOMD and Argoverse challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6121aad",
   "metadata": {},
   "source": [
    "# Multimodal Scene Understanding\n",
    "Driving scenarios consist of multimodal data, such as road information, traffic light state, agent history, and agent interactions. \n",
    "In this section we detail the representation of these modalities in our setup. \n",
    "For readability, we define the following symbols: $A$ denotes the number of modeled ego-agents, $T$ denotes the number of past and current timesteps being considered in the history, with a feature size $D_m$. \n",
    "For a modality $m$, we might have a $4^{th}$ dimension ($S_m$) representing a \"set of contextual objects\" (i.e. representations of other road users) for each modeled agent.\n",
    "\n",
    "**Agent History** contains a sequence of past agent states along with the current state $[A, T, 1, D_h]$.\n",
    "For each timestep $t \\in T$, we consider features that define the state of the agent e.g. x, y, velocity, acceleration, bounding box and so on. \n",
    "We include a context dimension $S_h = 1$ for homogeneity.\n",
    "\n",
    "**Agent Interactions** The interaction tensor $[A, T, S_i, D_i]$ represents the relationship between agents. \n",
    "For each modeled agent $a \\in A$, a fixed number of the closest context agents $c_i \\in S_i$ around the modeled agent are considered. \n",
    "These context agents represent the agents which influence the behavior of our modeled agent. \n",
    "The features in $D_i$ represent the physical state of each context agents (as in $D_h$ above), but transformed into the frame of reference of our ego-agent.\n",
    "\n",
    "**Roadgraph** The roadgraph $[A, 1, S_r, D_r]$ contains road features around the agent. \n",
    "Following [2], we represent roadgraph segments as polylines, approximating the road shape with collections of line segments specified by their endpoints and annotated with type information. \n",
    "We use $S_r$ roadgraph segments closest to the modeled agent. \n",
    "Note that there is no time dimension for the road features, but we include a time dimension of 1 for homogeneity with the other modalities.\n",
    "\n",
    "**Traffic Light State** For each agent $a \\in A$, traffic light information $[A, T, S_{tls}, D_{tls}]$ contains the states of the traffic signals that are closest to that agent. \n",
    "Each traffic signal point $tls \\in S{tls}$ has features $D_{tls}$ describing the position and confidence of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f3788",
   "metadata": {},
   "source": [
    "# Wayformer\n",
    "We design the family of Wayformer models to consist of two main components: a Scene Encoder and a Decoder. \n",
    "The scene encoder is mainly composed of one or more attention encoders that summarize the driving scene. \n",
    "The decoder is a stack of one or more standard transformer crossattention blocks, in which learned initial queries are fed in, and then cross-attended with the scene encoding to produce trajectories. \n",
    "Figure 1 shows the Wayformer model processing multimodal inputs to produce scene encoding. \n",
    "This scene encoding serves as the context for the decoder to generate k possible trajectories covering the multimodality of the output space.\n",
    "\n",
    "**Frame of Reference** As our model is trained to produce futures for a single agent, we transform\n",
    "the scene into an ego-centric frame of reference by centering and rotating the scene’s spatial features\n",
    "around the ego-agent’s position and heading at the current time step.\n",
    "Projection Layers Different input modalities may not share the same number of features, so we\n",
    "project them to a common dimension D before concatenating all modalities along the temporal and\n",
    "spatial dimensions [S, T]. We found the simple transformation Projection(xi) = relu(Wxi + b),\n",
    "where xi 2 RDm, b 2 RD, andW 2 RD\u0002Dm, to be sufficient. Concretely, given an input of shape\n",
    "[A; T; Sm;Dm] we project its last dimension producing a tensor of size [A; T; Sm;D].\n",
    "Positional Embeddings Self-attention is naturally permutation equivariant, therefore, we may\n",
    "think of them as set-encoders rather than sequence encoders. However, for modalities where the\n",
    "data does follow a specific ordering, for example agent state across different time steps, it is beneficial\n",
    "to break permutation equivariance and utilize the sequence information. This is commonly\n",
    "done through positional embeddings. For simplicity, we add learned positional embeddings for all\n",
    "modalities. As not all modalities are ordered, the learned positional embeddings are initially set to\n",
    "zero, letting the model learn if it is necessary to utilize the ordering within a modality.\n",
    "3.1 Fusion\n",
    "Once projections and positional embeddings are applied to different modalities, the scene encoder\n",
    "combines the information from all modalities to generate a representation of the environment.\n",
    "Concretely, we aim to learn a scene representation Z = Encoder(fm0;m1; :::;mkg); where\n",
    "mi 2 RA\u0002(T\u0002Sm)\u0002D, Z 2 RA\u0002L\u0002D, and L is a hyperparameter.\n",
    "However, the diversity of input sources makes this integration a non-trivial task. Modalities might\n",
    "not be represented at the same abstraction level or scale: fpixels vs objectsg. Therefore, some\n",
    "modalities might require more computation than the others. Splitting compute and parameter count\n",
    "among modalities is application specific and non-trivial to hand-engineer. We attempt to simplify\n",
    "the process by proposing three levels of fusion: fLate, Early, Hierarchicalg.\n",
    "Late Fusion This is the most common approach used by motion forecasting models, where each\n",
    "modality has its own dedicated encoder (See Figure 2). We set the width of these encoders to be\n",
    "equal to avoid introducing extra projection layers to their outputs. Moreover, we share the same\n",
    "depth across all encoders to narrow down the exploration space to a manageable scope. Transfer of\n",
    "information across modalities is allowed only in the cross-attention layers of the trajectory decoder.\n",
    "Early Fusion Instead of dedicating a self-attention encoder to each modality, early fusion reduces\n",
    "modality specific parameters to only the projection layers (See Figure 2). In this paradigm, the scene\n",
    "encoder consists of a single self-attention encoder (“Cross-Modal Encoder”), giving the network\n",
    "maximum flexibility in assigning importance across modalities with minimal inductive bias.\n",
    "Hierarchical Fusion As a compromise between the two previous extremes, capacity is split between\n",
    "modality-specific self-attention encoders and the cross-modal encoder in a hierarchical fashion.\n",
    "As done in late fusion, width and depth is common across attention encoders and the cross\n",
    "modal encoder. This effectively splits the depth of the scene encoder between modality specific\n",
    "encoders and the cross modal encoder (Figure 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2831bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
